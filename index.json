[{"categories":["microservice"],"content":" 本文为系列篇微服务的关于 深入 gRPC 的文章。本篇将会从 gRPC 的基本概念、gRPC 的使用、gRPC 的编程模型、gRPC 的编程模型的实现、gRPC 的编程模型的实现的细节等多个角度来了解。 本篇为 深入了解gRPC 的下篇，篇幅原因，将这篇文章拆分成上下篇，点击这里查看上篇 ","date":"2022-07-07","objectID":"/posts/microservices-grpc-part2/:0:0","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part2","uri":"/posts/microservices-grpc-part2/"},{"categories":["microservice"],"content":"1. 前言 上一篇文章介绍了 grpc 的基本概念，基础用法和其基本编程模式 – 应答模式 相关的内容，这篇将会继续讲解 grpc 下的编程模式。本篇将会介绍 数据流编程模式 的使用和实现，之后介绍 grpc 其他核心逻辑和使用经验。 ","date":"2022-07-07","objectID":"/posts/microservices-grpc-part2/:1:0","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part2","uri":"/posts/microservices-grpc-part2/"},{"categories":["microservice"],"content":"2. 数据流流模式 数据流模式是服务端或客户端以流的形式持续向对方读/写数据，直到任意一方结束这次通信。这种模式的使用场景也比较多，比如： 客户端上传文件，文件被客户端切分成多个块，然后发送给服务端 服务端想客户端下发一个数据流（类似 tail -f 远端文件 or log）或者下载一个较大文件，由服务端分片下发给客户端。 客户端订阅服务端数据。 客户端与服务端进行交互式通信，像聊天一样。 而数据流模式从细节上可以有三种情况，而这三种情况各有一些细节上的区别，下面我们来看看这三种情况。 ","date":"2022-07-07","objectID":"/posts/microservices-grpc-part2/:2:0","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part2","uri":"/posts/microservices-grpc-part2/"},{"categories":["microservice"],"content":"2.1 客户端单向数据流 首先定义 rpc 方法： // 假如批量创建大量订单 rpc CreateOrder(stream Empty) returns (Empty) {} 生成的客户端侧代码如下： type OrderServiceClient interface { CreateOrder(ctx context.Context, opts ...grpc.CallOption) (OrderService_CreateOrderClient, error) } type OrderService_CreateOrderClient interface { Send(*Empty) error CloseAndRecv() (*Empty, error) grpc.ClientStream } 客户端请求服务端后，得到一个 OrderService_CreateOrderClient 对象，然后调用 Send 方法，向服务端持续写入 Empty 数据，直到最后一次的时候，调用 CloseAndRecv 方法，返回服务端的响应。服务端仅在最后一次进行响应。 而服务端侧生成的代码如下： type OrderServiceServer interface { CreateOrder(OrderService_CreateOrderServer) error } type OrderService_CreateOrderServer interface { SendAndClose(*Empty) error // 实际上并不会执行任何 close 操作，由客户端在 recv 时 close Recv() (*Empty, error) grpc.ServerStream } 那么对用使用者来说应该如何使用这些生成的代码来实现自己的需求呢？请看使用案例： // client func createOrder() error { c,err := orderpb.NewOrderServiceClient(grpcConn).CreateOrder(ctx) if err != nil { return err } for someCondition { if err := c.Send(\u0026orderpb.Empty{});err != nil { return err } } // finish send empty,err := c.CloseAndRecv() if err != nil { return err } // finish recv // do something with empty return nil } // server func handleCreateOrder(s orderpb.OrderService_CreateOrderServer) error { for someCondition { empty,err := s.Recv() if err != nil { return err } // do something with empty } // finish recv if err := s.SendAndClose(\u0026orderpb.Empty{});err != nil { return err } // finish send return nil } ","date":"2022-07-07","objectID":"/posts/microservices-grpc-part2/:2:1","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part2","uri":"/posts/microservices-grpc-part2/"},{"categories":["microservice"],"content":"2.2 服务端单向数据流 首先定义 rpc 方法： // 假如返回的数据量很多 or 需要持续返回最新数据，更多的像一种 订阅模式 rpc GetOrderList(Empty) returns (stream Empty) {} 生成的客户端侧代码如下： type OrderServiceClient interface { GetOrderList(ctx context.Context, in *Empty, opts ...grpc.CallOption) (OrderService_GetOrderListClient, error) } type OrderService_GetOrderListClient interface { Recv() (*Empty, error) grpc.ClientStream } 客户端请求服务端后拿到一个 OrderService_GetOrderListClient 对象，然后调用 Recv 方法，接收服务端的数据流，一直到报错或自己逻辑中断。 服务端侧生成的代码如下： type OrderServiceServer interface { GetOrderList(*Empty, OrderService_GetOrderListServer) error } type OrderService_GetOrderListServer interface { Send(*Empty) error grpc.ServerStream } 服务端收到请求时，会传参 Empty 和 OrderService_GetOrderListServer 对象，第一个参数是由客户端传过来，第二个参数用来写入数据流。服务端向 OrderService_GetOrderListServer 持续写入数据，直到报错或自己逻辑中断。 那么对用使用者来说应该如何使用这些生成的代码来实现自己的需求呢？请看使用案例： // client func listOrder() error { c,err := orderpb.NewOrderServiceClient(grpcConn).GetOrderList(ctx) if err != nil { return err } for someCondition { empty,err := c.Recv() if err != nil { return err } // do something with empty } // finish recv return nil } // server func handleListOrder(e *orderpb.Empty, s orderpb.OrderService_CreateOrderServer) error { for someCondition { if err := s.Send(e);err != nil { return err } } // finish send return nil } ","date":"2022-07-07","objectID":"/posts/microservices-grpc-part2/:2:2","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part2","uri":"/posts/microservices-grpc-part2/"},{"categories":["microservice"],"content":"2.3 双向数据流 双向数据流可以理解为上面两种数据流模型的组合，客户端和服务端均可以向 socket 写入流数据，同时可以从 socket 读取流数据。 首先定义 rpc 方法： service OrderService { rpc BothWayStream(stream Empty) returns (stream Empty) {} } 生成的客户端侧代码如下： // For semantics around ctx use and closing/ending streaming RPCs, please refer to https://pkg.go.dev/google.golang.org/grpc/?tab=doc#ClientConn.NewStream. type OrderServiceClient interface { BothWayStream(ctx context.Context, opts ...grpc.CallOption) (OrderService_BothWayStreamClient, error) } type OrderService_BothWayStreamClient interface { Send(*Empty) error Recv() (*Empty, error) grpc.ClientStream } 客户端发起请求后，拿到一个 OrderService_BothWayStreamClient 对象，然后调用 Send 方法，向服务端写入数据，然后调用 Recv 方法，接收服务端的数据流，一直到报错或自己逻辑中断。 服务端侧生成的代码如下： type OrderServiceServer interface { BothWayStream(OrderService_BothWayStreamServer) error } type OrderService_BothWayStreamServer interface { Send(*Empty) error Recv() (*Empty, error) grpc.ServerStream } 而服务端的定义的方式接受的参数是 OrderService_BothWayStreamServer 对象，通过该对象的 Send 方法向客户端写入数据，通过 Recv 方法接收客户端的数据流。 以实现一个 ssh proxy 的例子来介绍双向数据流模式的使用： 客户端的实现： func client(stdin io.Reader, stdout io.Writer) error { // 定义一个双向数据流 stream, err := orderpb.NewOrderServiceClient(grpcConn).BothWayStream(ctx) if err != nil { return } // read from stdin go func() { buf := make([]byte, 1024) for { n, err := stdin.Read(buf) if err != nil { return } err = stream.Send(\u0026orderpb.Empty{buf}) if err != nil { return } } }() // write to stdout for { empty, err := stream.Recv() if err != nil { return err } _, err = stdout.Write(empty.Data) if err != nil { return err } } } 服务端的实现： func server(stream orderpb.OrderService_BothWayStreamServer) error { for { req, err := stream.Recv() if err != nil { return err } // do something with req resp := doSomething(req) err = stream.Send(resp) if err != nil { return err } } } 以上就是三种数据流模式的定义和使用示例，下面我们从源码层面去理解，客户端/服务端是如何实现的读写流数据的。 ","date":"2022-07-07","objectID":"/posts/microservices-grpc-part2/:2:3","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part2","uri":"/posts/microservices-grpc-part2/"},{"categories":["microservice"],"content":"2.4 数据流的实现 数据流的实现我们分成客户端和服务端来讲解。 2.4.1 客户端读写数据流 客户端的数据流通过 ClientStream 接口实现，客户端对当前数据流的操作都是通过 ClientStream 接口来实现的，比如 Send、Recv 这些方法都是基于 SendMsg、RecvMsg 方法封装的。 // ClientStream defines the client-side behavior of a streaming RPC. // // All errors returned from ClientStream methods are compatible with the // status package. type ClientStream interface { // Header returns the header metadata received from the server if there // is any. It blocks if the metadata is not ready to read. Header() (metadata.MD, error) // Trailer returns the trailer metadata from the server, if there is any. // It must only be called after stream.CloseAndRecv has returned, or // stream.Recv has returned a non-nil error (including io.EOF). Trailer() metadata.MD // CloseSend closes the send direction of the stream. It closes the stream // when non-nil error is met. It is also not safe to call CloseSend // concurrently with SendMsg. CloseSend() error // Context returns the context for this stream. // // It should not be called until after Header or RecvMsg has returned. Once // called, subsequent client-side retries are disabled. Context() context.Context // SendMsg is generally called by generated code. On error, SendMsg aborts // the stream. If the error was generated by the client, the status is // returned directly; otherwise, io.EOF is returned and the status of // the stream may be discovered using RecvMsg. // // SendMsg blocks until: // - There is sufficient flow control to schedule m with the transport, or // - The stream is done, or // - The stream breaks. // // SendMsg does not wait until the message is received by the server. An // untimely stream closure may result in lost messages. To ensure delivery, // users should ensure the RPC completed successfully using RecvMsg. // // It is safe to have a goroutine calling SendMsg and another goroutine // calling RecvMsg on the same stream at the same time, but it is not safe // to call SendMsg on the same stream in different goroutines. It is also // not safe to call CloseSend concurrently with SendMsg. SendMsg(m interface{}) error // RecvMsg blocks until it receives a message into m or the stream is // done. It returns io.EOF when the stream completes successfully. On // any other error, the stream is aborted and the error contains the RPC // status. // // It is safe to have a goroutine calling SendMsg and another goroutine // calling RecvMsg on the same stream at the same time, but it is not // safe to call RecvMsg on the same stream in different goroutines. RecvMsg(m interface{}) error } 我们现在一起过一下一次 SendMsg 的流程： 客户端调用 OrderServiceClient.BothWayStream 方法。 BothWayStream 调用 grpcConn.NewStream 方法创建一个新的数据流。而 grpcConn.NewStream 方法主要做以下几件事： 解析服务端的地址，并创建一个连接。 初始化 http2 transport，并创建一个新的 http2 stream。 将 http2 stream, http2 transport 和其他 dial 参数封装成一个 clientStream 对象。 调用 clientStream.SendMsg 方法发送数据。而 clientStream.SendMsg 方法内主要做以下几件事： 将消息 encode, compress 和处理 header 将消息写入到 http2 transport 中。从这里开始往下逻辑与上一篇讲到的应答模式实现时一样的，都是由 http2 client 来实现的。 客户端写入数据流程(点击放大) 下面我们看一下接受消息 RecvMsg 的流程： 与上面一样，先初始化 grpcConn.ClientStream 对象。 调用 clientStream.Recv.Msg 方法读取数据。而 clientStream.Recv.Msg 方法内主要做以下几件事： 从 http2 transport 中读取数据。 解析数据，并将数据解压和 decode。 将数据解析成消息。 读取消息的相关代码： // parser reads complete gRPC messages from the underlying reader. type parser struct { // r is the underlying reader. // See the comment on recvMsg for the permissible // error types. r io.Reader // The header of a gRPC message. Find more detail at // https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md header [5]byte } func (p *parser) recvMsg(maxReceiveMessageSize int) (pf payloadFormat, msg []byte, err error) { // p.r 是 http2 stream 的 reader。 if _, err := p.r.Read(p.header[:]); err != nil { return 0, nil, err } // 第一位记录消息类型 pf = payloadFormat(p.header[0]) // 会四位记录消息长度 length := binary.BigEndian.Uint32(p.header[1:]) if length == 0 { return pf, nil, nil } if int64(length) \u003e int64(maxInt) { return 0, nil, status.Errorf(codes.ResourceExhausted, \"grpc: receiv","date":"2022-07-07","objectID":"/posts/microservices-grpc-part2/:2:4","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part2","uri":"/posts/microservices-grpc-part2/"},{"categories":["microservice"],"content":"4. 性能调优 MaxSendMsgSizeGRPC 最大允许发送的字节数，默认4MiB，如果超过了GRPC会报错。如果有传输大数据的需求，请适当调高这个参数。 MaxRecvMsgSizeGRPC 最大允许接收的字节数，默认4MiB，如果超过了GRPC会报错。同上。 InitialWindowSize 基于Stream的滑动窗口，类似于TCP的滑动窗口，用来做流控，默认64KiB，吞吐量上不去，根据自己的流量往上调整。 InitialConnWindowSize 基于Connection的滑动窗口，默认 64KiB，吞吐量上不去，同上。 至于 MaxConcurrentStreams 的配置（一个连接上的并发 stream 数量），很多文章指出默认是 100，会影响性能，其实不对的。从源码层面来看， http2 server 端支持配置这个参数，但是默认是 0，而该值为 0 的时候，server 端 transport 初始化时做了判断的，如果是 0，则会设置为 math.MaxUint32。 // TODO(zhaoq): Have a better way to signal \"no limit\" because 0 is // permitted in the HTTP2 spec. maxStreams := config.MaxStreams if maxStreams == 0 { // 注意看这里！ maxStreams = math.MaxUint32 } else { isettings = append(isettings, http2.Setting{ // 请记住这个 ID ID: http2.SettingMaxConcurrentStreams, Val: maxStreams, }) } 而 client 端初始化一个新的 http2 client 时，也有一个 maxConcurrentStreams 参数且默认值的确是 100，这个参数是用来限制 client 端的并发 stream 数量的，如果超过了这个值，则会报错。但是这个 100 并非是最终的值，在 client 初始化方法中有个异步处理的流程： // http2_client.go:newHTTP2Client() // // Start the reader goroutine for incoming message. Each transport has // a dedicated goroutine which reads HTTP2 frame from network. Then it // dispatches the frame to the corresponding stream entity. go t.reader() // http2_client.go:http2Client.reader() t.handleSettings(sf, true/*isFirst*/) 而这个 t.handleSettings 是关键方法，它给 client 的一些参数进行了重新赋值，让我们看一下源码： // http2_client.go:http2Client.handleSettings() // http2.SettingFrame 是从服务端读取的数据 func (t *http2Client) handleSettings(f *http2.SettingsFrame, isFirst bool) { if f.IsAck() { return } var maxStreams *uint32 var ss []http2.Setting var updateFuncs []func() f.ForeachSetting(func(s http2.Setting) error { switch s.ID { // 请注意这个 ID，这是服务端设置的 case http2.SettingMaxConcurrentStreams: maxStreams = new(uint32) *maxStreams = s.Val // 也就是这个值现在是 math.MaxUint32 // 这也是从服务端设置的参数 case http2.SettingMaxHeaderListSize: updateFuncs = append(updateFuncs, func() { t.maxSendHeaderListSize = new(uint32) *t.maxSendHeaderListSize = s.Val }) default: ss = append(ss, s) } return nil }) // 此时 maxStreams != nil if isFirst \u0026\u0026 maxStreams == nil { maxStreams = new(uint32) *maxStreams = math.MaxUint32 } sf := \u0026incomingSettings{ ss: ss, } if maxStreams != nil { updateStreamQuota := func() { delta := int64(*maxStreams) - int64(t.maxConcurrentStreams) t.maxConcurrentStreams = *maxStreams // 这里重新赋值了 现在 t.maxConcurrentStreams == math.MaxUint32 t.streamQuota += delta if delta \u003e 0 \u0026\u0026 t.waitingStreams \u003e 0 { close(t.streamsQuotaAvailable) // wake all of them up. t.streamsQuotaAvailable = make(chan struct{}, 1) } } updateFuncs = append(updateFuncs, updateStreamQuota) } // executeAndPut 会直接执行这个方法 t.controlBuf.executeAndPut(func(interface{}) bool { for _, f := range updateFuncs { f() } return true }, sf) } 也就是说虽然client 端的确默认值是 0，但是由于服务端默认不赋值从而设置的是 math.MaxUint32，所以 client 端的默认值也是 math.MaxUint32。 对于想更进一步优化性能的同学，建议最好看一下 grpc/transport 包下的实现 http2 客户端/服务端的代码，了解一下连接管理和数据传输过程，看一下哪些参数会对数据传输大小延迟有影响，从而针对性的优化。 ","date":"2022-07-07","objectID":"/posts/microservices-grpc-part2/:3:0","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part2","uri":"/posts/microservices-grpc-part2/"},{"categories":["microservice"],"content":"6. 总结 本篇主要讲述： 了解 grpc 的流式编程模式的使用，包括单向流和双向流。 了解 grpc 的客户端和服务端如何实现流式数据的读写并了解客户端服务端的读写数据时的函数调用流程。 了解常见 grpc 的性能调优并澄清一个常见的关于 maxConcurrentStreams 的误解。 ","date":"2022-07-07","objectID":"/posts/microservices-grpc-part2/:4:0","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part2","uri":"/posts/microservices-grpc-part2/"},{"categories":["microservice"],"content":" 本文为系列篇微服务的关于 深入 gRPC 的文章。本篇将会从 gRPC 的基本概念、gRPC 的使用、gRPC 的编程模型、gRPC 的编程模型的实现、gRPC 的编程模型的实现的细节等多个角度来了解。 本篇为 深入了解gRPC 的下篇，篇幅原因，将这篇文章拆分成上下篇，下篇继续更新中。 ","date":"2022-06-29","objectID":"/posts/microservices-grpc-part1/:0:0","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part1","uri":"/posts/microservices-grpc-part1/"},{"categories":["microservice"],"content":"1. 前言 gRPC 作为一个 Google 开源的 RPC 框架，由于其优异的性能和支持多种流行语言的特点，被众多的开发者所熟悉。我接触 gRPC 也有至少五年的时间，但是由于种种原因，在很长时间内对 gRPC 的了解处于一个入门或者只是知道个大概的水平。直到大概 2~3 年前在上家公司机缘巧合的缘故，需要对部门内做一次关于 gRPC 的知识分享，而那次我花了 2 周多的时间去了解去背后的原理、实现、数据流向。那时候我记得是白班分享没有写 PPT，所以那时候对这些知识点有了比较深刻的理解。 然而，我上家我所在部门的业务几乎没有涉及到 gRPC 的开发，因此这些理解只是变成一个知道的概念，并没有在实际开发工作中提到实际的应用。但是从那次分享后，我对 gRPC 有了一些迷恋现象，想做一些实际的 gRPC 相关项目，从实际项目中提炼自己的知识面。 到现在，我回过头来看，已经参与了几个基于 gRPC 通信的项目以及基于 gRPC 的微服务框架，最近也在写一个比较完整的微服务项目，也是基于 gRPC 通信。的确从实践中提炼到了一定的知识，自己对整体的理解也有了一定的提升。 今天想写这篇文章的原因有两个，其一是我前前后后对 gRPC 有了很多的交集并且也在上家极力推荐使用（但是能力不够，没能推广起来），我对这块有了一些自己的看法和观点，但是一直没有一个比较完整的记录。其二是之前与大学同学做一次线上分享的时候，有人提问关于 gRPC 的性能问题（由于其基于 HTTP/2,所以对其性能持怀疑态度），我觉得这个问题确实也是需要一个深究的问题，所以这篇文章也会提到相关内容。 因此，这篇文件将会从 gRPC 的基本概念、gRPC 的使用、gRPC 的编程模型、gRPC 的编程模型的实现、gRPC 的编程模型的实现的细节等多个角度来一一进行讲解，给自己一个总结，给对这方面有疑问的同学一定的帮助。 注意 本篇所有的示例代码均用 Go 本篇完全以个人的理解和官方文档为准，若有错误不准之处，请帮忙支持评论一下，谢谢！ ","date":"2022-06-29","objectID":"/posts/microservices-grpc-part1/:1:0","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part1","uri":"/posts/microservices-grpc-part1/"},{"categories":["microservice"],"content":"2. gRPC 的基本概念 Definition by official gRPC is a modern open source high performance Remote Procedure Call (RPC) framework that can run in any environment. It can efficiently connect services in and across data centers with pluggable support for load balancing, tracing, health checking and authentication. It is also applicable in last mile of distributed computing to connect devices, mobile applications and browsers to backend services. 简单来说，gRPC 是一个高性能的远程过程调用框架，可以在任何环境中运行，可以在数据中心之间高效地连接服务，并且支持负载均衡、跟踪、健康检查和身份验证。它还适用于分布式计算，将设备、移动应用和浏览器连接到后端服务。 gRPC 是由 CNCF 孵化的项目,目前在 GitHub 上有 43.8k 的 star 和 9.2k 的 fork。gRPC 有以下几个核心特点： 简单的服务定义。通过 Protocol Buffer 去定义数据结构和服务的接口 (关于 pb 更详细的介绍请查这篇：[系列]微服务·如何通过 protobuf 定义数据和服务)。 快速使用。仅通过一行代码就进行服务注册和远程调用。 跨语言和平台。gRPC 支持众多主流语言，可以在不同语言之间无缝远程调用且均可通过 pb 生成对应语言的相关代码。 支持双向流。gRPC 支持基于 HTTP/2 的双向流，即客户端和服务端均可以向对方读写流数据。 插件化。内置可插拔的负载均衡、跟踪、健康检查和身份验证插件。 微服务。gRPC 非常适合微服务框架，且有众多微服务框架均支持 gRPC。 高性能。得益于 HTTP/2 的链路复用能力，gRPC 可以在同一个连接上同时处理多个请求，同时得益于 pb 为编码出包更快更小的二进制数据包，从而提高了性能。 这些特性使得 gRPC 在微服务架构中的应用非常广泛。以 Go 语言为例，主流的微服务框架 go-micro, go-zero, go-kit, kratos 等都是默认支持 gRPC 的。 ","date":"2022-06-29","objectID":"/posts/microservices-grpc-part1/:2:0","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part1","uri":"/posts/microservices-grpc-part1/"},{"categories":["microservice"],"content":"3. gRPC 的使用 ","date":"2022-06-29","objectID":"/posts/microservices-grpc-part1/:3:0","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part1","uri":"/posts/microservices-grpc-part1/"},{"categories":["microservice"],"content":"3.1 生成 gRPC 代码 在 proto 文件定义服务后，我们通过 protoc 工具生成 gRPC 的代码。此时需要在生成命令中添加 --go-grpc_out 参数来指定生成代码的路径和其他参数。以下面的简单 proto 文件为例： // 为了演示，这里返回值定义为空的结构 message Empty { } // 定义服务和其方法 // 为确保生成的代码尽量简单，我们只定义了两个方法 service OrderService { rpc GetOrder(Empty) returns (Empty) {} rpc CreateOrder(Empty) returns (Empty) {} } 我们执行 protoc --go_out=paths=source_relative:. --go-grpc_out=paths=source_relative:. proto_file 命令，生成代码后，我们可以看到在当前目录下会生成两个文件，分别是 order_service.pb.go 和 order_service_grpc.pb.go。第一个文件包含定义的 enum, message 以及 pb 文件的信息所对应的 Go 代码，第二个文件包含定义的 service 所对应的 Go 代码。本篇不讨论第一个文件内容。我们现在来看一下 order_service_grpc.pb.go 文件和核心内容（篇幅原因会忽略一些非必要代码的展示）。 3.1.1 客户端相关代码 客户端代码相对来说比较简单好理解，定了 OrderServiceClient 之后实现这个接口，而显示方式就是通过 gRPC 连接去调用服务端的 OrderService 服务的对应的方法。我们看的类似这种 /api.user.session.v1.OrderService/GetOrder 字符串可以理解为路由地址，server 端代码生成时会将同样的字符串与其对应的方法共同注册上去，从而确定唯一的方法。 type OrderServiceClient interface { GetOrder(ctx context.Context, in *Empty, opts ...grpc.CallOption) (*Empty, error) CreateOrder(ctx context.Context, in *Empty, opts ...grpc.CallOption) (*Empty, error) } type orderServiceClient struct { cc grpc.ClientConnInterface } func NewOrderServiceClient(cc grpc.ClientConnInterface) OrderServiceClient { return \u0026orderServiceClient{cc} } func (c *orderServiceClient) GetOrder(ctx context.Context, in *Empty, opts ...grpc.CallOption) (*Empty, error) { out := new(Empty) err := c.cc.Invoke(ctx, \"/api.user.session.v1.OrderService/GetOrder\", in, out, opts...) if err != nil { return nil, err } return out, nil } func (c *orderServiceClient) CreateOrder(ctx context.Context, in *Empty, opts ...grpc.CallOption) (*Empty, error) { out := new(Empty) err := c.cc.Invoke(ctx, \"/api.user.session.v1.OrderService/CreateOrder\", in, out, opts...) if err != nil { return nil, err } return out, nil } 我们在自己程序内如果需要调用第三方服务的话，只需要通过 NewOrderServiceClient 函数生成 OrderServiceClient 实例，然后调用对应的方法即可。如： // conn 为 grpc connection，可以通过 grpc.Dial 来生成或大部分微服务框架都提供了连接方法 resp,err := NewOrderServiceClient(conn).GetOrder(context.Background(), \u0026Empty{}) if err != nil { fmt.Println(err) } // end of rpc call, do own biz 3.1.2 服务端相关代码 服务端代码相对客户端代码会多一些，生成代码分为两部分，一部分是定义 interface 然后由一个默认实现类来实现，另一部分是提供注册实现接口的方法。因为我们需要自己去实现定义的服务逻辑，然后注册上去，这样才能让客户端调用。 第一部分代码： // OrderServiceServer is the server API for OrderService service. // All implementations must embed UnimplementedOrderServiceServer // for forward compatibility // 这里需要说明一下，为了确保服务的稳定性，实现该接口的结构必需包含 UnimplementedOrderServiceServer，这样即便我们只实现其中一部分的方法，也不会导致服务崩溃或不可用。 type OrderServiceServer interface { GetOrder(context.Context, *Empty) (*Empty, error) CreateOrder(context.Context, *Empty) (*Empty, error) mustEmbedUnimplementedOrderServiceServer() } // UnimplementedOrderServiceServer must be embedded to have forward compatible implementations. type UnimplementedOrderServiceServer struct { } func (UnimplementedOrderServiceServer) GetOrder(context.Context, *Empty) (*Empty, error) { return nil, status.Errorf(codes.Unimplemented, \"method GetOrder not implemented\") } func (UnimplementedOrderServiceServer) CreateOrder(context.Context, *Empty) (*Empty, error) { return nil, status.Errorf(codes.Unimplemented, \"method CreateOrder not implemented\") } func (UnimplementedOrderServiceServer) mustEmbedUnimplementedOrderServiceServer() {} // UnsafeOrderServiceServer may be embedded to opt out of forward compatibility for this service. // Use of this interface is not recommended, as added methods to OrderServiceServer will // result in compilation errors. type UnsafeOrderServiceServer interface { mustEmbedUnimplementedOrderServiceServer() } 第二部分代码： // 这里是我们外部注册入口 func RegisterOrderServiceServer(s grpc.ServiceRegistrar, srv OrderServiceServer) { s.RegisterService(\u0026OrderService_ServiceDesc, srv) } // 每个接口的处理方法，内部调用的是这个方法 func _OrderService_GetOrder_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) { in := new(Empty) if err := dec(in); err != nil { return nil, err } if interceptor == nil { return sr","date":"2022-06-29","objectID":"/posts/microservices-grpc-part1/:3:1","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part1","uri":"/posts/microservices-grpc-part1/"},{"categories":["microservice"],"content":"4. gRPC 的编程模型 grpc 编程模型可以从大体上分为两种情况，分别是应答模式，数据流模式。应答模式是指客户端发送一个请求，服务端返回一个响应（常见的 http request-response 模式），然后这次请求完成。而数据流模式是客户端和服务端其中一方以流的形式持续读/写数据（也可能双方都是持续读写，双向流），另一方只需要一次请求或响应（如果是双向流则均可以多次读写）。 ","date":"2022-06-29","objectID":"/posts/microservices-grpc-part1/:4:0","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part1","uri":"/posts/microservices-grpc-part1/"},{"categories":["microservice"],"content":"4.1 应答模式 这个模式属于是最常见大家最熟悉的一种模式，在我们定义服务的方法的时候也是基本用的是应答模式。我们上面提到的 GetOrder 方法，就是一个应答模式的例子。请求时构造输入参数，然后等到响应返回，然后结束这次远程调用，这就是应答模式。 4.1.1 使用 该方式的使用我们在上面其实已经演示过了，这里不再赘述。点击这里跳回查看 4.1.2 实现 一次客户端远程调用服务端方法的流程步骤大体如下： 客户端调用对应的 Client 方法 client 方法实现内调用 invoke 方法 并带上对应的 method 和其他参数 invoke 方法内总共分三步： 创建一个 ClientStream 对象，初始化请求需要的参数，确定请求 endpoint 地址，初始化 buffer size，获取 http2 transport 对象等 调用 ClientStream.SendMsq 方法。首先初始化请求 header, payload 和 data， 然后调用 http2 client 的 Write 方法，该方法是异步处理请求的，会把 send request 写入到一个单向链表内，然后由一个单独的 goroutine 去消费这个链表上的数据，然后批量写入到 socket 中。 write: // Write formats the data into HTTP2 data frame(s) and sends it out. The caller // should proceed only if Write returns nil. func (t *http2Client) Write(s *Stream, hdr []byte, data []byte, opts *Options) error { if opts.Last { // If it's the last message, update stream state. if !s.compareAndSwapState(streamActive, streamWriteDone) { return errStreamDone } } else if s.getState() != streamActive { return errStreamDone } df := \u0026dataFrame{ streamID: s.id, endStream: opts.Last, h: hdr, d: data, } if hdr != nil || data != nil { // If it's not an empty data frame, check quota. if err := s.wq.get(int32(len(hdr) + len(data))); err != nil { return err } } // controlBuf 底层为一个缓冲区，用于存储控制数据，比如 header 和 data。基于单向链表实现 return t.controlBuf.put(df) } // writeLoop 内部调用 write 方法，循环发送数据 read from buf and write to socket: // 这段注释其实写的很详细了，我们可以看到，这里的 writeLoop 内部调用了 write 方法，然后再调用了一个单独的 goroutine，这个 goroutine 就 // 是一个单向链表的消费者，直到链表为空，然后再一次性写入到 socket 中。 // run should be run in a separate goroutine. // It reads control frames from controlBuf and processes them by: // 1. Updating loopy's internal state, or/and // 2. Writing out HTTP2 frames on the wire. // // Loopy keeps all active streams with data to send in a linked-list. // All streams in the activeStreams linked-list must have both: // 1. Data to send, and // 2. Stream level flow control quota available. // // In each iteration of run loop, other than processing the incoming control // frame, loopy calls processData, which processes one node from the activeStreams linked-list. // This results in writing of HTTP2 frames into an underlying write buffer. // When there's no more control frames to read from controlBuf, loopy flushes the write buffer. // As an optimization, to increase the batch size for each flush, loopy yields the processor, once // if the batch size is too low to give stream goroutines a chance to fill it up. func (l *loopyWriter) run() (err error) { defer func() { if err == ErrConnClosing { // Don't log ErrConnClosing as error since it happens // 1. When the connection is closed by some other known issue. // 2. User closed the connection. // 3. A graceful close of connection. if logger.V(logLevel) { logger.Infof(\"transport: loopyWriter.run returning. %v\", err) } err = nil } }() for { it, err := l.cbuf.get(true) if err != nil { return err } if err = l.handle(it); err != nil { return err } if _, err = l.processData(); err != nil { return err } gosched := true hasdata: for { it, err := l.cbuf.get(false) if err != nil { return err } if it != nil { // 根据数据类型做不同的处理 // 如果是stream data，则会把数据写入到 loopWriter 的 activeStreams 中， 也是个单向链表 if err = l.handle(it); err != nil { return err } // 从 activeStreams 中读取一个数据 然后把数据写入到 loopWriter 的 frameBuf 中 // 该方法的第一参数为 bool，当 activeStreams 为空是返回true，否则返回false if _, err = l.processData(); err != nil { return err } // 读完读取下一个 continue hasdata } isEmpty, err := l.processData() if err != nil { return err } // activeStreams 中依然有数据还没 process if !isEmpty { continue hasdata } if gosched { gosched = false // 如果当前处理的数据大小小于 minBatchSize（1000），则休眠一下，等待下一次的数据 if l.framer.writer.offset \u003c minBatchSize { runtime.Gosched() continue hasdata } } // 数据 flush 到 socket l.framer.writer.Flush() break hasdata } } } 调用 ClientStream.RecvMsg 方法。该方法会先响应的 header 消息，从 header 读取数据 encoding，然后根据 encoding 读取数据解压数据，并把数据绑定到这次请求响应的 pb message 结构上。最后会调用 ClientStream.finish 方法，表示结束该请求。 客户端请求流程(点击放大) 一次服务端收到一个请求，然后处理完响应回去的流程是这样的： grpc 服务启动，开始监听端口 net.Li","date":"2022-06-29","objectID":"/posts/microservices-grpc-part1/:4:1","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part1","uri":"/posts/microservices-grpc-part1/"},{"categories":["microservice"],"content":"5. 总结 由于篇幅原因，本篇将在这里结束，关于 grpc 的数据流变成模式和相关实现以及其他更多关于 grpc 的内容，请持续关注，我会在下一篇中进行详细的介绍。 本篇主要讲述了： grpc 的概念 grpc 在 go 语言环境下的使用 grpc 的常见编程模式之一的应答模式的使用和实现源码解析 ","date":"2022-06-29","objectID":"/posts/microservices-grpc-part1/:5:0","tags":["微服务","系列篇","grpc"],"title":"[系列]微服务·深入了解gRPC Part1","uri":"/posts/microservices-grpc-part1/"},{"categories":["microservice"],"content":" 本文为系列篇微服务的关于 protobuf 定义数据和服务的文章。本篇将会介绍如何通过 pb 定义数据结构和服务以及 pb 的一些另类玩法。 ","date":"2022-06-15","objectID":"/posts/microservices-protobuf/:0:0","tags":["微服务","系列篇","protobuf","grpc"],"title":"[系列]微服务·如何通过 protobuf 定义数据和服务","uri":"/posts/microservices-protobuf/"},{"categories":["microservice"],"content":"1. 前言 Definition by Google Protocol buffers provide a language-neutral, platform-neutral, extensible mechanism for serializing structured data in a forward-compatible and backward-compatible way. It’s like JSON, except it’s smaller and faster, and it generates native language bindings. Protocol buffers are a combination of the definition language (created in .proto files), the code that the proto compiler generates to interface with data, language-specific runtime libraries, and the serialization format for data that is written to a file (or sent across a network connection). Protocol buffer(下面使用 pb 来代替) 是一个 接口定义语言(Interface Definition Language -- IDL)和消息编码格式。旨在提供一种简单、易于使用、可扩展的方式来定义数据结构和服务。pb 是一种纯文本格式，而其内部是纯二进制格式，比其他编码格式(如：json，xml)更加精炼。pb 包含一个或多个消息类型，每个消息类型包含一个或多个字段。其主要特性为： 编码速度快 编码后数据更小 根据 pb 生成各个语言代码(本文以 Go 为例) 支持类型定义 支持定义服务 语法简单 而 gRPC 作为 Google 推出的 rpc 协议，将 pb 作为默认的数据传输格式，也说明了 pb 作为消息编码格式的优秀性。 Pb 解决了什么问题？ pb 提供序列化的消息格式定义，适用于短连接和长连接 适用于微服务中服务之间通信和数据落盘 消息格式由服务提供者定义，而使用者可根据自身条件生成不同语言的代码，免去编码和解码的工作和其中可能出现各类问题 消息定义可以随时修改，而不会影响使用者的代码，使用者只需要保持最新的 pb 文件即可 下面我们从简单到复杂的介绍，如何使用 pb 定义数据结构和服务。 ","date":"2022-06-15","objectID":"/posts/microservices-protobuf/:1:0","tags":["微服务","系列篇","protobuf","grpc"],"title":"[系列]微服务·如何通过 protobuf 定义数据和服务","uri":"/posts/microservices-protobuf/"},{"categories":["microservice"],"content":"2. 数据定义 首先，我们需要看一下 pb 支持的数据类型有哪些，以及这些数据类型生成的代码中的类型的对照。 Pb Go double float64 float float32 int32 int32 int64 int64 uint32 uint32 uint64 uint64 sint32 int32 sint64 int64 fixed32 uint32 fixed64 uint64 sfixed32 int32 sfixed64 int64 bool bool string string bytes []byte map map enum int32 message struct 可以看出 pb 定义的数据类型几乎与大部分编程语言很相似，因此入门 pb 的门槛可以说是很低。 ","date":"2022-06-15","objectID":"/posts/microservices-protobuf/:2:0","tags":["微服务","系列篇","protobuf","grpc"],"title":"[系列]微服务·如何通过 protobuf 定义数据和服务","uri":"/posts/microservices-protobuf/"},{"categories":["microservice"],"content":"2.1 基础用法 下面分别以枚举和消息的角度，来介绍 pb 的基本用法。 注意 下面所有提到的 pb 定义均是以 proto3为准，本文不讨论 proto2 以及 proto2 与 proto3 的区别。 下面提到的代码生成规则均是基于 Go 语言版本的，且经过本人测试验证，但是不会对其他语言生成代码规则做任何保证。 写本文时，使用的工具版本如下： protoc --version :libprotoc 3.19.4 protoc-gen-go : v1.28.0 在定义数据之前，先说一下 .proto 文件的头部规则： syntax = \"proto3\"; // 表示使用 proto3 的语法 // 包名，如果其他 proto 文件引用该文件时，使用该值去引用， 如： // import \"api.user.v1.proto\"; // message xxx { // api.user.v1.Person person= 1; // ... // } package api.user.v1; // go 的包名，可以根据在当前项目的路径定义，需要注意的是，如果其他包引入当前 proto 文件， // 则其他 proto 文件生成 go 代码时，会以 go_package 作为包包名引入使用,因此如果当前项目的 proto 文件会被其他项目引入 // 或者 项目包名是以 github.com/xx/xx 的方式定义，那这里也按这个格式定义完整的路径 option go_package = \"api/user/v1\"; 2.1.1 枚举 enum Sex { Unknown = 0; Male = 1; Female = 2; Other = 3; Alien = -1; } 上面我们定义了一个枚举类型(enum) Sex ，并定义了几个枚举值。这个枚举类型可以作为一个数据类型，可以在当前 proto 文件内被引用。定义使用枚举有几点需要注意： 枚举的值只能是整数 枚举值不能重复 枚举的第一个元素的值必须是 0，且不能不定义 从第二个元素开始，其值可以为任意整数，不需要严格的递增，甚至可以定义为负数 通过 protoc 命令行工具，我们可以根据 .proto 文件不同语言的代码，下面是根据上述定义的枚举值生成的代码一部分： type Sex int32 const ( Sex_Unknown Sex = 0 Sex_Male Sex = 1 Sex_Female Sex = 2 Sex_Other Sex = 3 Sex_Alien Sex = -1 ) // Enum value maps for Sex. var ( Sex_name = map[int32]string{ 0: \"Unknown\", 1: \"Male\", 2: \"Female\", 3: \"Other\", -1: \"Alien\", } Sex_value = map[string]int32{ \"Unknown\": 0, \"Male\": 1, \"Female\": 2, \"Other\": 3, \"Alien\": -1, } ) // 还会生成 Sex 的 String() Type() 等方法，这里忽略不贴代码了 可以看到定义 const 类型和值之外，还会生成两个 map，枚举的名字和值可互相转换。这里也可以更加确定为什么枚举值不能重复的原因了。 2.1.2 消息 message Person { string name = 1; Sex sex = 3; int32 age = 2; float score = 4; map\u003cstring,bytes\u003e extra_data = 5; } 我们定义了一个简单的消息(message)为 Person 并且包含了上面定义的枚举值。定义消息也是有一套自己的规则： 消息的名字必须以字母开头，后面可以跟字母、数字、下划线，且大小写不明感，生成的代码中会自动将消息名字转换为大写 消息字段定义是，先指定类型，再指定字段名，最后需要指定索引值 消息索引值必须是整数，且不重复即可，无需要严格的递增 消息字段名可以是小写 或 snake case,生成的代码会转换成首字母大写的 Camel Case 下面看一下基于这个消息结构生成的代码： type Person struct { state protoimpl.MessageState sizeCache protoimpl.SizeCache unknownFields protoimpl.UnknownFields Name string `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"` Sex Sex `protobuf:\"varint,3,opt,name=sex,proto3,enum=api.user.session.v1.Sex\" json:\"sex,omitempty\"` Age int32 `protobuf:\"varint,2,opt,name=age,proto3\" json:\"age,omitempty\"` Score float32 `protobuf:\"fixed32,4,opt,name=score,proto3\" json:\"score,omitempty\"` ExtraData map[string][]byte `protobuf:\"bytes,5,rep,name=extra_data,json=extraData,proto3\" json:\"extra_data,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"` } // 同时会生成一堆方法，这里忽略不贴代码了 可以看到，消息会生成一个结构体，并每个字段都会带上 protobuf 和 json 的 tag，方便序列化更方便。protobuf tag 会详细记录字段的在 proto 文件定义的名字，索引值、proto 版本等信息，用于编码和解码。而 json tag 仅记录字段名。 ","date":"2022-06-15","objectID":"/posts/microservices-protobuf/:2:1","tags":["微服务","系列篇","protobuf","grpc"],"title":"[系列]微服务·如何通过 protobuf 定义数据和服务","uri":"/posts/microservices-protobuf/"},{"categories":["microservice"],"content":"2.2 高级玩法 2.2.1 组合使用 上面定义了些简单的使用方式，但是实际开发过程中需要更复杂的场景，下面我们以一个比较复杂的场景为例，讲解如何定义复杂的消息类型。 enum Sex { Unknown = 0; Male = 1; Female = 2; Other = 3; Alien = -1; } message School { string name = 1; string grade = 2; int64 graduated_at = 3; repeated string teachers = 4; } message Person { optional string name = 1; Sex sex = 3; int32 age = 2; float score = 4; map\u003cstring,bytes\u003e extra_data = 5; repeated School schools = 6; oneof contact { string email = 7; string phone = 8; } message Company { string name = 1; string address = 2; int32 salary = 3; repeated string employees = 4; } Company company = 9; } 在之前的 Person 基础上做了一个更复杂的消息结构，新增了学校、联系方式、公司三个字段，并且各个字段的类型并不相同，下面一个个进行讲解。 school 这个字段引入了两个特性，第一个是 repeated ，表示这个字段是一个数组，而数组的元素类型就是 repeated 之后的值 School。第二个特性是消息的嵌套，可以看到上面已经定义了一个 School 的消息，然后在Person 消息内嵌套使用。 contact 这个字段引入了 oneof 这个特性，oneof 可以看做是一个 switch 的语句，它的作用是根据 contact 字段的值，来选择使用哪个字段。你可以赋值 email 也可以赋值 phone 或者均不赋值，在生成的代码里，是有 GetEmail(), GetPhone 方法来获取这个字段的值。 company 字段引入了一个特性，也就是可以在消息内定义另一个消息并用在某个字段上。最终生成的代码里会有一个 Person_Company 的结构体，表示这个结构体属于 Person. 除此之外， name 字段也加了一个 option 的标识，在生成代码时会生成 *string 的类型，可以区分nil 和空值。 下面我们看一下，生成的代码（仅展现核心部分,忽略其他无关部分）： type School struct { // ...ignored... Name string `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"` Grade string `protobuf:\"bytes,2,opt,name=grade,proto3\" json:\"grade,omitempty\"` GraduatedAt int64 `protobuf:\"varint,3,opt,name=graduated_at,json=graduatedAt,proto3\" json:\"graduated_at,omitempty\"` Teachers []string `protobuf:\"bytes,4,rep,name=teachers,proto3\" json:\"teachers,omitempty\"` } type Person struct { // ...ignored... Name *string `protobuf:\"bytes,1,opt,name=name,proto3,oneof\" json:\"name,omitempty\"` Sex Sex `protobuf:\"varint,3,opt,name=sex,proto3,enum=api.user.session.v1.Sex\" json:\"sex,omitempty\"` Age int32 `protobuf:\"varint,2,opt,name=age,proto3\" json:\"age,omitempty\"` Score float32 `protobuf:\"fixed32,4,opt,name=score,proto3\" json:\"score,omitempty\"` ExtraData map[string][]byte `protobuf:\"bytes,5,rep,name=extra_data,json=extraData,proto3\" json:\"extra_data,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"` Schools []*School `protobuf:\"bytes,6,rep,name=schools,proto3\" json:\"schools,omitempty\"` // Types that are assignable to Contact: // *Person_Email // *Person_Phone Contact isPerson_Contact `protobuf_oneof:\"contact\"` // 注意这个字段 Company *Person_Company `protobuf:\"bytes,9,opt,name=company,proto3\" json:\"company,omitempty\"` } type isPerson_Contact interface { isPerson_Contact() } type Person_Email struct { Email string `protobuf:\"bytes,7,opt,name=email,proto3,oneof\"` } type Person_Phone struct { Phone string `protobuf:\"bytes,8,opt,name=phone,proto3,oneof\"` } func (*Person_Email) isPerson_Contact() {} func (*Person_Phone) isPerson_Contact() {} type Person_Company struct { // ...ignored... Name string `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"` Address string `protobuf:\"bytes,2,opt,name=address,proto3\" json:\"address,omitempty\"` Salary int32 `protobuf:\"varint,3,opt,name=salary,proto3\" json:\"salary,omitempty\"` Employees []string `protobuf:\"bytes,4,rep,name=employees,proto3\" json:\"employees,omitempty\"` } 关于 oneof 需要注意的时上面生成的 contact 的字段值 isPerson_Contact 是一个接口定义，它的实现是 Person_Email 和 Person_Phone 两个结构体。 而 Person 结构会同时生成一下代码，从而实现了 oneof 的功能： func (m *Person) GetContact() isPerson_Contact { if m != nil { return m.Contact } return nil } func (x *Person) GetEmail() string { if x, ok := x.GetContact().(*Person_Email); ok { return x.Email } return \"\" } func (x *Person) GetPhone() string { if x, ok := x.GetContact().(*Person_Phone); ok { return x.Phone } return \"\" } 2.2.2 项目内 proto 的引用 作为一个合格的程序员，代码是需要根据功能、类型等因素进行拆分的，每个文件/模块 负责一部分的逻辑，各个模块之间可以有相互的依赖关系。 因此引进来一个问题是，我不同的 proto 文件之间如何相互引用？如果有第三方的 proto 文件又怎么引入使用呢？ 答案是，pb 是支持 import 能力的。自己的 proto 文件之间可以互相引用，也可以引入其他 proto 文件。但是需要注意不要在不同 package 之间循环引用（写 go 的都知道这个是坑，不用过多解释）。 先说一下引入自己项目内的其他 proto 文件的情况。 假设我现在有两个 proto 文件，其路径入下： api |--user | |--user.proto |-- order | |--order.proto 而这个项目的 g","date":"2022-06-15","objectID":"/posts/microservices-protobuf/:2:2","tags":["微服务","系列篇","protobuf","grpc"],"title":"[系列]微服务·如何通过 protobuf 定义数据和服务","uri":"/posts/microservices-protobuf/"},{"categories":["microservice"],"content":"2.3 消息校验 在业务正常的业务开发中，我们需要对接口传参的数据进行数据合法性验证，一般是通过结构体注入 tag 的方式统一处理。大家最熟悉的应该是 github.com/go-playground/validator 这个包，通过在 tag 上定义验证规则，然后用统一的方法进行规则验证。用习惯了可以说是很方便，而且很多主流的http 框架也对这个库进行支持的（比如 gin）。 但是在基于 gRPC \u0026 pb 的场景下，这个库就只是个摆设了，因为代码是自动生成的，没办法改动，更没办法注入 tag 信息（当然不能说不行，你可以自己开发一个 protoc 的插件去做这个事儿，但是这个过程比你想想的要麻烦多，可以看一下 如何自定义 protoc 插件 这篇文件）。 所以想验证数据的合法性好像只能挨个字段去去判断，为了解决这个问题，出现另一个非常 nb 的插件 – github.com/envoyproxy/protoc-gen-validate。 该库定义了每个基础类型（包括 Google 提供 duration, timestamp 等类型）的验证规则，并生成对应的代码。使用时直接调用结构体的 Validate() 方法即可。 2.3.1 基础类型 对于基础类型，比如 int32、int64、string、bool 等等，会有大于小于等于，必须，非必须，空，非空等等的验证规则。 如： message UpdateUserRequest { string uid = 1 [(validate.rules).string = {min_len: 20, max_len: 24}]; string name = 2 [(validate.rules).string = {min_len: 2, max_len: 20, ignore_empty: true}]; string email = 3 [(validate.rules).string = {email: true,ignore_empty: true}]; string phone = 4 [(validate.rules).string = {pattern: \"^1[3-9]\\\\d{9}$\", ignore_empty: true}]; string avatar = 5 [(validate.rules).string = {max_len:128, ignore_empty: true}]; } 生成的代码比较多，就不再这里展示。但是生成代码逻辑是，一个个判断字段上的规则，不符合规则时，会返回很详细的错误信息，包括字段名，规则等，一眼就能看出哪个字段不符合哪个规则。 其他基础类型也类似，建议阅读官方文档或者直接看 proto 文件，因为 proto 文件比文档看起来更简单明了。 2.3.2 高级类型 对于 oneof, message 这种高级用法，他也有对应的检验规则，这里提一下 oneof。因为原生的 oneof 可以传其中一个字段或者不传，但是我们希望我定义了 n 个，你必选传其中一个，这个时候只需要在 oneof 上第一行加上 option (validate.required) = true; 即可。如： oneof id { // either x, y, or z must be set. option (validate.required) = true; string x = 1; int32 y = 2; Person z = 3; } 2.3.3 扩展类型 对于第三方包（如 google/protobuf/duration, google/protobuf/timestamps）也支持了规则配置，可以要求必传，可以要求传的值必须等于某个指定值或者是在一定的时间范围内。如： message config { // range [10ms, 10s] google.protobuf.Duration dial_timeout_sec = 3 [(validate.rules).duration = { gte: {nanos: 1000000, seconds: 0}, lte: {seconds: 10} }]; } 该包的能力比较强，由于篇幅只讲了几个类型，所以不再展示。这个库的潜力我个人认为是很大的，强烈推荐大家使用。 ","date":"2022-06-15","objectID":"/posts/microservices-protobuf/:2:3","tags":["微服务","系列篇","protobuf","grpc"],"title":"[系列]微服务·如何通过 protobuf 定义数据和服务","uri":"/posts/microservices-protobuf/"},{"categories":["microservice"],"content":"3. 服务定义 ","date":"2022-06-15","objectID":"/posts/microservices-protobuf/:3:0","tags":["微服务","系列篇","protobuf","grpc"],"title":"[系列]微服务·如何通过 protobuf 定义数据和服务","uri":"/posts/microservices-protobuf/"},{"categories":["microservice"],"content":"3.1 常规服务定义 聊了这么多 pb 中消息的定义，现在聊一聊 pb 中的服务定义，毕竟服务才是核心部分。 pb 中服务定义是定义一个服务和其下面的方法，而这些方法需要一个请求和一个响应。如： service UserService { rpc CreateUser(CreateUserRequest) returns (CreateUserResponse); // ... } message CreateUserRequest { Person person = 1; } message CreateUserResponse { string id = 1; } 这是一个最简的服务定义，其包含一个创建用户的方法，输入输出也分别定义了。需要注意的是，方法必须要有输入输出且不支持多个参数，如果需要多个参数，请嵌套一个结构体。如果方法没有返回值，则可以定义一个空的 message 即可。而我的做法是定义一个通用的 response，在没有返回值的方法返回这个 response，有返回值的方法则嵌套一层，response 作为参数。 // BaseResponse use as define response code and message message BaseResponse { Code code = 1; string reason = 2; string message = 3; } ","date":"2022-06-15","objectID":"/posts/microservices-protobuf/:3:1","tags":["微服务","系列篇","protobuf","grpc"],"title":"[系列]微服务·如何通过 protobuf 定义数据和服务","uri":"/posts/microservices-protobuf/"},{"categories":["microservice"],"content":"3.2 stream 流服务定义 处了上述的定义服务之外，还可以定义输入或输出位 stream 的方法，如： service UserService { rpc CreateUser1(stream CreateUserRequest) returns (CreateUserResponse); rpc CreateUser2(CreateUserRequest) returns (stream CreateUserResponse); rpc CreateUser3(stream CreateUserRequest) returns (stream CreateUserResponse); } 表示请求或响应可以是个 stream 流，而不同的 stream 的定义生成的代码也不一样，如(以 client 端代码为例)： // For semantics around ctx use and closing/ending streaming RPCs, please refer to https://pkg.go.dev/google.golang.org/grpc/?tab=doc#ClientConn.NewStream. type UserServiceClient interface { CreateUser1(ctx context.Context, opts ...grpc.CallOption) (UserService_CreateUser1Client, error) CreateUser2(ctx context.Context, in *CreateUserRequest, opts ...grpc.CallOption) (UserService_CreateUser2Client, error) CreateUser3(ctx context.Context, opts ...grpc.CallOption) (UserService_CreateUser3Client, error) } type UserService_CreateUser1Client interface { Send(*CreateUserRequest) error CloseAndRecv() (*CreateUserResponse, error) grpc.ClientStream } type UserService_CreateUser2Client interface { Recv() (*CreateUserResponse, error) grpc.ClientStream } type UserService_CreateUser3Client interface { Send(*CreateUserRequest) error Recv() (*CreateUserResponse, error) grpc.ClientStream } 三个方法返回的值均不一样，分别为：发送端为 stream 流，接收端为 stream 流，双向 stream 流。同样的 server 端实现这些方式时，也需要实现相应的接口。 ","date":"2022-06-15","objectID":"/posts/microservices-protobuf/:3:2","tags":["微服务","系列篇","protobuf","grpc"],"title":"[系列]微服务·如何通过 protobuf 定义数据和服务","uri":"/posts/microservices-protobuf/"},{"categories":["microservice"],"content":"3.3 服务定义中嵌套 http 定义 在 google/api/annotations.proto 库的支持下， pb 支持服务中嵌套 http 定义，如： import \"google/api/annotations.proto\"; service Hello { rpc Add(AddRequest) returns (AddResponse) { option (google.api.http) = { post: \"/api/hello/service/v1/add\" body: \"*\" }; } rpc Get(GetRequest) returns (GetResponse) { option (google.api.http) = { get: \"/api/hello/service/v1/get\" }; } } 可以通过 grpc-gateway (官方项目)生成对应的 http 接口并注册到 grpc-gateway 中。也可以通过其他插件去生成 http 代码。而 kratos 这个框架就做了这个事儿，单独生成 .http.go 文件，可以将生成的路由注册到 kratos 中。我之前也写过类似的插件，可以参考这篇文章：如何自定义 protoc 插件。 不管那种方式，最终目标都是多生产一套 http 接口，方便调试或者对外提供 grpc \u0026 http 服务。 ","date":"2022-06-15","objectID":"/posts/microservices-protobuf/:3:3","tags":["微服务","系列篇","protobuf","grpc"],"title":"[系列]微服务·如何通过 protobuf 定义数据和服务","uri":"/posts/microservices-protobuf/"},{"categories":["microservice"],"content":"4. 总结 到这里本篇文章就结束了，基本讲完我对 pb 的理解和使用上遇到的经验都写出来了。当然由于篇幅原因，没有讲述太多 grpc 相关的问题，因为 grpc 也算是个大头，我想以后单独写一篇讲述 grpc 的原理和通信以及使用的文章。 本篇主要讲述了： pb 的定义和解决的问题 pb 的基础类型定义和花样玩法 pb 类型的数据校验（介绍了一个第三方库：protoc-gen-validate） pb 定义普通服务 pb 定义 stream 流服务 pb 定义 http 服务 如果你有任何问题或者有不一样的想法，请通过评论区或者邮件联系我。 如果在使用 pb 过程中有什么不明白的 本文中的知识点，大部分都是我在写项目的时候积累下来的，如果你有什么不明白的地方，可以参考我的一个项目: goim/api。 本文提到的能力我在这个项目基本都用到了，你可以同时看代码和本文，应该对你有一定的帮助。 ","date":"2022-06-15","objectID":"/posts/microservices-protobuf/:4:0","tags":["微服务","系列篇","protobuf","grpc"],"title":"[系列]微服务·如何通过 protobuf 定义数据和服务","uri":"/posts/microservices-protobuf/"},{"categories":["microservice"],"content":"5. 链接🔗 系列篇 如何自定义 protoc 插件 https://developers.google.com/protocol-buffers/docs/overview https://www.grpc.io/docs/what-is-grpc/introduction/ https://github.com/envoyproxy/protoc-gen-validate ","date":"2022-06-15","objectID":"/posts/microservices-protobuf/:5:0","tags":["微服务","系列篇","protobuf","grpc"],"title":"[系列]微服务·如何通过 protobuf 定义数据和服务","uri":"/posts/microservices-protobuf/"},{"categories":["k8s"],"content":" 本篇为转载文章，原文章链接：https://ying-zhang.github.io/cluster/2015-borg-eurosys-cn/ Large-scale cluster management at Google with Borg Abhishek Vermay, Luis Pedrosaz, Madhukar Korupolu David Oppenheimer, Eric Tune, John Wilkes. Google Inc. EuroSys, 2015. 译者：难易（Simpson） 修订：Ying ZHANG. 2017-11; 2018-06 https://dl.acm.org/doi/10.1145/2741948.2741964或https://research.google/pubs/pub43438/或 https://pdos.csail.mit.edu/6.824/papers/borg.pdf 译文 PDF 文件 说明：我是基于原译者的文本修订的，但最终改动太多，就不标注了。最近又看到 深度译文｜Google的大规模集群管理系统Borg - 王勇桥 。可以通过摘要对比一下。 Google的Borg系统是一个集群管理器。它在多个万台机器规模的集群上运行着来自几千个不同应用的几十万个作业。Borg通过准入控制、高效的任务装箱、超售、机器共享、以及进程级别的性能隔离，实现了高利用率。它为高可用应用提供了可以减少故障恢复时间的运行时特性，以及降低关联故障概率的调度策略。Borg提供了声明式的作业描述语言、域名服务集成、实时作业监控、分析和模拟系统行为的工具。这些简化了用户的使用。 本文介绍了Borg系统架构和特性，重要的设计决策，对某些策略选择的定量分析，以及十年来的运营经验和教训。 难易： 谷歌的Borg系统群集管理器运行几十万个以上的jobs，来自几千个不同的应用，跨多个集群，每个集群有上万个机器。它通过管理控制、高效的任务包装、超售、和进程级别性能隔离实现了高利用率。它支持高可用性应用程序与运行时功能，最大限度地减少故障恢复时间，减少相关故障概率的调度策略。Borg简化了用户生活，通过提供一个声明性的工作规范语言，名称服务集成，实时作业监控，和分析和模拟系统行为的工具。 我们将会展现Borg系统架构和特点，重要的设计决策，定量分析它的一些策略，和十年以来的运维经验和学到的东西。王： Google的Borg系统是一个运行着成千上万项作业的集群管理器，它同时管理着很多个应用集群，每个集群都有成千上万台机器，这些集群之上运行着Google的很多不同的应用。 Borg通过准入控制，高效的任务打包，超额的资源分配和进程级隔离的机器共享，来实现超高的资源利用率。 它通过最小化故障恢复时间的运行时特性和减少相关运行时故障的调度策略来支持高可用的应用程序。Borg通过提供一个作业声明的标准语言，命名服务的集成机制，实时的作业监控，以及一套分析和模拟系统行为的工具来简化用户的使用。 我们将通过此论文对Borg系统的架构和主要特性进行总结，包括重要的设计决定，一些调度管理策略的定量分析，以及对十年的使用经验中汲取的教训的定性分析。 Google’s Borg system is a cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. It achieves high utilization by combining admission control, efficient task-packing, over-commitment, and machine sharing with process-level performance isolation. It supports high-availability applications with runtime features that minimize fault-recovery time, and scheduling policies that reduce the probability of correlated failures. Borg simplifies life for its users by offering a declarative job specification language, name service integration, real-time job monitoring, and tools to analyze and simulate system behavior. We present a summary of the Borg system architecture and features, important design decisions, a quantitative analysis of some of its policy decisions, and a qualitative examination of lessons learned from a decade of operational experience with it. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). EuroSys'15, April 21-24, 2015, Bordeaux, France. Copyright is held by the owner/author(s). ACM 978-1-4503-3238-5/15/04. https://doi.org/10.1145/2741948.2741964 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:0:0","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"1. 简介 我们内部称为Borg的集群管理系统，负责接收、调度、启动、重启和监控Google所有的应用。本文介绍它是如何实现的。 Borg提供了三个主要的好处：（1）隐藏资源管理和故障处理细节，使用户可以专注于应用开发；（2）高可靠和高可用的运维，并支持应用程序也能够如此；（3）让我们可以在几万台机器上高效地运行负载。Borg不是第一个涉及这些问题的系统，但它是少有的运行在如此大规模，具有弹性且完整的系统之一。 本文围绕这些主题来编写，总结了十多年来我们在生产环境运行Borg的一些定性观察。 图1. Borg的架构。图中只画出了数千个工作节点的很小一部分 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:1:0","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"2. 用户视角 Borg的用户是Google的开发人员以及运行Google应用和服务的系统管理员（站点可靠性工程师，SRE）。用户以作业（Job）的方式将他们的工作提交给Borg。作业由一个或多个任务（Task）组成，每个任务执行相同的二进制程序。每个作业只运行在一个Borg单元（Cell）里。Cell是一组机器的管理单元。下面的小节将介绍用户视角看到的Borg系统的主要特性。 SRE的职责比系统管理员多得多：他们是负责Google生产服务的工程师。他们也设计和实现包括自动化系统等软件，管理应用、服务基础设施和平台，以保证在Google如此大的规模下的高性能和高可靠性。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:2:0","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"2.1 工作负载 Borg Cell主要运行2种异构的工作负载。第一种是应该“永不”停止的长期运行的服务，处理持续时间较短但对延迟敏感的请求（从几微秒到几百毫秒）。这些服务用于面向最终用户的产品，如Gmail、Google Docs、网页搜索，以及内部基础设施服务（例如Bigtable）。第二种是批处理作业，执行时间从几秒到几天，对短期性能波动不敏感。这2种负载在不同Cell中的比例不同，取决于其主要租户（例如，有些Cell就以批处理作业为主）。工作负载也随时间变化：批处理作业不断提交或结束，而很多面向终端用户的服务表现出昼夜周期性的使用模式。Borg需要都处理好这些情况。 Borg的代表性负载是一个公开的2011年5月整月的记录数据集[80]。这个数据集已经得到了广泛的分析[1, 26, 27, 57, 68]。 最近几年，以Borg为基础构建了很多应用框架，包括我们内部的MapReduce系统[23]、FlumeJava[18]、Millwheel[3]和Pregel[59]。这些框架大多有一个控制器来提交Master Job，还有多个Worker Job。前两个框架类似于YARN的应用管理器[76]。我们的分布式存储系统，例如GFS[34]和它的后继者CFS、Bigtable[19]、以及Megastore[8]，都是运行在Borg上的。 本文中，我们把高优先级的Borg作业称为为生产作业（prod），其它的则是非生产的（non-prod）。大多数长期服务是prod的，大部分批处理作业是non-prod的。一个典型Cell里，prod作业分配了约70%的总CPU资源，占总CPU使用量约60%；分配了约55%的总内存资源，占总内存使用量约85%。§5.5 节表明分配量和使用量的差异是值得注意的。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:2:1","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"2.2 集群（Cluster）和单元（Cell） 一个Cell里的机器属于同一个集群。集群由数据中心级的高性能光纤的组网来定义。一个集群位于数据中心的一栋建筑内，而一个数据中心有多栋建筑原注1: 这些关系会有少数例外情况。一个集群通常包括一个大的Cell，还可能有一些小规模的测试用或其它特殊用途的Cell。我们尽力避免任何单点故障。 不计测试用的Cell，中等规模的Cell约有一万台机器；有些Cell还要大得多。Cell中的机器从多个维度看都是异构的：大小（CPU、内存，硬盘，网络）、处理器类型、性能、以及是否有外网IP地址或SSD等。Borg负责决定任务在Cell中的哪些机器上执行、为其分配资源、安装程序及依赖、监控健康状态并在失败后重启，从而使用户几乎不必关心机器异构性。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:2:2","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"2.3 作业（Job）和任务（Task） 一个Borg作业的属性有：名称、拥有者和任务个数。作业可以有一些约束来强制其任务运行在有特定属性的机器上，比如处理器架构、操作系统版本、是否有外网IP地址等。约束可以是硬性的或者柔性的，柔性约束表示偏好，而非需求。一个作业可以推迟到前一个作业结束后再开始。一个作业只在一个Cell中运行。 每个任务对应着一组Linux进程，运行在一台机器上的一个容器内[62]。绝大部分Borg的工作负载没有运行在虚拟机里，因为我们不想付出虚拟化的开销。而且，在Borg设计的时候，我们有很多处理器还没有硬件虚拟化功能呢。 任务也有一些属性，如资源需求量，在作业中的序号等。一个作业中的任务大多有相同的属性，但也可以被覆盖 ——例如特定任务的命令行参数。各维度的资源（CPU核、内存、硬盘空间、硬盘访问速度、TCP端口等原注2: Borg负责管理一台机器上的可用端口并将其分配给任务）。可以互相独立的以细粒度指定。我们不强制使用固定大小的资源桶或槽（见§5.4）。Borg运行的程序都是静态链接的，以减少对运行环境的依赖，这些程序组织成由二进制文件和数据文件构成的包，由Borg负责安装。 用户通过向Borg发送RPC来控制作业。RPC大多是从命令行工具、其它作业、或我们的监控系统（§2.6）发出的。大多作业描述文件使用一种声明式配置语言BCL。BCL是GCL[12]的一个变种，即增加了一些Borg专有的关键字，而GCL会生成若干protobuf文件[67]。GCL还提供了匿名函数以支持计算，这样就能让应用根据环境调整自己的配置。有上万个超过一千行的BCL配置文件，系统中累计有数千万行BCL。Aurora的配置文件与Borg的作业配置[6]类似。 图2展示了作业和任务整个生命周期的状态变化。 图2. 作业和任务的状态图。用户可以触发提交，杀死和更新操作 要想在运行时改变一个作业中若干或全部任务的属性，用户可以向Borg提交一个新的作业配置，并命令Borg将任务更新到新的配置。更新是轻量的，非原子性的事务，在事务结束（提交）之前可以很容易地撤销。更新通常是滚动执行的，而且可以限制由更新导致的任务中断（被重新调度或抢占）的数量；超过限值后，变更将会被跳过。 一些任务更新（如更新二进制程序）需要重启任务；另外一些更新（如增加资源需求或修改约束）可能使该任务不适合运行在当前机器上，导致停止并重新调度该任务；还有一些更新（如修改优先级）总是可以进行的，不需要重启或者移动任务。 任务可以要求在被Unix的SIGKILL信号立即杀死之前获得SIGTERM信号通知，这样它们还有时间清理资源、保存状态、结束当前请求、拒绝新请求。但如果抢占者设置了延迟限值，就可能来不及发通知。实践中，80%的情况下能发出通知信号。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:2:3","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"2.4 分配（Allocs） Borg的alloc（allocation的缩写）是一台机器上的预留资源，可以用来执行一个或多个任务；不管有没有被使用，这些资源都算分配出去了。Allocs可以给将来的任务预留资源，或在任务暂停和重启的间隔保持资源，以及将不同作业的多个任务绑定在同一台机器上——例如一个Web服务器实例和附加的将其URL日志从本机硬盘拷贝到分布式文件系统的日志转存任务。Alloc像一台机器那样来管理；运行在同一个Alloc内的多个任务共享其资源。如果一个Alloc需要迁移到其它机器上，那么它的任务也要跟着重新调度。 一个Alloc集合，即一组在多台机器上预留了资源的Alloc，类似于一个作业。一旦创建了一个Alloc集合，就可以向其提交若干作业。简便起见，我们用任务表示一个Alloc或者一个顶层任务（即运行在Alloc之外的任务），用作业表示一个普通作业或者Alloc集合。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:2:4","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"2.5 优先级、配额和准入控制 当出现超过系统容量的工作负载会产生什么情况？我们对此的解决方案是优先级和配额。 每个作业都有一个小的正整数表示的优先级。高优先级的任务可以优先获得资源，甚至抢占（杀死）低优先级的任务。Borg为不同用途定义了不重叠的优先级区间，包括（优先级降序）：监控、生产、批处理、尽力（即测试的或免费的）。本文中，prod作业的优先级包括监控和生产两个区间。 虽然一个被抢占的任务通常会被重新调度到Cell的其它机器上，但级联抢占也可能发生：如果某个任务抢占了一个优先级稍低的任务，而后者又抢占了另一个优先级稍低的，如此继续。为避免这种情况，我们禁止生产区间的任务互相抢占。细粒度的优先级在其它场景下也很有用 —— 如MapReduce的Master任务的优先级比其管理的Worker高一点，以提高其可靠性。 优先级表示了Cell中运行或等待的作业之间的相对重要性。配额（Quota）则用来决定准许哪个作业可以被调度。配额是特定优先级和时间段（典型是几个月）的一个资源向量（CPU，内存，硬盘等）。配额限制了用户的作业一次可以申请资源的最大数量（如：20TB内存，prod优先级，从现在到7月末，在xx Cell内）。配额检查是准入控制的一部分，而不是调度的：配额不足的作业提交时当即就会被拒绝。 高优先级的配额比低优先级的成本要高。生产级的配额限定于一个Cell的物理资源量。因此，用户提交了不超过配额的生产级作业时，不考虑资源碎片和约束，可以预期这个作业一定会运行。尽管我们鼓励用户不要购买超过其需求的配额，但很多用户仍然超买了，这样他们就不用担心由于将来应用用户量增长可能导致的配额短缺。我们的应对方案是对低优先级资源配额的超售：所有用户的0优先级配额是无限的，尽管这无法实现。低优先级的作业虽然被接收了，但可能由于资源不足而一直等待。 配额分配是Borg之外的系统处理的，与我们的物理容量规划紧密相关。容量规划的结果反映在各数据中心的价格和可用配额上。只有在其要求的优先级有足够的配额，用户的作业才能被接收。采用配额使得主导资源公平性（DRF）[29, 35, 36, 66]这样的策略不是那么必要了。 Borg的容量系统可以给某些用户一些特殊权限。例如，允许管理员删除或修改Cell里的任意作业，或者允许某个用户操作特定的内核特性或Borg行为（如对其作业禁用资源估计。§5.5）。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:2:5","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"2.6 域名和监控 仅仅创建和部署任务是不够的：一个服务的客户端和其它系统需要能找到它们，即使该服务被重新部署到另一台机器之后。为实现该需求，Borg为每个任务创建了一个固定的BNS域名（BNS，Borg name Service），这个域名包括了Cell名，作业名称和任务序号。Borg把任务的主机名和端口写入Chubby[14]的一个持久化高可用文件里，以BNS域名为文件名。这个文件被RPC用来发现任务的实际地址。BNS域名也是任务的DNS域名的一部分，例如，cc Cell的ubar用户的jfoo 作业的第50个任务可以通过50.jfoo.ubar.cc.borg.google.com来访问。每当状态改变时，Borg还会把作业的大小和任务的健康信息写入到Chubby，这样负载均衡器就知道如何路由请求了。 几乎每个任务都有一个内置的HTTP服务器，用来发布任务的健康信息和几千个性能指标（如RPC延时）。Borg监控这些健康检查的URL，重启那些没有立刻响应或返回HTTP错误码的任务。监控工具跟踪其它数据并显示在仪表盘上，当违反服务水平目标（SLO）时报警。 用户可以使用一个称为Sigma的Web界面来检查他的所有作业的状态，针对某个Cell，或者深入某个作业及任务，检查其资源使用行为、详细日志、执行历史和最终结果。我们的应用产生大量的日志，它们都会被自动的滚动以避免耗尽硬盘空间。任务退出后，日志会保留一小段时间以帮助调试。如果一个作业没有运行起来，Borg会提供一个挂起原因的标注，以及建议如何修改作业的资源请求，以使其更适合Cell。我们发布了如何使资源请求更容易被调度的指南。 Borg将所有的作业提交、任务事件、以及每个任务的详细资源使用都记录在Infrastore里。Infrastore是一个可扩展的只读数据存储，通过Dremel[61]提供了类似SQL的交互式接口。这些数据用以支持基于使用量的收费，调试作业和系统故障，以及长期容量规划。公开的Google集群负载数据集[80]也来自于这些数据。 所有这些特性帮助用户理解和调试Borg及其作业的行为，并帮助我们的SRE实现每人管理超过上万台机器。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:2:6","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"3. Borg架构 一个Borg的Cell包括一组机器，一个逻辑上集中的控制器，称为Borgmaster，以及运行在每台机器上的称为Borglet的代理进程（见图1）。Borg的组件都是用C++实现的。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:3:0","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"3.1 Borgmaster Cell的Borgmaster由两个进程组成：Borgmaster主进程和一个单独的调度进程（§3.2）。Borgmaster主进程处理客户端的RPC，包括修改状态（如创建作业），或提供只读数据（如查找作业）。它还管理着系统中所有对象（机器、任务、Allocs等）的状态，与Borglet通信，并提供一个Web UI（作为Sigma的备份）。 Borgmaster在逻辑上是单个进程，但实际上有5个副本。每个副本在内存维护着Cell状态的拷贝，该状态同时保存在由这些副本的本地硬盘组成的一个基于Paxos[55]的高可用、分布式存储上。每个Cell中仅有一个选举出来的Master，它同时作为Paxos的Leader和状态修改者，处理所有变更Cell状态的请求，例如提交作业或者结束某台机器上的一个任务。当Cell启动或者上一个Master故障时，新的Master会通过Paxos算法选举出来；新Master会获取一个Chubby锁，这样其它的系统就可以找到它。选举并转移到新的Master通常需要10秒，但在大的Cell里可能需要长达1分钟，因为需要重构一些内存状态。当一个副本从宕机恢复后，它会动态地从其它最新的Paxos副本中重新同步自己的状态。 某个时刻的Borgmaster状态被称为检查点（Checkpoint），以定期快照加变更日志的形式保存在Paxos存储里。检查点有很多用途：如重建过去任意时刻的Borgmaster状态（例如，在接收一个触发了Borg故障的请求之前的时刻，这样就可以用来调试）；特别情况下可以手工修复检查点；构建一个持久的事件日志供日后查询；或用于离线仿真。 一个高保真的Borgmaster模拟器，称为Fauxmaster，可以读取检查点文件。Fauxmaster的代码拷贝自线上的Borgmaster，还有对Borglet的存根接口。它接收RPC来改变状态，执行操作，例如“调度所有等待的任务”。我们用它来调试故障，像跟在线的Borgmaster那样与模拟器交互，用模拟的Borglet重放检查点文件里的真实交互。用户可以单步执行并观察系统过去确实发生了的状态变化。Fauxmaster也用于容量规划（可以接收多少个此类型的作业？），以及在实际更改Cell配置前做可行性检查（这个变更会导致关键作业异常退出吗？） ","date":"2022-05-31","objectID":"/posts/brog-in-google/:3:1","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"3.2 调度 当提交一个作业后，Borgmaster会把它保存在持久的Paxos存储上，并将这个作业的所有任务加入等待队列中。调度器异步地扫描等待队列，将任务分配到满足作业约束且有足够资源的机器上（调度是针对任务的，而非作业）。队列扫描从高优先级到低优先级，同优先级则以轮转的方式处理，以保证用户间的公平，并避免队首的大型作业阻塞队列。调度算法有两个部分：可行性检查，找到一组可以运行任务的机器；评分，从中选择一个合适的机器。 在可行性检查阶段，调度器会找到一组满足任务约束且有足够可用资源的机器 —— 可用资源包括已经分配给低优先级任务但可以抢占的资源。在评分阶段，调度器确定每台可行机器的适宜性。评分考虑了用户特定的偏好，但主要取决于内建的标准：例如最小化被抢占任务的个数和优先级，选择已经有该任务安装包的机器，尽可能使任务分散在不同的供电和故障域，以及装箱（Packing）质量（在单台机器上混合高、低优先级的任务，以允许高优先级任务在负载尖峰扩容）等。 Borg早期使用修改过的E-PVM[4]算法来评分。这个算法对异构的资源生成等效的成本值，放置任务的目标是使成本的变化量最小。在实践中，E-PVM会把负载分散到所有机器，为负载尖峰预留出资源 —— 这样的代价是增加了碎片，特别是对需要大部分机器的大型任务而言；我们有时称其为“最差匹配”。 与之相反的是“最佳匹配”，把机器上的任务塞的越满越好。这就“空”出一些没有用户作业的机器（它们仍运行存储服务），这样放置大型任务就比较直接了。但是，如果用户或Borg错误估计了资源需求，紧实的装箱会对此造成（性能上的）惩罚。这种策略不利于有突发负载的应用，而且对申请少量CPU的批处理作业特别不友好，这些作业申请少量CPU本来是为了更容易被调度执行，并抓住机会使用空闲资源：20%的non-prod 任务申请少于0.1个CPU核。 我们目前的评分模型是混合的，试图减少搁浅（Stranded）的资源（指某些类型资源分配完之后，一台机器上无法分配的其它类型资源）。对我们的负载而言，这个模型比“最佳匹配”提升了3%-5%的装箱效率（以[78]定义的方式评价）。 如果评分后选中的一台机器仍没有足够的资源来运行新任务，Borg会抢占低优先级的任务，从最低优先级向上逐级抢占，直到资源足够运行该任务。被抢占的任务放回到调度器的等待队列里，而不是被迁移或休眠原注3: 例外情况是，为Google Compute Engine提供虚拟机的任务会被迁移。 任务的启动延迟（从提交作业到任务开始运行之间的时间段）是我们持续重点关注的。这个时间差别很大，中位数约25秒。安装软件包耗费了其中80%的时间：一个已知的瓶颈就是软件包写入时对本地硬盘的竞争。为了减少任务启动时间，调度器偏好将任务分配到已经有必需的软件包（程序及数据）的机器：大部分包是只读的，所以可以被共享和缓存（这是Borg调度器唯一的一种数据局部性支持）。另外，Borg通过树形和类似BT的协议并发地将软件包分发到多个机器上。 此外，调度器采用多种技术使其能够扩展到数万台机器的Cell（§3.4）。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:3:2","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"3.3 Borglet Borglet是部署在Cell每台机器上的本机Borg代理。它负责启动和停止任务；重启失败的任务；通过OS内核设置来管理本地资源；滚动调试日志；把本机的状态上报给Borgmaster和其它监控系统。 Borgmaster每过几秒就会轮询每个Borglet来获取机器的当前状态，并向其发送请求。这让Borgmaster能控制通信频率，省去了显式的流量控制机制，而且防止了恢复风暴[9]。 选举出来的Master负责准备发送给Borglet的消息，并根据Borglet的响应更新Cell的状态。为使性能可扩展，每个Borgmaster副本会负责一个无状态的链接分片（Link Shard）来处理部分Borglet的通信；Borgmaster选举后重新计算链接的分片。为了保证弹性（Resiliency），Borglet总是汇报全部状态，但是Link Shard只汇报变化值，从而聚合、压缩这些信息，减少Master更新的负担。 如果某个Borglet几次没有响应轮询请求，该机器会被标记为宕机，其上运行的所有任务会被重新调度到其它机器。如果通讯恢复了，Borgmaster会让这个Borglet杀掉已经被重新调度出去的任务，以避免重复。即便无法与Borgmaster通信，Borglet仍会继续正常运行。所以即使所有的Borgmaster都出故障了，正在运行的任务和服务还会保持运行。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:3:3","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"3.4 扩展性 我们还没有遇到Borg这种集中式架构的终极扩展上限。我们顺利地突破了遇到的每个限制。一个单独的Borgmaster可以管理有数千台机器的Cell，有些Cell每分钟有10000多个任务到达。一个繁忙的Borgmaster使用10~14个CPU核以及50GB内存。我们用了几项技术来实现这种扩展性。 早期版本的Borgmaster使用一个简单的，同步的循环来处理请求、调度任务，并与Borglet通信。为了处理更大的Cell，我们把调度器分离为一个单独的进程，这样它就可以与其它的Borgmaster功能并行执行，而这些其它的功能有多副本以便容错。调度器使用一份缓存的Cell状态拷贝，重复执行下面的操作：从选举出来的Master获取状态变更（包括已分配的和等待中的工作）；更新自己的本地拷贝；执行一轮调度来分配任务；将分配信息发送给Master。Master会接受并应用这些分配，但如果分配不适合（例如，是基于过时的状态做出的），就会等待调度器的下一轮调度。这与Omega[69]使用的乐观并发控制思路很相似，而且我们最近还给Borg添加了对不同负载类型使用不同调度器的功能。 为了改进响应时间，Borglet使用独立的线程分别进行通信和响应只读RPC。为了更好的性能，我们将这些请求划分给5个Borgmaster副本（§3.3）。总的效果是，UI响应时间的99%分位数小于1秒，而Borglet轮询间隔的95%分位数小于10秒。 一些提高Borg调度器扩展性的方法如下： 缓存评分：计算一台机器的可行性和评分是比较昂贵的，所以Borg会一直缓存评分，直到这台机器或者任务的属性发生了变化——例如，这台机器上的某个任务结束了，一些属性修改了，或者任务的需求改变了。忽略小额的资源变化可以减少缓存失效。 任务等效类（Equivalence classes）：一般来说，同一个Borg作业的任务有相同的请求和约束。任务等效类即一组有相同需求的任务。Borg只对等效类中的一个任务进行可行性检查和评分，而不是对等待的每个任务去检查一遍所有机器的可行性并对可行的机器评分。 适度随机：在一个大的Cell中，对所有机器都去计算一遍可行性和评分是很浪费的。调度器会随机地检查机器，直到找到足够多的可用机器来评分，然后从中挑选出最好的一个。这减少了任务启动和退出所需的评分次数及导致的缓存失效，加快了任务分配过程。适度随机有点类似Sparrow[65]的批量采样技术，但Borg还处理了优先级、抢占、异构性和安装软件包的成本。 在我们的实验中（§5），从零开始调度整个Cell的工作负载只要几百秒，但禁用上面几项技术的话，3天都不够。正常情况下，半秒之内就能完成一遍等待队列的在线调度。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:3:4","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"4. 可用性 图3. 不同类型任务的异常退出率及原因 包括抢占、资源不足、机器故障、机器关机、其它。数据从2013-08-01开始大型系统里故障是很常见的[10, 11, 12]。图3展示了在15个样本Cell里任务异常退出的原因分类。在Borg上运行的应用需要能处理这种事件，可采用的技术有多副本、保存持久状态到分布式存储，或定期快照（如果可行的话）等。当然，我们也尽可能的缓解异常事件的影响。例如，Borg提供了： 自动重新调度异常退出的任务，如果必要，可以放置到另一台机器上去运行 把一个作业的任务分散到不同的可用域，例如机器、机架、供电域层次，以减少关联失效 在机器/OS升级等维护活动期间，限制任务受影响的速率，以及同一作业中同时中止的任务的个数 使用声明式的预期状态表示，及幂等的变更操作，这样故障的客户端可以无损地重复提交故障期间漏掉的请求 对于失联的机器上的任务，限制重新调度的速率，因为大规模的机器故障和网络分区是很难区分的 回避造成崩溃的 $\\langle$任务，机器$\\rangle$ 组合 通过不断重新执行日志保存任务（§2.4），恢复已写入本地硬盘的关键中间数据，就算这个日志关联的Alloc已经终止或调度到其它机器上了。用户可以设置系统持续重复尝试多久，通常是几天时间。 Borg的一个关键设计特性是：就算Borgmaster或者Borglet退出了，已经运行的任务还会继续运行下去。不过，保持Master正常运行仍然重要，因为在它退出后就无法提交新的作业，无法更新运行作业的状态，也不能重新调度故障机器上的任务。 Borgmaster使用多项的技术支持其获得99.99%的实际可用性：多副本应对机器故障；准入控制应对过载；使用简单、底层的工具部署实例，以减少外部依赖。Cell彼此是独立的，减少了关联误操作和故障传播的机会。同时这也是我们不扩大Cell规模的主要考虑，而并非是受限于扩展性。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:4:0","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"5. 利用率 Borg的一个主要目标就是有效地利用Google的大量机器（这是一大笔财务投资）：让效率提升几个百分点就能省下几百万美元。这一节讨论和评估了一些Borg使用的策略和技术。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:5:0","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"5.1 评估方法 作业有部署约束，而且需要处理负载尖峰（尽管比较少见）；机器是异构的；我们回收服务型作业的资源来运行批处理作业。因此，我们需要一个比“平均利用率”更高级的指标来评估我们的策略选择。大量实验后，我们选择了Cell压缩量（Compaction）：给定一个负载，我们不断地移除机器，直到无法容纳该负载，从而得知所需最小的Cell规模。从空集群开始部署该负载并重复多次，以减少特殊情况的影响。终止条件是明确的，对比可以自动化，避免了生成和建模合成负载的陷阱[31]。[78]提供了评估技术的定量比较，其中的细节非常微妙。 我们不可能在线上Cell进行实验，但是我们用了Fauxmaster来获得高保真的模拟效果，它使用了真实生产Cell和负载的数据，包括所有约束、实际限制、预留和使用量数据（§5.5）。实验数据提取自2014-10-01 14:00 PDT的Borg快照（其它快照也有类似的结论）。我们首先排除了特殊用途的、测试用的、小型的（少于5000台机器）的Cell，然后从剩下的Cell中选取了15个样本，抽样尽量关于Cell的大小均匀分布。 为了保持机器异构性，在Cell压缩实验中，我们随机地移除机器。为了保持工作负载的异构性，我们保留了所有负载（除了那些绑定到特定机器的服务和存储任务，如Borglet）。我们把那些需要超过原Cell大小一半的作业的硬性限制改成柔性的，允许不超过0.2%的任务一直等待，这是针对一些特别“挑剔”的，只能放置在很少的特定机器上的任务；充分的实验表明结果是可复现的，波动很小。如果需要一个大型的Cell，就把原Cell复制几倍；如果需要更多的Cell，也是复制原Cell。 每个实验都用不同的随机数种子对每个Cell重复了11次。图中，我们用误差线线来表示所需机器数量的最大和最小值，选择90%分位数作为结果——平均值或中位数不能反映系统管理员所期望的充分把握。我们认为Cell压缩率是一个公平一致的比较调度策略的方法，而且可以直接转化为成本/收益的结果：更好的策略只需要更少的机器来运行相同的负载。 我们的实验关注于即时的调度（装箱），而不是重放一段长时间的负载记录。部分原因是避免处理开放或闭合的队列模型[71, 79]的困难；部分是传统的完成时间不适用于长时间运行的服务；部分是这样可以提供明确的比较结果；部分是因为我们认为不会对结果产生显著影响；还有部分现实原因，我们发现一次实验使用了20万个Borg CPU核 —— 即便对Google而言，这个成本也不是个小数目。 图4. 压缩的效果。15个Cell在压缩后相比原规模的百分比累积分布（CDF） 生产环境中，我们特意保留了一些裕度（Headroom），以应对负载增长、偶然的“黑天鹅”事件、负载尖峰、机器故障、硬件升级、以及大范围的局部故障（如供电母线短路）。图4显示了如果应用Cell压缩，实际的Cell可以压缩到多小。下文的图使用这些压缩后的大小作为基准值。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:5:1","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"5.2 Cell共享 几乎所有的机器都同时运行prod和non-prod的任务：在共享的Cell里是98%的机器，在所有Borg管理的机器里是83%（有一些Cell是专用的）。 图5. 将prod和non-prod工作划分到不同的集群将需要更多的机器。两幅图中的百分比都是相对于单个集群所需机器的最少数量而言的 鉴于很多外部组织将面向用户的作业和批处理作业分别运行在不同的集群上，我们检查一下如果我们也这么做会怎样。图5表明，在一个中等大小的Cell上，分开运行prod和non-prod的工作负载将需要增加20-30%的机器。这是因为prod的作业通常会保留一些资源来应对极少发生的负载尖峰，但大多情况下用不到这些资源。Borg回收了这些用不到的资源（§5.5），来运行non-prod的工作，所以总体我们只需要更少的机器。 图6. 将用户分开到不同的集群也会需要更多的机器 大部分Borg Cell被数千个用户共享使用。图6展示了为什么要共享。测试中，如果一个用户消费了超过10TiB（或100TiB）的内存，我们就把这个用户的工作负载分离到另一个Cell中。我们目前的共享策略是有效的：即使100TiB的阈值，也需要2-16倍的Cell，增加20-150%的机器。将资源池化再次显著地节省了成本。 但是，把很多不相关的用户和作业类型放置到同一台机器上可能会造成CPU冲突，我们是否需要更多的机器来补偿？为评估这一点，我们看一下固定机器类型和时钟频率，任务的CPI（Cycles per Instruction，执行每条指令平均所需时钟数，越大则程序执行越慢）在其它环境条件不同的影响下是如何变化的。在这种实验条件下，CPI是一个可比较的指标，而且可以表征性能冲突，因为2倍的CPI意味着一个CPU密集型程序需要2倍的执行时间。数据是在一周内从约12000个随机选择的prod任务获取的，使用了[83]中介绍的硬件剖析工具记录5分钟内的时钟数和指令数，并调整了采样的权重，使CPU时间的每秒都均等处理。结果并非直截了当的： 我们发现CPI在同一个时间段内和下面两个量正相关：这台机器上总的CPU使用量，以及这个机器上同时运行的任务个数（基本上独立）；每向一台机器上增加一个任务，就会使其它任务的CPI增加0.3%（从数据拟合的线性模型给出的预测值）；将一台机器的CPU使用量增加10%，就会增加2%弱的CPI。尽管相关性在统计意义上是显著的，也只是解释了CPI变化的5%。还有其它的因素，支配着CPI的变化，例如，应用程序固有的差别，以及特殊的干扰模式[24, 83]。 比较从共享Cell和只运行几种应用的少数专用Cell获取的CPI采样，我们看到共享Cell里的CPI平均值为1.58（σ=0.35，标准差），专用Cell的CPI平均值是1.53（σ=0.32）—— 也就是说，共享Cell的性能差3%。 为了搞定不同Cell的应用会有不同的工作负载，或者会有幸存者偏差（或许对冲突更敏感的程序会被挪到专用Cell里面去），我们观察了Borglet的CPI。所有Cell的所有机器上都运行着Borglet。我们发现专用Cell里Borlet的CPI平均值是1.20（σ=0.29），而共享Cell里的CPI平均值为1.43（σ=0.45），表明在专用Cell上比在共享Cell上快1.19倍，不过这个结果忽略了专用Cell中的机器负载较轻的因素，即稍偏向专用Cell。 这些实验表明了仓库级别的性能比较是复杂的，强化了[51]中的观察，但也说明共享并没有显著增加运行程序的开销。 不过，就算从结果中最差的数据来看，共享还是有益的：比起CPU的降速，共享比各个划分方案都减少了机器，这一点更重要，而且共享的收益适用于包括内存和硬盘等各种资源，不仅仅是CPU。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:5:2","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"5.3 大型Cell 图7. 将Cell分成更小的规模将需要更多的机器 Google建立了大型Cell，一是为了允许运行大型任务，二是为了减少资源碎片。为测试减少碎片的效果，我们把负载从一个Cell分散多个较小的Cell中 —— 首先将作业随机排列，然后轮流分配到各小的Cell中。图7确认了使用小型Cell需要增加相当多的机器。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:5:3","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"5.4 细粒度资源请求 Borg用户请求的CPU单位是0.001个核，内存和硬盘的单位是字节。（一个核实际上是一个CPU的超线程，对不同机器类型的性能进行了标准化）。图8表明用户充分利用了细粒度：请求的CPU核和内存数量的“特别偏好值”是很少的，这些资源也没有明显的相关性。这与[68]里的分布非常相似，除了我们在90%分位数及以上的内存请求多一点之外。 尽管IaaS普遍只提供一组固定尺寸的容器或虚拟机[7, 33]，但不符合我们的需求。为说明这一点，我们对prod的作业和Alloc（§2.4）申请的CPU核和内存分别向上取整到最接近的2的幂，形成固定大小的“桶”，最小的桶有0.5个核和1GiB内存。图9显示一般情况下这样需要增加30-50%的资源。上限的情形是，有的大型任务即便将Cell扩大为未压缩尺寸的四倍也无法容纳，只好为其分配一整台机器。下限是允许这些任务一直等待。（这比[37]给出的将近100%的额外开销要小一些，因为我们支持不止4种尺寸的桶，而且允许CPU和内存分别改变）。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:5:4","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"5.5 资源回收 作业可以声明一个资源限额（Limit），是每个任务能获得的资源上限。Borg会用它来检查用户是否有足够的配额来接受该作业，并检查某个机器是否有足够的可用资源来运行任务。因为Borg通常会杀死那些试图使用超出内存和硬盘申请值的任务，或者限制其CPU使用量不超过申请值，所以有的用户会为任务申请超过实际需要的资源，就像有的用户会购买超过实际需要的配额一样。另外，一些任务只是偶尔需要使用它们申请的所有资源（例如，在一天中的高峰期或者受到了拒绝服务攻击），但大多时候用不了。 与其把那些分配出来但暂时没有被用到的资源浪费掉，我们估计了一个任务会用多少资源，然后把剩余的资源回收给那些可以忍受低质量资源的任务，例如批处理作业。这整个过程称为资源再利用。这个估值称为任务的资源预留（Reservation）。Borgmaster每隔几秒就会根据Borglet获取的细粒度资源使用量信息来计算一次预留值。最初的预留资源被设置为资源限额；在300秒之后，也就过了启动阶段，预留资源会缓慢的下降到实际使用量加上一个安全值。在实际使用量超过它时，预留值会迅速增加。 Borg调度器使用资源限额来计算prod级任务原注4: 准确的说，是高优先级的、延迟敏感的任务，见§6.2是否可以执行（§3.2），所以这些任务不依赖于回收的资源，也与资源超售无关；对于non-prod的任务，运行任务使用的资源在预留值之内，这样新任务就可以使用回收的资源。 一台机器有可能因为预留（预测）错误而导致运行时资源不足 —— 即使所有的任务都在资源限额之内。如果这种情况发生了，我们会杀掉或者限制non-prod任务，但从来不对prod任务下手。 图10表明，如果没有资源回收，将需要更多的机器。在一个中等规模的Cell中大概有20%的工作负载（§6.2）使用了回收的资源。 图11可以看到更多的细节，其中有预留值、使用量与限额的比例。当资源紧张时，超出内存限额的任务首先会被抢占，不论优先级有多高，所以很少有任务超过内存限额。另一方面，CPU使用量是可以被轻易限制住的，所以短时的毛刺虽然会导致使用量超过预留值，但这没什么损害。 图11表明了资源回收可能还过于保守：在预留值和实际使用量中间还有一大段差距。为了测试这一点，我们选择了一个线上Cell，（第一周作为参照基准，）第二周将其估计算法的参数调整为比较激进的设置，即把安全裕度留小一点；第三周采取的是介于激进和基准之间的适度策略，最后一周恢复到基准策略。 图12展示了结果。第二周的预留值明显更接近实际使用量，第三周稍大一点，最大的是第一周和第四周。和预期的一样，第二周和第三周的OOM比率轻微地增加了原注5: 第3周后期的异常情况与本次实验无关。在评估了这个结果后，我们认为利大于弊，于是在其它Cell上也采用了适度策略的资源回收参数。 图12. 更激进的资源估计可以回收更多资源，但会稍增加OOM事件 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:5:5","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"6. 隔离 50%的机器运行了9个以上的任务；处于90%分位数的机器则有大约25个任务，4500个线程[83]。虽然在应用之间共享机器会增加利用率，但也需要一个比较好的机制来保证任务之间不产生干扰。这同时适用于安全和性能两个方面。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:6:0","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"6.1 安全隔离 我们使用Linux的chroot作为同一台机器上不同任务之间的主要安全隔离机制。仅当某台机器有用户运行的任务时，为了允许远程调试，我们以前会自动分发（或废除）SSH秘钥，使用户可以访问这台机器。对大多数用户来说，现在被替换为borgssh命令，这个程序和Borglet协同构建一个SSH通道，连接到与任务运行在同一个chroot和cgroup下的Shell，这样限制就更加严格了。 Google的AppEngine（GAE）[38]和Google Compute Engine（GCE）使用VM和安全沙箱技术运行外部的软件。我们把每个运行在KVM进程中的VM作为一个Borg任务来运行。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:6:1","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"6.2 性能隔离 早期的Borglet使用了一种相对原始的资源隔离措施：事后检查内存、硬盘和CPU使用量，终止使用过多内存和硬盘的任务，或者降低使用过多CPU的任务的Linux CPU优先级。不过，一些粗暴的任务还是能很容易地影响到同台机器上其它任务的性能，于是有的用户就会多申请资源来让Borg减少与其共存的任务数量，降低了资源利用率。资源回收可以弥补一些损失，但不是全部，因为涉及到安全裕度。在极端情况下，用户会要求使用专属的机器或者Cell。 目前，所有Borg任务都运行在基于Linux cgroup的资源容器[17, 58, 62]里。Borglet控制着这些容器的设置。有了OS内核的帮助，控制能力得到了改善。即使这样，偶尔还是有底层的资源发生冲突（例如内存带宽或L3缓存污染），见[60, 83]。 为了应对过载和超售，Borg任务有一个应用类别（appclass）属性。最重要的区分是延迟敏感（LS）的应用和本文中称为批处理（batch）的其它类别。LS任务包括面向用户的应用和需要快速响应的共享基础设施。高优先级的LS任务得到最高优待，可以暂时让批处理任务等待几秒种。 第二个区分是：可压缩资源（例如CPU，硬盘I/O带宽），都是基于速率的，可以通过降低一个任务的服务质量而不是杀死它来回收；不可压缩资源（例如内存、硬盘空间），这些一般来说不杀掉任务是没办法回收的。如果一个机器用光了不可压缩资源，Borglet马上就会开始杀死任务，从低优先级开始，直到能满足剩下的资源预留。如果机器用完了可压缩资源，Borglet会限制使用量（偏好LS任务），这样不用杀死任何任务也能处理短期负载尖峰。如果情况没有改善，Borgmaster会从这个机器上移除一个或多个任务。 Borglet有一个用户态的控制循环，负责以下操作：为容器确定内存量，prod任务基于预测值，而non-prod任务则基于内存压力；处理来自内核的OOM事件；当任务试图分配超过其自身限额的内存时，或者超售的机器上确实耗尽内存时，都会杀死任务。Linux激进的文件缓存让我们的实现复杂得多，因为需要精确计算内存使用量。 为了增强性能隔离，LS任务可以预留整个物理CPU核，以阻止别的LS任务来使用它们。批处理任务被允许运行在任何核上，但是相比LS任务，批处理任务只分配了很少的调度份额。Borglet动态地调整贪婪的LS任务的资源上限，以保证它们不会把批处理任务饿上几分钟，必要时有选择的使用CFS带宽控制[75]；仅用份额来表示是不够的，因为我们有多个优先级。 图13. 调度延迟与负载的关系。即一个就绪线程需要等待超过1 ms才能运行的比率，与机器繁忙程度的关系。每组数据条中，左侧是延迟敏感的任务，右侧是批处理任务 只有很少的比率需要等5 ms以上，超过10 ms就极少了。这是2013年12月从一个 代表性的Cell中获取的一个月的数据；误差线是每天的波动同Leverich[56]一样，我们发现标准的Linux CPU调度器（CFS）需要大幅调整才能同时支持低延迟和高利用率。为了减少调度延迟：我们内部版本的CFS对每个cgroup都有单独的负载历史[16]；允许LS任务抢占批处理任务；当一个CPU有多个就绪的LS任务时，会减少其调度量（Quantum）。幸运的是，我们的大多应用使用一个线程处理一个请求的模型，这样就缓解了持续的负载失衡。我们节俭地使用cpusets为有特别严格的延迟需求的应用分配CPU核。这些努力的一些效果展示在图13中。我们持续在这方面投入，增加感知NUMA、超线程、能耗（如[81]）的线程放置和CPU管理，改进Borglet的控制精确度。 任务被允许在其上限之内消费资源。大部分任务还允许去使用超出上限的可压缩资源，例如CPU，以利用空闲资源。只有5%的LS任务禁止这么做，主要是为了改善可预测性；小于1%的批处理任务也禁止了。使用超量内存默认是被禁止的，因为这会增加任务被杀掉的概率，不过即使这样，10%的LS任务解除了这个限制，79%的批处理任务也解除了，因为这是MapReduce框架的默认设置。这补偿了资源回收（§5.5）的后果。批处理任务很乐意使用空闲的或回收的内存：大多情况下这样运作得很好，即使偶尔批处理任务会被急需资源的LS任务杀掉。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:6:2","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"7. 相关工作 数十年来，资源调度已经在多种场景得到了研究，如广域高性能计算网格、工作站网络、和大规模服务器集群等。我们这里只关注最相关的大规模服务器集群这个场景。 最近的一些研究分析了来自于Yahoo!、Google和Facebook的集群记录数据[20, 52, 63, 68, 70, 80, 82]，展现了这些现代的数据中心和工作负载固有的异构性和大规模带来的挑战。[69]包含了对集群管理器架构的分类。 Apache Mesos[45]将资源管理和任务放置功能拆分到一个集中资源管理器（类似于去掉调度器的Bormaster）和多种“框架”（比如Hadoop[41]和Spark[73]）之间，两者基于供应（Offer）机制交互。Borg则把这些功能集中在一起，使用基于请求的机制，而且扩展性相当好。DRF[29, 35, 36, 66]最初是为Mesos开发的；Borg则使用优先级和准入配额来替代。Mesos开发者已经宣布了他们扩展Mesos的雄心壮志：预测性资源分配和回收，以及解决[69]中发现的一些问题。 YARN[76]是一个针对Hadoop的集群管理器。每个应用都有一个另外的管理器，与中央资源管理器谈判所需资源；这跟大约2008年开始Google的MapReduce作业已经使用的向Borg获取资源的模式如出一辙。YARN的资源管理器最近才支持容错。一个相关的开源项目是Hadoop Capacity Scheduler（基于容量的调度器）[42]，提供了多租户下的容量保证、多层队列、弹性共享和公平调度。YARN最近扩展支持了多种资源类型、优先级、抢占和高级准入控制[21]。Tetris俄罗斯方块研究原型[40]支持完成时间感知的作业装箱。 Facebook的Tupperware[64]，是一个在集群中调度cgroup容器的类Borg系统；只有少量细节披露出来了，看起来它也提供了某种形式的资源回收功能。Twitter开源的Aurora[5]是一个类似Borg的，用于长期运行服务的调度器，运行与Mesos之上，其配置语言和状态迁移与Borg类似。 微软的Autopilot[48]为其集群提供了“自动化的软件供应和部署；系统监控；以及采取修复行为处理软硬件故障”的功能。Borg生态系统提供了相似的特性，不过篇幅所限，不再深入讨论；作者Isaard概括了很多我们也赞成的最佳实践。 Quincy[49]使用了一个网络流模型来提供公平性和数据局部性感知的调度，应用在几百个节点的集群的数据处理DAG上。Borg使用配额和优先级在用户间共享数据，可以扩展到上万台机器。Quincy可以直接处理执行图，而Borg需要在其上层另外构建。 Cosmos[44]聚焦在批处理上，强调了用户可以公平获取他们已经捐献给集群的资源。每个作业分别有一个管理器来获取资源；只有很少公开的细节。 微软的Apollo系统[13]为每个短期批处理作业分别使用单独的调度器，以获得高吞吐量，其集群规模看起来与Borg的Cell相当。Apollo投机地执行低优先级后台任务来提升资源利用率，代价是有时有长达多日的队列延迟。Apollo的各节点都一个关于开始时间的预测矩阵，其行列分别为CPU和内存两个资源维度。调度器会综合开始时间、估计的启动开销、获取远程数据的开销来决定部署位置，并用一个随机延时来减少冲突。Borg使用的是中央调度器，基于之前的分配来决定部署位置，可以处理更多的资源维度，而且更关注高可用、长期运行的应用；Apollo也许能处理比Borg更高的任务到达率。 阿里巴巴的伏羲（Fuxi）[84]支持数据分析的应用，从2009年就开始运行了。类似Borgmaster，一个集中的FuxiMaster（也做了多副本容错）从节点上获取可用资源的信息、接受应用的资源请求，然后匹配两者。伏羲的增量调度策略与Borg的任务等价类是相反的：伏羲用最新的可用资源匹配等待队列里的任务。类似Mesos，伏羲允许定义“虚拟资源”类型。只有对合成工作负载的实验结果是公开的。 Omega[69]支持多个并发的调度器，粗略相当于没有持久存储和链接分片的Borgmaster。Omega调度器使用乐观并发控制的方式去操作一个共享的集群预期的和观察的状态表示。集群状态存储在一个集中持久存储中，用单独的连接组件与Borglet同步。Omage架构设计为支持多种不同的工作负载，它们有自己特定的RPC接口、状态迁移和调度策略（例如长期运行的服务、多个框架批处理作业、如集群存储这样的基础服务、Google云平台上的虚拟机）。相反，Borg提供了一种通用方案，同样的RPC接口、状态迁移、调度策略，为支持多种不同的负载，其规模和复杂度逐渐增加，但目前来说可扩展性还不算一个问题（§3.4）。 Google的开源项目Kubernetes系统[53]把应用放在Docker容器内[28]，再分发到多个机器上。它既可以运行在物理机上（像Borg那样），也可以运行在多个云供应商（比如Google Compute Engine，GCE）的主机上。Kubernetes正在快速开发中，它的很多开发者也参与开发了Borg。Google提供了一个托管的版本，称为Google Container Engine（GKE）[39]。我们会在下一节里面讨论Kubernetes从Borg中学到了哪些东西。 在高性能计算社区对这个领域有长期的研究传统（如Maui, Moab, Platform LSF[2, 47, 50]）；但是这和Google Cell所面对的规模、工作负载、容错性是不同的。总体而言，为达到高用率，这些系统需要让任务在一个很长的队列中等待。 虚拟化供应商，例如VMware[77]，和数据中心方案供应商，例如HP和IBM[46]提供了典型情况下可以扩展到一千台机器规模的集群管理解决方案。另外，一些研究小组的原型系统以多种方式提升了调度质量（如[25, 40, 72, 74]）。 最后，正如我们所指出的，大规模集群管理的另外一个重要部分是自动化和无人化。[43]指出，失效预案、多租户、健康检查、准入控制，以及可重启对实现单个运维人员管理更多的机器的目标是必要的。Borg的设计哲学也是这样的，而且支撑了我们的每个SRE管理数万台机器。 Borg从它的前任继承了很多东西，即我们内部的全局工作队列（Global Work Queue）系统，它最初是由Jeff Dean，Olcan Sercinoglu, 和Percy Liang开发的。 Conder[85]曾被广泛应用于收集空闲资源，其ClassAds机制[86]支持声明式的语句和自动属性匹配。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:7:0","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"8. 经验教训和未来工作 在这一节中我们介绍了十多年来我们在生产环境运行Borg得到的定性的经验教训，然后介绍设计Kubernetes[53]是如何吸收这些经验的。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:8:0","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"8.1 教训 我们从一些Borg作为反面警示的特性开始，然后介绍Kubernetes的替代方案。 将作业作为唯一的任务分组机制比较受限 Borg没有内置的方法将多个作业组成单个实体来管理，或将相关的服务实例关联起来（例如，测试通道和生产通道）。作为一个技巧，用户把他们的服务拓扑编码到作业的名称中，然后构建了更高层的管理工具来解析这些名称。这个问题的另外一面是，没办法指向服务的任意子集，这就导致了僵硬的语义，以至于无法滚动升级或改变作业的实例数。 为了避免这些困难，Kubernetes不再使用作业这个概念，而是用标签（Label）来组织它的调度单元（Pod）。标签是任意的键值对，用户可以对系统的任何对象打上标签。Borg作业可以等效地通过对一组Pod打上 $\\langle$作业：作业名$\\rangle$ 这样的标签来实现。其它有用的分组方式也可以用标签来表示，例如服务、层级、发布类型（如，生产、就绪、测试）。Kubernetes用标签查询的方式来选取待操作的目标对象。这样就比固定的作业分组更加灵活。 同一台机器的任务共用一个IP太复杂了 Borg中，同一台机器上的所有任务都使用主机的同一个IP地址，共享端口空间。这就带来几个麻烦：Borg必须把端口当做资源来调度；任务必须先声明它需要多少端口，而且需要支持启动时传入可用端口号；Borglet必须强制端口隔离；域名和RPC系统必须像IP一样处理端口。 多亏了Linux的namespace、虚拟机、IPv6和软件定义网络SDN的出现，Kubernetes可以用一种更用户友好的方式来消除这些复杂性：每个Pod和Service都自己的IP地址，允许开发者选择端口，而不是让他们的软件支持从基础设施获得分配，这也消除了基础设施管理端口的复杂性。 给资深用户优化而忽略了初级用户 Borg提供了一大堆针对“资深用户”的特性，这样他们就可以仔细地调节他们程序的运行方式（BCL规范约有230个参数）：开始的目的是为了支持Google的大型资源用户，提升他们的效率会带来显著的效益。但不幸的是，这么复杂的API让初级用户用起来很复杂，而且限制了API的演进。我们的解决方案是在Borg上又做了一些自动化的工具和服务，从实验中决定合理的配置。由于应用支持容错，实验可以自由进行：即使自动化出了问题，也只是小麻烦，不会导致灾难。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:8:1","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"8.2 经验 另一方面，有不少Borg的设计特性是非常有益的，而且经历了时间考验。 Alloc是有用的 Borg的Alloc抽象适用于广泛使用的保存日志模式（§2.4），另一个流行的模式是：一个简单的数据加载任务定期更新Web服务器使用的数据。Alloc和软件包机制允许这些辅助服务由不同的小组开发。Kubernetes对应于Alloc的概念是Pod，它是对一个或多个容器的资源封装，其中的容器共享Pod的资源，而且总是被调度到同一台机器上。Kubernetes使用Pod里的辅助容器来替代Alloc里面的任务，不过思路是一样的。 集群管理不只是任务管理 虽然Borg的主要角色是管理任务和机器的生命周期，但Borg上的应用还从其它的集群服务中收益良多，例如域名服务和负载均衡。Kubernetes用Service这个抽象概念来支持域名服务和负载均衡：Service有一个域名和用标签选出的多个Pod的动态集合。集群中的任何容器都可以通过Service域名连接到该服务。幕后，Kubernetes自动将连接到该Service的负载分散到与其标签匹配的Pod之间，由于Pod挂掉后会被重新调度到其它机器上，Kubernetes还会跟踪这些Pod的位置。 自省是至关重要的 虽然Borg总体上是工作良好的，但出了问题后，定位根本原因是非常有挑战性的。Borg的一个关键设计选择是把所有的调试信息暴露给用户而不是隐藏起来：Borg有几千个用户，所以“自助”是调试的第一步。虽然一些用户的依赖项让我们难以废弃一些特性或修改内部策略，但这还是成功的，我们还没找到其它实际的替代方式。为管理大量的数据，我们提供了多个层次的UI和调试工具，这样用户就可以快速定位与其作业相关的异常事件，深入挖掘来自其应用和基础设施本身的详细事件和错误日志。 Kubernetes也计划引入Borg的大部分自省技术。和Kubernetes一起发布了很多工具，比如用于资源监控的cAdvisor[15]，它基于Elasticsearch/Kibana[30]和Fluentd[32]聚合日志。Master可以用来查询某个对象的状态快照。Kubernetes提供了一致机制，所有可以记录事件的组件（例如，被调度的Pod、出错的容器）都可以被客户端访问。 Master是分布式系统的核心 Borgmaster最初设计为一个单体的系统，随着时间发展，它演变成了一组服务生态系统的核心。用户作业管理的管理是由这些服务协同完成的。比如，我们把调度器和主要的UI（Sigma）分离成单独的进程，增加了一组服务，包括准入控制、纵向和横向扩展、任务重新装箱、周期性作业提交（cron）、工作流管理，用于离线查询的系统活动归档等。总体而言，这让我们能扩展工作负载和特性集合，但无需牺牲性能和可维护性。 Kubernetes的架构走的更远一些：它的核心是一个仅处理请求和操作底层状态目标的API服务。集群管理逻辑构建为一个小型的、可组合的微服务，作为API服务的客户端，如故障后仍维持Pod副本个数在期望值的副本管理器，以及管理机器生命周期的节点管理器。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:8:2","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"8.3 总结 在过去十年间，所有几乎所有的Google集群负载都迁移到了Borg上。我们仍在持续改进它，并把经验应用到了Kubernetes上。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:8:3","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"致谢 这篇文章的作者负责撰写文章，并完成了评估实验。几十位设计、实现和维护Borg组件和生态系统的工程师才是它成功的关键。我们在这里列出直接参与设计、实现和维护Borgmaster及Borglet的人员。如果有遗漏，我们深表歉意。 早期版本的Borgmaster设计和实现人员有：Jeremy Dion和Mark Vandevoorde，以及Ben Smith, Ken Ashcraft, Maricia Scott, Ming-Yee Iu 和 Monika Henzinger。早期版本的Borglet主要是由Paul Menage设计和实现的。 后续的参与者包括：Abhishek Rai, Abhishek Verma, Andy Zheng, Ashwin Kumar, Beng-Hong Lim, Bin Zhang, Bolu Szewczyk, Brian Budge, Brian Grant, Brian Wickman, Chengdu Huang, Cynthia Wong, Daniel Smith, Dave Bort, David Oppenheimer, David Wall, Dawn Chen, Eric Haugen, Eric Tune, Ethan Solomita, Gaurav Dhiman, Geeta Chaudhry, Greg Roelofs, Grzegorz Czajkowski, James Eady, Jarek Kusmierek, Jaroslaw Przybylowicz, Jason Hickey, Javier Kohen, Jeremy Lau, Jerzy Szczepkowski, John Wilkes, Jonathan Wilson, Joso Eterovic, Jutta Degener, Kai Backman, Kamil Yurtsever, Kenji Kaneda, Kevan Miller, Kurt Steinkraus, Leo Landa, Liza Fireman, Madhukar Korupolu, Mark Logan, Markus Gutschke, Matt Sparks, Maya Haridasan, Michael Abd-El-Malek, Michael Kenniston, Mukesh Kumar, Nate Calvin, Onufry Wojtaszczyk, Patrick Johnson, Pedro Valenzuela, Piotr Witusowski, Praveen Kallakuri, Rafal Sokolowski, Richard Gooch, Rishi Gosalia, Rob Radez, Robert Hagmann, Robert Jardine, Robert Kennedy, Rohit Jnagal, Roy Bryant, Rune Dahl, Scott Garriss, Scott Johnson, Sean Howarth, Sheena Madan, Smeeta Jalan, Stan Chesnutt, Temo Arobelidze, Tim Hockin, Todd Wang, Tomasz Blaszczyk, Tomasz Wozniak, Tomek Zielonka, Victor Marmol, Vish Kannan, Vrigo Gokhale, Walfredo Cirne, Walt Drummond, Weiran Liu, Xiaopan Zhang, Xiao Zhang, Ye Zhao, Zohaib Maya. Borg SRE团队也是非常重要的，包括：Adam Rogoyski, Alex Milivojevic, Anil Das, Cody Smith, Cooper Bethea, Folke Behrens, Matt Liggett, James Sanford, John Millikin, Matt Brown, Miki Habryn, Peter Dahl, Robert van Gent, Seppi Wilhelmi, Seth Hettich, Torsten Marek, 和 Viraj Alankar。Borg配置语言（BCL）和borgcfg工具最初是Marcel van Lohuizen 和 Robert Griesemer开发的。 我们不小心漏掉了 Brad Strand, Chris Colohan, Divyesh Shah, Eric Wilcox, 和 Pavanish Nirula。 谢谢我们的审稿人（尤其是Eric Brewer, Malte Schwarzkopf 和 Tom Rodeheffer），以及我们的导师Christos Kozyrakis，对这篇论文的反馈。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:9:0","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"勘误 2015-04-23 定稿后，我们发现了若干无意的疏漏和歧义。 译注：译文已将勘误内容放置到对应章节。补充的2条参考文献与已有序号冲突，故放在了列表之后，并继续编号为85，86。 ","date":"2022-05-31","objectID":"/posts/brog-in-google/:10:0","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["k8s"],"content":"参考文献 O. A. Abdul-Rahman and K. Aida. Towards understanding the usage behavior of Google cloud users: the mice and elephants phenomenon. In Proc. IEEE Int'l Conf. on Cloud Computing Technology and Science (CloudCom), pages 272–277, Singapore, Dec. 2014. Adaptive Computing Enterprises Inc., Provo, UT. Maui Scheduler Administrator's Guide, 3.2 edition, 2011. T. Akidau, A. Balikov, K. Bekiroğlu, S. Chernyak, J. Haberman, R. Lax, S. McVeety, D. Mills, P. Nordstrom,and S. Whittle. MillWheel: fault-tolerant stream processing at internet scale, In Proc. Int'l Conf. on Very Large Data Bases (VLDB), pages 734–746, Riva del Garda, Italy, Aug.2013. Y. Amir, B. Awerbuch, A. Barak, R. S. Borgstrom, and A. Keren. An opportunity cost approach for job assignment in a scalable computing cluster, IEEE Trans. Parallel Distrib.Syst., 11(7):760–768, July 2000. Apache Aurora. http://aurora.incubator.apache.org/, 2014. Aurora Configuration Tutorial. https://aurora.incubator.apache.org/documentation/latest/configuration-tutorial/, 2014. AWS. Amazon Web Services VM Instances. http://aws.amazon.com/ec2/instance-types/, 2014. J. Baker, C. Bond, J. Corbett, J. Furman, A. Khorlin, J. Larson, J.-M. Leon, Y. Li, A. Lloyd, and V. Yushprakh. Megastore: Providing scalable, highly available storage for interactive services, In Proc. Conference on Innovative Data Systems Research (CIDR), pages 223–234, Asilomar, CA, USA, Jan. 2011. M. Baker and J. Ousterhout. Availability in the Sprite distributed file system, Operating Systems Review,25(2):95–98, Apr. 1991. L. A. Barroso, J. Clidaras, and U. Hölzle. The datacenter as a computer: an introduction to the design of warehouse-scale machines, Morgan Claypool Publishers, 2nd edition, 2013. L. A. Barroso, J. Dean, and U. Holzle. Web search for a planet: the Google cluster architecture, In IEEE Micro, pages 22–28, 2003. I. Bokharouss. GCL Viewer: a study in improving the understanding of GCL programs, Technical report, Eindhoven Univ. of Technology, 2008. MS thesis. E. Boutin, J. Ekanayake, W. Lin, B. Shi, J. Zhou, Z. Qian, M. Wu, and L. Zhou. Apollo: scalable and coordinated scheduling for cloud-scale computing, In Proc. USENIX Symp. on Operating Systems Design and Implementation (OSDI), Oct. 2014. M. Burrows. The Chubby lock service for loosely-coupled distributed systems, In Proc. USENIX Symp. on Operating Systems Design and Implementation (OSDI), pages 335–350,Seattle, WA, USA, 2006. cAdvisor. https://github.com/google/cadvisor, 2014 CFS per-entity load patches. http://lwn.net/Articles/531853, 2013. cgroups. http://en.wikipedia.org/wiki/Cgroups, 2014. C. Chambers, A. Raniwala, F. Perry, S. Adams, R. R. Henry, R. Bradshaw, and N. Weizenbaum. FlumeJava: easy, efficient data-parallel pipelines, In Proc. ACM SIGPLAN Conf. on Programming Language Design and Implementation (PLDI), pages 363–375, Toronto, Ontario, Canada, 2010. F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows, T. Chandra, A. Fikes, and R. E. Gruber. Bigtable: a distributed storage system for structured data, ACM Trans. on Computer Systems, 26(2):4:1–4:26, June 2008. Y. Chen, S. Alspaugh, and R. H. Katz. Design insights for MapReduce from diverse production workloads, Technical Report UCB/EECS–2012–17, UC Berkeley, Jan. 2012. C. Curino, D. E. Difallah, C. Douglas, S. Krishnan, R. Ramakrishnan, and S. Rao. Reservation-based scheduling: if you're late don't blame us! In Proc. ACM Symp. on Cloud Computing (SoCC), pages 2:1–2:14, Seattle, WA, USA, 2014. J. Dean and L. A. Barroso. The tail at scale, Communications of the ACM, 56(2):74–80, Feb. 2012. J. Dean and S. Ghemawat. MapReduce: simplified data processing on large clusters, Communications of the ACM, 51(1):107–113, 2008. C. Delimitrou and C. Kozyrakis. Paragon: QoS-aware scheduling for heterogeneous datacenters, In Proc. Int'l Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS), Mar. 201. C. Delimitrou and C. Kozyrakis. Quasar: resource-efficient and","date":"2022-05-31","objectID":"/posts/brog-in-google/:11:0","tags":["转载","brog","cluster"],"title":"[译]使用Borg在Google管理大规模集群","uri":"/posts/brog-in-google/"},{"categories":["microservice"],"content":" 本文为系列篇微服务的第一篇，将会介绍什么是微服务、为什么是微服务、微服务的核心理念和相关周边组件的介绍。 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:0:0","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"1. 什么是微服务 aws 关于微服务的解释 微服务是一种开发软件的架构和组织方法，其中软件由通过明确定义的 API 进行通信的小型独立服务组成。这些服务由各个小型独立团队负责。 微服务架构使应用程序更易于扩展和更快地开发，从而加速创新并缩短新功能的上市时间。 微服务并非是一个全新的技术而是跟随着互联网技术的发展的一个服务部署架构方案。与微服务相对的便是传统巨石架构(整体架构)。可以理解为微服务的出现解决了巨石架构的大部分的问题，因此微服务这个概念也被大部分的开发者所熟悉。 微服务架构 下面我们通过对比微服务和巨石架构的方式对微服务进行了解。 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:1:0","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"1.1 巨石架构 巨石架构又名为整体架构，在以往互联网应用开发过程一直是大家默认的一种架构方案。将所有的功能能力都在一个项目内实现，甚至包括前端页面的代码。 这种方式对于快速发展的业务来说非常适合的，不管太多的代码规范架构规范，功能快速实现快速上线作为第一目标，从而衍生出了很多一键生成项目的各类框架模板，只是为了更快速的完成项目的迭代。 这种架构的优点是快速迭代、代码相对聚合、团队内所有人只维护一个仓库即可。但是其缺点也比较明显，首先随着代码的增长维护仓库的成本越来越高从而新增、修改新功能变得越来越麻烦。不仅仅是开发同学的工作量变多变复杂，QA 同学的工作也会越来越复杂。仅仅一个小功能的添加，由于是一个整体的大项目，功能测试回归测试都变得异常棘手。逐渐的项目的快速迭代的优点就没有了，迭代速度会越来越慢。 其次是项目中局部升级几乎变得不可能了，例如依赖的包、底层数据库、MQ 的切换等等，很难确保这些组件不与业务逻辑代码没有关联。即便是升级一些第三方的包，也需要考虑所有引用的地儿，升级了也需要大量的回归测试。 然后是代码 debug 变得异常麻烦。巨石架构内虽说包含了所有的代码无需跳转项目的 debug，但是由于都是直接调用代码内的方法，很难做到日志和链路追踪，线上出故障的话只能硬着头皮在大量日志中找相关日志。 然后是部署方式比较死板，由于最后打包出来的进程包含了所有的能力，部署之后很难预估服务的请求量和压力，资源利用率会相对低。 最后是容易受语言的限制。我认为没有所谓的好语言不好的语言，每个语言都有各自的擅长和不擅长的领域。随着项目的迭代总会有一些需求，用项目的主语言实现遇到很多的困难，虽然明智换一种语言很快能解决当前的需求，但是巨石架构不允许这样的操作，也很难说为了这个功能新增一个项目进行部署。 总体来说，巨石架构在项目初期的时候几乎不会展现出他的缺点，但是经过一段时间的迭代，巨石架构下的项目就很难进行很好地维护了。 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:1:1","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"1.2 微服务架构 巨石架构 -\u003e 微服务架构 微服务架构是由一组小微的服务的组合而成。这些服务之前通过 rpc 的方式进行通信，从而提供一个整体的能力。微服务的优势有以下几点： 敏捷性：微服务促进若干小型独立团队形成一个组织，这些团队负责自己的服务。各团队在小型且易于理解的环境中行事，并且可以更独立、更快速地工作。这缩短了开发周期时间。您可以从组织的总吞吐量中显著获益。 灵活扩展：通过微服务，您可以独立扩展各项服务以满足其支持的应用程序功能的需求。这使团队能够适当调整基础设施需求，准确衡量功能成本，并在服务需求激增时保持可用性。 轻松部署：微服务支持持续集成和持续交付，可以轻松尝试新想法，并可以在无法正常运行时回滚。由于故障成本较低，因此可以大胆试验，更轻松地更新代码，并缩短新功能的上市时间。 技术自由：微服务架构不遵循“一刀切”的方法。团队可以自由选择最佳工具来解决他们的具体问题。因此，构建微服务的团队可以为每项作业选择最佳语言和工具。 弹性：服务独立性增加了应用程序应对故障的弹性。在整体式架构中，如果一个组件出现故障，可能导致整个应用程序无法运行。通过微服务，应用程序可以通过降低功能而不导致整个应用程序崩溃来处理总体服务故障。 当然微服务并不是什么 silver bullet，微服务虽说相对巨石架构有很多有点，但是随微服务一起来的那就是比较复杂的部署环境。同时有多个不同的服务在运行且得确保各个服务直接的网络的互通，加上微服务会依赖服务发现、日志收集、链路追踪等组件，需要一定的架构能力。 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:1:2","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"2. 为什么是微服务 在对巨石架构和微服务的优缺点有了一定的认知。我们现在聊一聊为什么是微服务或者是说 为什么微服务为什么被越来越多的开发者所接受。 微服务并非是近几年才出现的概念，其实这种架构方式一直都有被提到，但是由于以往的服务都是物理机的方式部署，很难为灵活的微服务架构提供一个展现他优势的平台。 而容器技术的出现我认为是微服务架构崛起的开始。服务可以以一个镜像为单位部署在服务器上，且可限制资源、可任意扩缩容、可快速升级。这种部署方式使得微服务架构的优点更加体现出来了，可以说是容器技术的出现成就了微服务。 之后再出现的容器编排（kubernetes），把容器技术的优势展现给了所有人，越来越多的服务不再像以前那么臃肿，一个服务只做一件事儿，与其他服务组成一个完整的架构体系。 与此同时出现了一大批优秀的开源产品/概念如 etcd、Prometheus、open tracing 等，为微服务架构的发展和发扬起了很大作用。 除此之外，云计算/云平台的出现，让我们认识了 aws、azure、Google Cloud、腾讯云、阿里云等这些云厂商。他们给我带来了很多 Paas，Saas 服务，使得我们的项目架构可以更灵活，我们不用自己搞所有的东西了，在我们的架构体系里可以有不同语言实现服务、可以有不同云厂商提供的服务。 这些种种新的的技术的横空出世以及原先技术的更新换代，为微服务架构体现带来了很多的可能性，这就是为什么是微服务的答案。 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:2:0","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"3. 微服务的核心理念 微服务核心理念我认为是小而精，由多个不同职责的小微服务去组成一个强大的服务体系。这个体系中的每一个模块可单独维护、可单独升降级、可单独替换从而不影响整体能力。 而这些模块作为一个个微服务或者基础组件的形式，存在这个体系当中。从外部来看，整体体系只有一个或几个入口，各个服务和组件互相依赖调用，从而做到一个完整的请求链，与此同时会有类似 sidecar 的组件去维护整体的完整性。整体架构更像一个金字塔模型，越底层的组件/服务会越多，这些组件/服务的组合为上一层提供服务，而上一层的职责就是根据业务整合不同的底层组件/服务提供的数据，为其上一层服务。 微服务的金字塔模型 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:3:0","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"4. 微服务架构下的主要组件 在本篇开头的图片我们能看到，一个比较完整的微服务架构中会有很多与业务无关的组件和服务在运行，下面我看挑几个核心的组件进行其能力的讲解和该组件在架构中的作用。 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:4:0","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"4.1 服务发现服务注册 服务发现服务注册应该算是微服务架构中的核心概念之一，各个服务之间的通信都离不开该组件的能力。可以从字面意思理解，该组件的核心能力就是一个服务注册中心，服务启动后会将自己的信息，包括名字、通信协议、端口等，注册到注册中心，而其他服务从注册中心拉取其他服务信息，需要时根据服务信息去调用这些服务的 rpc 方法（或 http 接口）。而整个过程中各个服务之前都不需要互相感知服务的状态，仅通过统一的服务注册中心来拉取所需要的服务的基本信息即可。 服务发现服务注册 而这种模式的好处在于，任意一个服务意外退出或用新的服务区替换一个旧的服务，对于其他服务来说是几乎无感知的，不需要对需要调用这个服务的其他服务进行任何改动。如果服务意外退出，其他服务不再从服务注册中心拿到该服务的信息，因此不会出现请求超时或者网络不通的位置情况，从请求发出前已经知道对方服务已经是不可用的状态了，可以做降级处理。 而更换服务（甚至更换服务 ip 端口）都对其他服务是无感知的，因为这些信息本来就是从服务注册中心拿到的，有变化很正常，直接用新的继续使用就可以。 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:4:1","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"4.2 配置中心 以往的服务部署都会带着一个配置文件，然后再服务启动时从配置文件或者环境变量读取服务启动必要的参数。但是随着服务数量的变多，不同服务之间的配置文件会有重叠的情况（比如公用一个数据库，mq，cache 等）。如果运行过程中需要改变这些配置文件会变得非常麻烦，至少需要一次改变配置和重启服务的过程。如果是修改哪些重叠的配置，则更加麻烦，很难保证所有的服务都已经修改且短时间内快速生效。 配置中心则很好的解决了这些问题，类似服务注册中心，配置中心统一管理所有的配置，并且可以做到同一块配置可以被多个应用共享，而一些敏感的配置，配置中心可以做到权限校验从而确保数据安全性（比如有的服务不应该拥有写权限的 db 角色）。 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:4:2","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"4.3 服务构建与部署 微服务架构中的服务，一般要求是可快速部署（甚至自动化部署），传统的服务器或者虚拟机上直接运行二进制文件的方式，对于微服务来说是不能接受的。前面也说了容器化技术的出现加快了微服务架构的发展，因此微服务的部署很大程度上是依赖容器技术的。在提交代码或者手动编译时，需要生成一个docker 镜像，推到镜像仓库中，而部署时直接拉取对应版本的镜像启动即可。 这样在业务迭代或者需要快速扩缩容时，配合容器编排器进行容器的调度，这相对于传统服务器/虚拟机环境下的调度时效性会高很多。同时可以更充分的利用现有的资源，提高利用率。 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:4:3","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"4.4 服务观测 服务的观测指的是日志、监控、链路追踪这三个模块，全面的对服务进行观测，达到尽快发现问题，尽快定位的作用。 4.4.1 日志服务 日志这块一般从两个方面入手，一是结构化日志，二是收集日志。 先说结构化日志。 日志是不仅是为了观察服务运行是否正常，更是为了在出现问题是时候根据相关日志尽快定位问题所在。因此日志怎么打，应该注意哪些事项应该有一份规范。这规范可根据自身情况而定，一般情况下是日志分等级，然后不同情况下打印不同级别的日志。除此之外，日志的结构化，日志不能只只是于人能看懂，更应是机器能看懂。根据日志分析服务质量、错误率、业务情况等大数据分析，从而可以预测一些场景，从而对服务带来一些益处。 再说收集日志。 由于微服务架构下的服务大多数是在 docker 或 kubernetes 环境下运行，这就意味着日志文件在各自的容器内。这个时候需要日志收集的 agent 和统一日志中心来聚合分散的日志，提升查询效率。一般部署一个 ES或者部署一套 ELK 来解决这个问题，当然对于依赖云厂商服务或者没有资源部署这些组件的开发者，对接到云厂商提供的日志组件也是足矣。我之前公司的时候也是对接阿里云的日志服务，整体使用起来也是 OK 的。 除此之外，最近还被安利了一个 Go 语言实现的轻量级的开源日志查询、分析、报警的可视化平台 – Click Visual。大概看了一下页面与阿里云的日志组件页面很相似，可能为了保持用户使用习惯。之后我会花时间去体验这个工具，并出一个安装\u0026体验的博客（先挖个坑）。 Click Visual 4.4.2 服务监控与告警 服务监控与告警并不是微服务独有的概念，正常任何一个线上业务都会基于各种技术手段去监控其业务的各种指标，同时根据规则进行告警。 一般微服务架构中常见的监控和告警组合为 Prometheus + Grafana。 Prometheus 作为一个这几年出现的开源项目,已经被众多知名的开源项目或开源组织所拥护，其指标收集和查询能力可以说是业界数一数二的了。看一下其用户就知道其强大了： Prometheus Users 而 Grafana 是一个数据展示面板，其丰富的组件和高度可制定性，给使用者带来了无数种可能性。且 Grafana 支持包括 Prometheus 在内的 100 多种数据源，可以做到一个数据面，看到所有组件的监控指标并且每一种指标都可以配置监控规则。因此 Grafana 也受大量开发者的欢迎。 4.4.3 OpenTracing OpenTracing 是 CNCF 提出的分布式追踪的标准。它提供用厂商中立的 API，并提供 Go、Java、JavaScript、Python、Ruby、PHP、Objective-C、C++ 和 C# 这九种语言的库。目前支持 Tracer 包括 Zipkin、Skywalking、Jaeger 等，支持的框架包括 gRPC、MOTAN、django、Flask、Sharding-JDBC 等。 OpenTracing 中有两个关键的概念，分别是 Trace，Span。 Trace Trace 表示一个完成的调用链，如一次完成的请求。 Span Span 表示调用链中的一个最小单元，一个 Trace 由多个 Span 组成。如一次请求中的中服务之间的调用可以是一个 span，一次数据库查询可以是一个 span。 如下图JaegerUI 展示的一次 Trace，整个过程是一个 Trace，而过程中的每一个模块是一个 Span。Trace 拥有一个唯一的 TraceID, Span 拥有一个唯一的 SpanID 和其所属的 TraceID。 Jaeger UI 配合 UI，可以很快定位哪一个阶段出问题、哪一个阶段处理变慢了等情况，对于跨多个服务多个组件的请求，这种追踪方式无疑使非常好用的。 目前 OpenTracing 最新状态是 CNCF 于 2022 年 1 月 31 日归档了（cncf-archives-the-opentracing-project）。其主要原因是 CNCF 推出了 OpenTelemetry 项目，是一个更全面的可观测性标准。 OpenTelemetry是什么？ OpenTelemetry（也称为 OTel）是一个开源可观测能力框架，由一系列工具、API 和 SDK 组成，使 IT 团队能够检测、生成、收集和导出远程监测数据以进行分析和了解软件性能和行为。 要了解 OTel 所做的工作，了解可观测能力会有所帮助。粗略定义一下，可观测能力是一种根据系统生成外部数据（通常是日志、指标和跟踪）研判系统内部发生情况的能力。 OpenTelemetry致力于如何收集和发送可观测能力数据并使其具有通用格式。作为云原生计算基金会 (CNCF) 的孵化项目，OTel 旨在提供与供应商无关的统一库和 API 集——主要用于收集数据并将其传输到某个地方。自项目启动以来，包括Dynatrace在内的众多厂商,已加入这一阵营，同心协力让海量数据收集更简便、更易于使用。 要了解可观测能力和 OTel 的处理方法的重要性，让我们更深入地了解远程监测数据本身，以及它如何帮助组织转变其经营方式。 官方文档：https://opentelemetry.io/docs/ ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:4:4","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"4.5 API 网关 API 网关作为整个服务的入口，其核心功能应该包括以下几点： 鉴权 限流熔断降级 感知后端服务，根据业务/规则调用不同的服务 一般成熟的微服务框架都带有 API 网关相关逻辑，也有单独的网关开源项目，如 istio, apisix等。 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:4:5","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"4.6 其他 除了上述几个常见的组件/概念之外，根据业务/架构需求，会有安全、负载均衡、DB、MQ、代理等组件。而这些组件/概念在开发中如何使用，应该怎么写微服务的代码对于一个刚接触微服务的人来说确实是个大问题。而这个问题的答案就是使用一些成熟的微服务框架，这些框架已经把上述的所有概念通过代码抽象出来，看完这些框架的 demo，基本可以上手使用。 常见的框架： asim/go-micro：该框架可以算得上是一个老牌一流的框架，很多新兴框架上都能看到他的影子，强烈建议阅读其各种接口，项目结构的定义，能收获很多。 go-kratos/kratos：B 站员工们推出的微服务框架，最近两年非常火，其设计理念与 go-micro 很类似，也很值得尝试使用，我本人 GoIM 项目也尝试使用该框架，整体还是不错的。其介绍也说了设计受 go-micro 的影响。 这些框架并非把所有微服务的代码都帮你写好，你完全可以替代其任意模块，因为大部分微服务框架都采用定义 api，然后单独实现api 后注册的方式使用。因此框架内几乎没有对固定某个组件的依赖，全部采用 api 的方式。你可以自己实现其 log 模块，或其服务注册服务发现模块。 4.6.1 说点 rpc 相关的 微服务之间的通信一般是以 rpc 的方式，一般我们常听过的 rpc 有： Dubbo：阿里推出的 rpc 框架，仅支持Java Tars：腾讯推出的 rpc 框架，仅支持Java Spring Cloud：Pivotal 公司推出的 rpc 框架，仅支持 Java 上述几个 rpc 框架都是在特定的语言范围内，对于整个微服务体系只有一种语言的情况下才能使用，如果跨语言只能靠通用的 http 等协议去通信。但是越来越多的的微服务架构内不止只有 java 一种语言，所以跨语言的 rpc 框架也是不可缺少的。 gRPC：Google 推出的 rpc 框架，支持大部分主流语言 Thrift：Apache 基金会的开源项目，也是支持大部分主流语言 在 Go 语言环境下，gRPC 无非是最好的选择，况且选择很多主流的开源项目都支持 gRPC，其适用面还是很广的。 在不同微服务之间，共同维护一个 proto buffer 文件（gRPC 的通信编码协议），服务通过protoc 工具生成各自语言的代码，从而可以达到服务之间的 rpc 调用。 以一个发送消息的接口定义为例： // Ignore file header message QueryOfflineMessageReq { string user_id = 1; string last_msg_seq = 2; bool onlyCount = 3; int32 page = 4; int32 page_size = 5; } message QueryOfflineMessageResp { transport.response.BaseResponse response = 1; int32 total = 2; repeated BriefMessage messages = 3; } service OfflineMessage { rpc QueryOfflineMessage(QueryOfflineMessageReq) returns (QueryOfflineMessageResp); } 这块定义可以通过 protoc 生成 Go 语言的代码，包括 struct 定义以及rpc 方法的定义和 client 端调用该 rpc 方法的代码。由于代码量比较多，这里放一个跳转地址，感兴趣的同学可以跳转查看。 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:4:6","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"5. 总结 本篇篇幅较长，由于微服务涉及到的面比较广，况且我想尽量写的全面一点，就写的有点多了。这里对本篇的内容做个简单的总结： 讲述巨石架构 vs 微服务架构 讲述为什么是微服务架构，是什么促成了微服务的发展 微服务的核心理念有哪些 微服务核心组件的介绍 服务发现服务注册 配置中心 服务的构建和部署 服务的观测 API 网关 微服务框架 \u0026 rpc 框架 \u0026 gRPC 本篇是我个人对微服务的一点理解，若有我理解错或者比我更好的理解，欢迎下面评论讨论。 ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:5:0","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":"6. 链接🔗 https://jimmysong.io/kubernetes-handbook/practice/opentracing.html https://github.com/yusank/goim https://opentelemetry.io/docs/ https://www.elastic.co/what-is/elk-stack https://clickvisual.gocn.vip/ https://prometheus.io/ https://grafana.com/ asim/go-micro go-kratos/kratos ","date":"2022-05-14","objectID":"/posts/what-is-microservices/:6:0","tags":["微服务","系列篇"],"title":"[系列]微服务·什么是微服务","uri":"/posts/what-is-microservices/"},{"categories":["microservice"],"content":" 本文为系列篇微服务的开篇文章，讲述本系列的开篇背景、核心内容、涉及到的方向以及往后的规划。 ","date":"2022-05-04","objectID":"/posts/microservices-introduction/:0:0","tags":["微服务","系列篇"],"title":"[系列]微服务·开篇介绍","uri":"/posts/microservices-introduction/"},{"categories":["microservice"],"content":"前言 关于微服务其实我前前后后学习了解使用有几年的时间了，之前也短暂的工作方向也是微服务框架的开发方向。并且去年年底其实以及开始计划写这系列文章了，但是迟迟没有开始在做起来。微服务这个概念已经是一个烂大街的概念了，大部分程序员都对这个概念有比较清晰的概念，我这里主要是想把自己的理解和一些心得分享出来，一是给自己一个经验积累，二是给一个新入行或者像对微服务有较深的理解的同行们一个启发或者一丁点帮助。 至于为什么没有做起来我能想到的原因为无非以下几个： 当时在写 Redis Server 用 Go 语言实现的项目，有足够的写博客的素材(点击查看Redis 系列篇) 当时没有一个现成的或者之前完整的一个微服务项目的经历和源码，从接触微服务到现在大部分时间都是看他人文章去或者去看源码理解底层实现，虽然写了一个初版的微服务框架，但是由于是工作上的项目，没办法拿出来分享。因此没有一个项目来支撑我的系列文章 而到目前这个时间的时候，其实我已经在写一个项目接近 3 个月了，并且基础功能也已经实现。在这种情况下我觉得我可以重启我这系列文章来把我之前的积累以及这次开发过程中的心得做出一个总结。 项目链接：https://github.com/yusank/goim 也可以点击右上角 GoIM 跳转查看该项目的项目文档 ","date":"2022-05-04","objectID":"/posts/microservices-introduction/:1:0","tags":["微服务","系列篇"],"title":"[系列]微服务·开篇介绍","uri":"/posts/microservices-introduction/"},{"categories":["microservice"],"content":"核心内容 该系列篇在我之前计划的结果方向之外在开发过程中我可能会新增一些相关方向的文章，尽可能把微服务的核心概念、思路以及常用的组件都涉及到。 目前想好的方向有以下： 微服务核心概念和解决的问题 微服务的服务发现与服务注册 微服务的配置管理 微服务的日志管理 微服务的监控告警 微服务中 DB 的使用和管理 微服务中服务之间数据共享问题（mq，redis 等方向） 微服务的构建部署问题 ","date":"2022-05-04","objectID":"/posts/microservices-introduction/:2:0","tags":["微服务","系列篇"],"title":"[系列]微服务·开篇介绍","uri":"/posts/microservices-introduction/"},{"categories":["microservice"],"content":"文章传送门 什么是微服务(未发布) 服务注册服务发现(未发布) 配置管理(未发布) 日志(未发布) 监控告警(未发布) DB(未发布) 数据共享(未发布) 构建部署(未发布) 待补充 ","date":"2022-05-04","objectID":"/posts/microservices-introduction/:3:0","tags":["微服务","系列篇"],"title":"[系列]微服务·开篇介绍","uri":"/posts/microservices-introduction/"},{"categories":["GoIM"],"content":" 转载 原文链接： 实现异步并发 worker 队列 在开发 broadcast 功能的时候，碰到一个比较棘手的问题，需要并发执行多个 worker 来讲 broadcast 消息推送到所有在线用户，同时我希望能控制并发数量。 ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:0:0","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"前言 以往遇到类似的问题我都会借助 sync.WaitGroup 加 channel 的方式去做，实现方式也比较简单。大致思路如下： type LimitedWaitGroup struct { wg *sync.WaitGroup ch chan int } func NewLimitedWaitGroup(size int) *LimitedWaitGroup { return \u0026LimitedWaitGroup{ wg : new(sync.WaitGroup), ch : make(chan int, size) } } func (w *LimitedWaitGroup) Add(f func()) { // wait if channel is full w.ch \u003c- 1 w.wg.Add(1) go func() { defer w.done() f() }() } func (w *LimitedWaitGroup) done() { \u003c-w.ch w.wg.Done() } func (w *LimitedWaitGroup) Wait() { w.wg.Wait() } 这样能解决我大部分的简单需求，但是现在我想要的能力用这个简单的 LimitedWaitGroup 无法完全满足，所以重新设计了一个 worker pool 的概念来满足我现在以及以后类似的需求。 ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:1:0","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"设计 ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:2:0","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"需求整理 首先将目前想到的需求以及其优先级列出来： 高优先级： worker pool 支持设置 size，防止 worker 无限增多 任务并发执行且能指定并发数 当 worker 达到上线时，新的任务在一定范围内支持排队等待（即 limited queue） 支持捕获任务错误 排队中的任务应该按顺序调度执行 低优先级： 任务支持实时状态更新 任务可以外部等待完成（类似 waitGroup.Done() ） 当空闲 worker 小于指定并发数时，支持占用空闲 worker 部分运行（如当前剩余 3 个 worker 可用，但是新的任务需要 5 个并发，则尝试先占用这 3 个worker，并在运行过程中继续监听 pool 空闲出来的 worker 并尝试去占用） 小结 小结 列出完需求及其优先级后，经过考虑决定，高优先级除了第五条, 低优先级除了第三条, 其他需求都在目前版本里实现。 原因如下： 首先说低优先级第三条，这块的部分调度执行 worker，目前没有想好比较优雅的实现方式，所以暂时没有实现（但是下个版本会实现） 高优先级的第五条也是跟调度有点关系，如果队列里靠前的任务需要大量的 worker，那很容易造成阻塞，后面的 task 一直没办法执行，即便需要很少的 worker。所以等部分调度执行开发完再把任务按需执行打开。 ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:2:1","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"Task Definition task 表示一次任务，包含了任务执行的方法，并发数，所属的 workerSet以及执行状态等。 type TaskFunc func() error type task struct { tf TaskFunc // task function concurrence int // concurrence of task ws *workerSet // assign value after task distribute to worker. status TaskStatus // store task status. } // TaskStatus is the status of task. type TaskStatus int ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:2:2","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"Worker Definition worker 作为最小调度单元，仅包含 workerSet 和 error . type worker struct { ws *workerSet err error } ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:2:3","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"TaskResult Definition TaskResult 是一个对外暴露的 interface, 用于外部调用者获取和管理任务执行状态信息。 // TaskResult is a manager of submitted task. type TaskResult interface { // get error if task failed. Err() error // wait for task done. Wait() // get task status. Status() TaskStatus // kill task. Kill() } 注意 task 和 TaskStatus 分别实现 TaskResult 的接口，从而外部统一拿到 TaskResult 之所以 TaskStatus 也需要实现 TaskResult 是因为部分情况下，不需要创建 task 直接返回错误状态即可。如： 提交的任务的并发数过高（超过 pool 的 size），当前 queue 已满不能再处理任何其他任务了，这种情况直接返回对应的状态码。 ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:2:4","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"WorkerSet Definition workerSet 为一组 worker的集合，作用是调度 worker 并维护起所属 task 的整个生命过程. // workerSet represent a group of task handle workers type workerSet struct { task *task runningWorker atomic.Int32 workers []*worker ctx context.Context cancel context.CancelFunc wg *sync.WaitGroup } ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:2:5","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"WorkerPool Definition Pool 是一个可指定 size 的 worker pool. 可并发运行多个 task 并且支持额外的任务排队能力。 // Pool is a buffered worker pool type Pool struct { // TODO: taskQueue should be a linked list, so that we can get the task from the head of the list and put it back to the head. // If we use a channel as taskQueue, we can't get the task from the head of the list and put it back to the head. // But make sure that before change it to linked list, we should have the ability run the task in min(taskQueue length, concurrence) goroutines. taskQueue chan *task enqueuedTaskCount atomic.Int32 // count of unhandled tasks bufferSize int // size of taskQueue buffer, means can count of bufferSize task can wait to be handled maxWorker int // count of how many worker run in concurrence workerSets []*workerSet lock *sync.Mutex stopFlag atomic.Bool } ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:2:6","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"实现 上面已经确定需要的能力和基础的数据结构了，下面一个个去实现各个模块的能力。 ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:3:0","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"Worker Implement worker 能力相对纯粹，看看 worker 是如何工作的: func (w *worker) run() { defer w.ws.done() var ec = make(chan error, 1) defer close(ec) go func() { ec \u003c- w.ws.task.tf() }() select { case e := \u003c-ec: w.err = e case \u003c-w.ws.ctx.Done(): } } ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:3:1","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"WorkerSet Implement workerSet 调度 worker，记录 worker 运行状态等。 func newWorkerSet(ctx context.Context, t *task) *workerSet { // 初始化参数 // ... 省略代码 return ws } func (ws *workerSet) run() { ws.task.updateStatus(TaskStatusRunning) for _, w := range ws.workers { ws.addOne() go w.run() } } func (ws *workerSet) stopAll() { ws.cancel() ws.task.updateStatus(TaskStatusKilled) } // err returns the first error that occurred in the workerSet. func (ws *workerSet) err() error { // ...省略代码 return nil } func (ws *workerSet) getRunningWorker() int { return int(ws.runningWorker.Load()) } // done called when worker stop. func (ws *workerSet) done() { ws.addRunningWorker(-1) ws.wg.Done() if ws.getRunningWorker() == 0 { ws.task.updateStatus(TaskStatusDone) } } // addOne called when worker start running. func (ws *workerSet) addOne() { ws.addRunningWorker(1) ws.wg.Add(1) } func (ws *workerSet) wait() { ws.wg.Wait() } ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:3:2","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"Task Implement task 主要是记录 task 的状态，并通过 workerSet 控制其下的 worker. func newTask(tf TaskFunc, concurrence int) *task { return \u0026task{ tf: tf, concurrence: concurrence, } } // Err returns the first error that occurred in the workerSet. func (t *task) Err() error { // check t.ws if nil return nil. if t.ws == nil { return nil } return t.ws.err() } // Wait for task done. // Please make sure task is done or running before call this function. func (t *task) Wait() { // check t.ws if nil. if t.ws == nil { return } t.ws.wait() } // Status returns task status. func (t *task) Status() TaskStatus { return t.status } // Kill task. func (t *task) Kill() { // check t.ws if nil. if t.ws == nil { return } t.ws.stopAll() } func (t *task) assignWorkerSet(ws *workerSet) { t.ws = ws } func (t *task) updateStatus(status TaskStatus) { t.status = status } ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:3:3","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"TaskStatus Implement TaskStatus 虽然实现了 TaskResult 接口，但是不能控制任何 task，其有效的方法只有 Status() 和 Err() func (t TaskStatus) Error() string { return t.String() } func (t TaskStatus) Err() error { switch t { case TaskStatusError, TaskStatusQueueFull, TaskStatusTooManyWorker, TaskStatusPoolClosed, TaskStatusKilled: return t } return nil } func (t TaskStatus) Wait() { // do nothing. } func (t TaskStatus) Status() TaskStatus { return t } func (t TaskStatus) Kill() { // do nothing. } ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:3:4","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"Pool Implement Pool 是总的入口，任务会提交到 Pool, 并由 Pool 创建 task 并调度到 workerSet 上，同时定时清理已完成的 workerSet, 确保空闲 worker 能被合理使用。 func NewPool(workerSize, queueSize int) *Pool { // ... 初始化各个参数 // check p.enqueue to find out why make this channel size with p.bufferSize+1. // p.taskQueue = make(chan *task, p.bufferSize+1) // 启动单独 goroutine 维护队列 go p.consumeQueue() return p } func (p *Pool) Submit(ctx context.Context, tf TaskFunc, concurrence int) TaskResult { if p.stopFlag.Load() { return TaskStatusPoolClosed } if concurrence \u003e p.maxWorker { return TaskStatusTooManyWorker } // check if there has any worker place left p.lock.Lock() defer p.lock.Unlock() t := newTask(tf, concurrence) if p.tryRunTask(ctx, t) { return t } if p.enqueueTask(t, true) { return t } return TaskStatusQueueFull } func (p *Pool) Stop() { // 关闭队列和正在运行的 workerSet } // tryRunTask try to put task into workerSet and run it.Return false if capacity not enough. // Make sure get p.Lock before call this func func (p *Pool) tryRunTask(ctx context.Context, t *task) bool { if p.curRunningWorkerNum()+t.concurrence \u003c= p.maxWorker { ws := newWorkerSet(ctx, t) p.workerSets = append(p.workerSets, ws) // run 为异步方法 ws.run() return true } return false } // curRunningWorkerNum get current running worker num // make sure lock mutex before call this func func (p *Pool) curRunningWorkerNum() int { // ...省略代码 return cnt } // enqueueTask put task to queue. // p.enqueuedTaskCount increase 1 if is new task func (p *Pool) enqueueTask(t *task, isNewTask bool) bool { // ... 省略代码 return true } func (p *Pool) consumeQueue() { var ticker = time.NewTicker(time.Second) for { select { case t, ok := \u003c-p.taskQueue: if !ok { // channel closed return } if p.tryRunTask(context.Background(), t) { p.enqueuedTaskCount.Sub(1) goto unlock } // if enqueueTask return false, means channel is closed. if !p.enqueueTask(t, false) { // channel is closed goto unlock } unlock: p.lock.Unlock() case \u003c-ticker.C: log.Printf(\"current running worker num: %d\", p.curRunningWorkerNum()) } } // never reach here } ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:3:5","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"使用 到这里相关开发基本结束了，有一些 TODO 项后面后补充完善，下面通过 test case 来看一下如何使用这个 worker pool: func TestPool_SubmitOrEnqueue(t *testing.T) { p := NewPool(5, 1) var ( cnt int concurrence = 5 ) tf := func() error { time.Sleep(time.Second) log.Println(\"hello world\") cnt++ return nil } got := p.Submit(context.Background(), tf, concurrence) if got.Status() != TaskStatusRunning { t.Errorf(\"SubmitOrEnqueue() = %v, want %v\", got.Status(), TaskStatusRunning) return } got.Wait() if cnt != concurrence { t.Errorf(\"cnt = %v, want %v\", cnt, concurrence) } if got := p.Submit(context.Background(), tf, concurrence); got.Status() != TaskStatusRunning { t.Errorf(\"SubmitOrEnqueue() = %v, want %v\", got, TaskStatusRunning) return } if got := p.Submit(context.Background(), tf, concurrence); got.Status() != TaskStatusEnqueue { t.Errorf(\"SubmitOrEnqueue() = %v, want %v\", got.Status(), TaskStatusEnqueue) return } if got := p.Submit(context.Background(), tf, 6); got.Status() != TaskStatusTooManyWorker { t.Errorf(\"SubmitOrEnqueue() = %v, want %v\", got.Status(), TaskStatusTooManyWorker) return } if got := p.Submit(context.Background(), tf, concurrence); got.Status() != TaskStatusQueueFull { t.Errorf(\"SubmitOrEnqueue() = %v, want %v\", got.Status(), TaskStatusQueueFull) return } p.Stop() if got := p.Submit(context.Background(), tf, 1); got.Status() != TaskStatusPoolClosed { t.Errorf(\"SubmitOrEnqueue() = %v, want %v\", got.Status(), TaskStatusPoolClosed) return } } ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:4:0","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"总结 到这里这篇文章内容全部结束了，下面做一个简单的总结： 介绍背景和需求 根据需求定义了一组概念：task, worker, workerSet, pool 各个结构之前的关系以及如何实现 最终给出使用的 test case. ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:5:0","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["GoIM"],"content":"链接 🔗 如果想仔细阅读源码，并持续关注这块功能的后续更新优化，请点击这里跳转到 GitHub. ","date":"2022-04-01","objectID":"/posts/goim-worker-pool/:6:0","tags":["goim","go","worker","转载"],"title":"[GoIM] 实现异步并发 worker 队列","uri":"/posts/goim-worker-pool/"},{"categories":["Go"],"content":" 最近意外发现一个文章，说是可以通过 go 语言控制 object-c，从而实现用 go 语言开发出简单的 mac app。我就是试着尝试一下的想法去把文章里说的 repo 下载下来本地运行了一下里面的示例，居然运行成功了。很意外也很惊喜，作为一个后端开发者，用后端语言写出简单的客户端页面简直就是开启了一个新时代的大门一样，以后可以制作一些简单的 mac app，满足自己的需求了（久坐提醒、定时器之类的，一时半会儿想不到太多）。本篇文章讲述如何开发、部署以及需要注意的问题。 ","date":"2022-03-23","objectID":"/posts/macapp-by-go/:0:0","tags":["mac-app","go"],"title":"Golang 开发 mac app","uri":"/posts/macapp-by-go/"},{"categories":["Go"],"content":"1. 背景 最近在帮一个朋友写一些程序把一些繁琐的工作做成自动化，帮他节省时间和精力。但是作为一个后端程序员，做出来的东西对外来说无非就是 http 接口 或者一个可执行的二进制文件。如果 http 接口还好，找个自己的服务器部署上去，写个简单的页面（我能力仅限于简单的页面）交给他人使用即可。但是有些需求可能在对方的电脑运行更方便（比如处理本地的一些文件，或者涉及到敏感信息等），这个时候就麻烦了，我给对方一个二进制文件让他用，对方也是一脸懵逼，运行失败了，报错了或者其他情况对方都不知道发生了什么，就很不友好。 所以我迫切希望一个可以通过后端语言生成一些简单页面化的 app（一开始相关 terminal gui，但还是太 geek）。试着搜了一下 go 语言开发 mac app，居然搜到了一个对我帮助很大的文章（原文连接）。看到里面提到的第二个例子，简直就是我想要的，直接在 mac 的 status bar 多一个入口，点击下来多个菜单，我可以把我开发的能力放到这里，用户一点就触发，就觉得很 nice。 官方例子如下： hello world always on top webview 这是我开发后的效果： status bar 其中状态栏显示的文字，可以在运行时实时更新，这样可以在状态栏就可以看到当前运行情况和进度了。 项目叫 MacDriver，是通过 go 语言调用 mac api的框架。 MacDriver MacDriver is a toolkit for working with Apple/Mac APIs and frameworks in Go. ","date":"2022-03-23","objectID":"/posts/macapp-by-go/:1:0","tags":["mac-app","go"],"title":"Golang 开发 mac app","uri":"/posts/macapp-by-go/"},{"categories":["Go"],"content":"2. 开发 我本人对 Mac APP 的开发以及 Mac 的 API 几乎完全不懂，所以本项目对这现有的 example 慢慢啃下来然后实现了自己的需求。 而 MacDriver 项目提供的能力和能做出来的东西远比我在这里实现的复杂和高级，如果有同学对这个十分感兴趣可以先看看项目的源码，大概了解一下已有的能力。 我需求比较简单，就是拉取最近未读邮件然后对其中需要处理的（自己指定了一些匹配规则）进行后台处理并回复一条自动邮件。 因为我的处理需求和匹配规则跟邮件内容有关，所以没办法使用邮箱提供的收信规则简单处理，所以自己动手写了一个程序。 ","date":"2022-03-23","objectID":"/posts/macapp-by-go/:2:0","tags":["mac-app","go"],"title":"Golang 开发 mac app","uri":"/posts/macapp-by-go/"},{"categories":["Go"],"content":"2.1. 初始化 APP func main() { runtime.LockOSThread() cocoa.TerminateAfterWindowsClose = false app := cocoa.NSApp_WithDidLaunch(func(n objc.Object) { // all code in here } app.Run() } 所有的逻辑在 cocoa.NSApp_WithDidLaunch 传参的函数里。 ","date":"2022-03-23","objectID":"/posts/macapp-by-go/:2:1","tags":["mac-app","go"],"title":"Golang 开发 mac app","uri":"/posts/macapp-by-go/"},{"categories":["Go"],"content":"2.2. 初始化 status bar 这里是定义程序启动时，默认是展示文字。 app := cocoa.NSApp_WithDidLaunch(func(n objc.Object) { obj := cocoa.NSStatusBar_System().StatusItemWithLength(cocoa.NSVariableStatusItemLength) obj.Retain() obj.Button().SetTitle(\"📧 准备就绪\") // 初始化 status bar 的展示文本 // ...省略 code } ","date":"2022-03-23","objectID":"/posts/macapp-by-go/:2:2","tags":["mac-app","go"],"title":"Golang 开发 mac app","uri":"/posts/macapp-by-go/"},{"categories":["Go"],"content":"2.3. 运行时动态更新 status bar 运行过程中我希望能实时更新处理的进度以及状态。 app := cocoa.NSApp_WithDidLaunch(func(n objc.Object) { obj := cocoa.NSStatusBar_System().StatusItemWithLength(cocoa.NSVariableStatusItemLength) obj.Retain() obj.Button().SetTitle(\"📧 准备就绪\") var ( eventChan = make(chan string, 1) indexChan = make(chan int, count) ) go func() { for { select { case \u003c-time.After(1 * time.Second): case e := \u003c-eventChan: // 这里我更新各类事件的实时情况和状态 core.Dispatch(func() { obj.Button().SetTitle(fmt.Sprintf(\"🏷 %s\", e)) }) case i := \u003c-indexChan: // 这里我实时更新处理到第几封邮件 core.Dispatch(func() { obj.Button().SetTitle(fmt.Sprintf(\"✴️ 处理邮件中 %d/%d\", i, count)) }) } } }() // .. 省略 code ","date":"2022-03-23","objectID":"/posts/macapp-by-go/:2:3","tags":["mac-app","go"],"title":"Golang 开发 mac app","uri":"/posts/macapp-by-go/"},{"categories":["Go"],"content":"2.4. 添加 menu 上面初始化了展示的 status bar 的文字，现在我们添加 menu 菜单，不同的 menu 处理不同的事件。 app := cocoa.NSApp_WithDidLaunch(func(n objc.Object) { // ... 省略code // set quit action itemQuit := cocoa.NSMenuItem_New() itemQuit.SetTitle(\"退出\") itemQuit.SetAction(objc.Sel(\"terminate:\")) // 设置自定义 menu 和处理方法 checkAndSetSeen := cocoa.NSMenuItem_New() checkAndSetSeen.SetTitle(fmt.Sprintf(\"处理最新%d封邮件✉️(并且设为已读)\", count)) checkAndSetSeen.SetAction(objc.Sel(\"checkAndSet:\")) cocoa.DefaultDelegateClass.AddMethod(\"checkAndSet:\", func(_ objc.Object) { // 这里就可以放我们自己的逻辑了 go func() { defer deferFunc(obj) log.Println(\"email start\") run(indexChan, eventChan, onlyCheckMode|setSeenMode) }() }) setAndReply := cocoa.NSMenuItem_New() setAndReply.SetTitle(fmt.Sprintf(\"处理最新%d封邮件✉️(并且设为已读和回复邮件)\", count)) setAndReply.SetAction(objc.Sel(\"setAndReply:\")) cocoa.DefaultDelegateClass.AddMethod(\"setAndReply:\", func(_ objc.Object) { go func() { defer deferFunc(obj) log.Println(\"email start\") run(indexChan, eventChan, onlyCheckMode|setSeenMode|replyMailMode) }() }) // menu 注册进去 menu := cocoa.NSMenu_New() menu.AddItem(checkAndSetSeen) menu.AddItem(setAndReply) menu.AddItem(itemQuit) obj.SetMenu(menu) } 到这里 status bar 的开发就完成了，业务逻辑代码我就不贴了。 ","date":"2022-03-23","objectID":"/posts/macapp-by-go/:2:4","tags":["mac-app","go"],"title":"Golang 开发 mac app","uri":"/posts/macapp-by-go/"},{"categories":["Go"],"content":"3. 编译部署 我一开始以为是需要各类的开发者账号或者 xcode 才能将代码运行起来，但是实际上简单的让人我怀疑（因为我知道苹果由于生态封闭 app 的开发就比较复杂） 编译： go build main.go 运行： ./main 这就 OK 了，完全不需要其他任何操作，非常清爽。 ","date":"2022-03-23","objectID":"/posts/macapp-by-go/:3:0","tags":["mac-app","go"],"title":"Golang 开发 mac app","uri":"/posts/macapp-by-go/"},{"categories":["Go"],"content":"4. 总结 本篇讲述内容如下： 讲述用 go 开发 mac app 的背景 介绍基于 go 语言调用 Mac api 的开源库 MacDriver 讲述基于 MacDriver 开发一个简单状态栏 app 的过程 讲述如何编译部署开发的 app ","date":"2022-03-23","objectID":"/posts/macapp-by-go/:4:0","tags":["mac-app","go"],"title":"Golang 开发 mac app","uri":"/posts/macapp-by-go/"},{"categories":["Go"],"content":"5.链接🔗 MacDriver Go 终于可以开发原生 Mac APP 了 ","date":"2022-03-23","objectID":"/posts/macapp-by-go/:5:0","tags":["mac-app","go"],"title":"Golang 开发 mac app","uri":"/posts/macapp-by-go/"},{"categories":["Docker"],"content":" 最近在某个项目中需要引入RocketMQ来处理一些业务逻辑，然后由于还没上线，需要本地搭建开发环境，由于第一次接触 RocketMQ 所以在部署的过程有些曲折，通过这篇文章记录部署过程和遇到的一些问题，希望对看到的读者有所帮助。 ","date":"2022-03-14","objectID":"/posts/rocketmq-deploy/:0:0","tags":["docker","rocketmq"],"title":"Docker 环境部署 RocketMQ","uri":"/posts/rocketmq-deploy/"},{"categories":["Docker"],"content":"1. 背景 由于业务需求需要搭建一个本地开发环境，而我几乎所有的依赖（各种数据库，工具之类的）都基于 docker 运行，方便随时启停和更新，同时保证宿主机上不会安装各类不常用的环境。 在需要一个 RocketMQ 的时候 我也是习惯性的直接搜其镜像，但是让我比较意外的是，作为一个 Apache 项目，其镜像最后更新时间居然在2年前。 而我不知道要不要用这个镜像以及怎么用的时候又发现了官方的一个 repo：apache/rocketmq-docker。 这个 repo 提供了如何自己打包一个镜像或者用官方的镜像在不同环境下部署 RocketMQ，包括单节点，docker-compose 以及 k8s 环境。 因为官方的镜像确实比较老旧，最新才 4.6.0，而最新版本以及到 4.9.x 了，我本来想自己打包一个最新的镜像来着。但是看到需要配置一堆 JAVA 环境，我就放弃了，一是不熟悉 JAVA 的配置，另外一方面我不想为了这个浪费我太多时间，所以选择以 4.6.0 版本下部署一个本地环境。 ","date":"2022-03-14","objectID":"/posts/rocketmq-deploy/:1:0","tags":["docker","rocketmq"],"title":"Docker 环境部署 RocketMQ","uri":"/posts/rocketmq-deploy/"},{"categories":["Docker"],"content":"2. 部署 关于本地部署的步骤，我们需要先将 apache/rocketmq-docker 这个 repo 下载到本地。 cd rocketmq-docker sh stage.sh 4.6.0 执行该脚本后，会生成一个 statge/4.6.0 的目录，并在目录下会有不同类型部署方式相关的脚本和需要的配置文件，我们现在只关注 docker-compose 相关的。 ~/workspace/rocketmq-docker/stages/4.6.0 on  master at 17:21:09 ➜ ls -al total 56 drwxr-xr-x 13 shan.yu staff 416 Mar 14 11:57 . drwxr-xr-x 3 shan.yu staff 96 Mar 14 10:24 .. drwxr-xr-x 6 shan.yu staff 192 Mar 14 10:24 data # 单节点部署方式需要的配置目录 drwxr-xr-x 5 shan.yu staff 160 Mar 14 12:04 docker-compose # docker-compose 部署方式需要的配置目录 drwxr-xr-x 4 shan.yu staff 128 Mar 14 10:24 kubernetes # k8s署方式需要的配置目录 -rwxr-xr-x 1 shan.yu staff 902 Mar 14 10:24 play-consumer.sh -rwxr-xr-x 1 shan.yu staff 1497 Mar 14 10:24 play-docker-compose.sh # docker-compose 部署脚本 -rwxr-xr-x 1 shan.yu staff 3201 Mar 14 10:24 play-docker-dledger.sh -rwxr-xr-x 1 shan.yu staff 2271 Mar 14 10:24 play-docker-tls.sh -rwxr-xr-x 1 shan.yu staff 2354 Mar 14 10:24 play-docker.sh -rwxr-xr-x 1 shan.yu staff 947 Mar 14 10:24 play-kubernetes.sh -rwxr-xr-x 1 shan.yu staff 901 Mar 14 10:24 play-producer.sh drwxr-xr-x 17 shan.yu staff 544 Mar 14 10:24 ssl 从目录结构可以看到，我们只需要关注 docker-compose 目录和 play_docker-compose.sh 脚本即可。 docker-compose 目录下有一个 yaml 文件，这个就是 docker-compose 的配置文件，我们来看一下： version: '2' services: #Service for nameserver namesrv: image: apacherocketmq/rocketmq:4.6.0 container_name: rmqnamesrv ports: - 9876:9876 volumes: - ./data/namesrv/logs:/home/rocketmq/logs command: sh mqnamesrv #Service for broker broker: image: apacherocketmq/rocketmq:4.6.0 container_name: rmqbroker links: - namesrv ports: - 10909:10909 - 10911:10911 - 10912:10912 environment: - NAMESRV_ADDR=rmqnamesrv:9876 volumes: - ./data/broker/logs:/home/rocketmq/logs - ./data/broker/store:/home/rocketmq/store - ./data/broker/conf/broker.conf:/opt/rocketmq-4.6.0/conf/broker.conf command: sh mqbroker -n rmqnamesrv:9876 -c /opt/rocketmq-4.6.0/conf/broker.conf 本来是启动的两个 broker, 我删掉一个，只保留一个 namesever 和一个 broker。 ./data/broker/conf/broker.conf 为 broker 的配置文件路径，也是最开始执行的脚本生成的，我们可以看一下： brokerClusterName = DefaultCluster brokerName = broker-a brokerId = 0 deleteWhen = 04 fileReservedTime = 48 brokerRole = ASYNC_MASTER flushDiskType = ASYNC_FLUSH ## 注意！默认是不生成这一行的，但是你 docker 环境部署的话，broker 把自己注册到 nameserver 的时候用的是containerIP ## 类似 172.0.2.3 这种，这就导致你使用本地程序连 broker 会连接超时，我在这块卡了很久。 ## 解决方案就是 将docker 宿主机执行 ifconfig 后，得到的本机 ip（不是 127.0.0.1）配到这里，从而解决这个问题。 brokerIP1=10.12.220.222 需要主要上面的 brokerIP1 的配置，其他的都不用管。 此时直接执行 ./play-docker-compose.sh 就可以了，等着镜像下完启动。启动完后，通过 docker ps 确认一下是否启动成功： ➜ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9653b92c46b5 apacherocketmq/rocketmq:4.6.0 \"sh mqbroker -n rmqn…\" 4 hours ago Up 4 hours 0.0.0.0:10909-\u003e10909/tcp, 9876/tcp, 0.0.0.0:10911-10912-\u003e10911-10912/tcp rmqbroker e34ca8cbf7fa apacherocketmq/rocketmq:4.6.0 \"sh mqnamesrv\" 4 hours ago Up 4 hours 10909/tcp, 0.0.0.0:9876-\u003e9876/tcp, 10911-10912/tcp rmqnamesrv 4957ecb3adc5 apacherocketmq/rocketmq-dashboard:latest \"sh -c 'java $JAVA_O…\" 7 hours ago Up 7 hours 0.0.0.0:8080-\u003e8080/tcp rocketmq-dashboard 上面发现多了一个 rocketmq-dashboard, 这是一个界面化管理的 UI 程序。因为自动生成的 docker-compose 的 yaml 文件里没有包含，我们可以自己单独运行或者补到 yaml 文件里，下面两种方式都提供。 直接 docker 运行： ➜ docker run -d --name rocketmq-dashboard --network docker-compose_default -e \"JAVA_OPTS=-Drocketmq.namesrv.addr=rmqnamesrv:9876\" -p 8080:8080 -t apacherocketmq/rocketmq-dashboard:latest 添加到 docker compose yaml 中： rocketmq-dashboard: image: apacherocketmq/rocketmq-dashboard:latest container_name: rocketmq-dashboard links: - namesrv ports: - 8080:8080 environment: - JAVA_OPTS=-Drocketmq.namesrv.addr=rmqnamesrv:9876 这样你从本地 8080 端口就能看到 UI 页面： ","date":"2022-03-14","objectID":"/posts/rocketmq-deploy/:2:0","tags":["docker","rocketmq"],"title":"Docker 环境部署 RocketMQ","uri":"/posts/rocketmq-deploy/"},{"categories":["Docker"],"content":"3. 遇到的问题 ","date":"2022-03-14","objectID":"/posts/rocketmq-deploy/:3:0","tags":["docker","rocketmq"],"title":"Docker 环境部署 RocketMQ","uri":"/posts/rocketmq-deploy/"},{"categories":["Docker"],"content":"3.1. 没有最新的镜像 通过 RocketMQ 发现目前最新版本以及到 v4.9.3， 而目前可用的镜像最新也是 v4.6.0，这个对于版本要求高的同学来说是一个麻烦的事儿，因为在本地直接执行最新代码需要以下环境： Prerequisite The following softwares are assumed installed: 64bit OS, Linux/Unix/Mac is recommended;(Windows user see guide below) 64bit JDK 1.8+; Maven 3.2.x; Git; 4g+ free disk for Broker server 我最后放弃使用最新版本了，也没有去研究 4.6.0 到 4.9.3 这个跨度有了什么比较大的更新，感兴趣的同学可以去官网了解一下。 ","date":"2022-03-14","objectID":"/posts/rocketmq-deploy/:3:1","tags":["docker","rocketmq"],"title":"Docker 环境部署 RocketMQ","uri":"/posts/rocketmq-deploy/"},{"categories":["Docker"],"content":"3.2. docker 环境宿主机上程序无法正常连接 使用官方的 rocketmq-docker repo 生成了 docker-compose 文件后,直接执行发现本机程序连不上，报错信息大概如下： ERRO[6243] get consumer list of group from broker error broker=\"172.18.0.3:10911\" consumerGroup=push_msg underlayError=\"dial tcp 172.18.0.3:10911: i/o timeout\" WARN[6243] do balance in group failed, get consumer id list failed consumerGroup=push_msg topic=\"%RETRY%push_msg\" ERRO[6246] get consumer list of group from broker error broker=\"172.18.0.3:10911\" consumerGroup=push_msg underlayError=\"dial tcp 172.18.0.3:10911: i/o timeout\" WARN[6246] do balance in group failed, get consumer id list failed consumerGroup=push_msg topic=def_topic WARN[6249] send heart beat to broker error underlayError=\"dial tcp 172.18.0.3:10911: i/o timeout\" 我一看这个 ip 就知道问题出在哪儿了，因为程序连接的是 nameserver 的端口，从 nameserver 服务拿到 broker 的 ip 端口再去连 broker，所以拿到了 broker 的 containerIP。 这个问题好在官方 repo 的 README 最后写了，需要配置宿主机的 ip，我配置完重新启动就 OK 了。 ","date":"2022-03-14","objectID":"/posts/rocketmq-deploy/:3:2","tags":["docker","rocketmq"],"title":"Docker 环境部署 RocketMQ","uri":"/posts/rocketmq-deploy/"},{"categories":["Docker"],"content":"4. 总结 本篇文件是讲述的内容重点如下： rocketmq docker 镜像现状 如何通过官方工具本地部署 rocketmq 部署和使用过程遇到的问题 如何部署最新rocketmq ","date":"2022-03-14","objectID":"/posts/rocketmq-deploy/:4:0","tags":["docker","rocketmq"],"title":"Docker 环境部署 RocketMQ","uri":"/posts/rocketmq-deploy/"},{"categories":["Docker"],"content":"5. 链接🔗 RocketMQ官方 RocketMQ-Docker ","date":"2022-03-14","objectID":"/posts/rocketmq-deploy/:5:0","tags":["docker","rocketmq"],"title":"Docker 环境部署 RocketMQ","uri":"/posts/rocketmq-deploy/"},{"categories":["代码技巧"],"content":" 本篇介绍一个如何在 go 语言环境下，如何解析/读取 pdf 文件内容从而进行一些业务逻辑。本篇将会介绍两种方案，可以按自己的需求进行对比和最终选择。 ","date":"2022-03-04","objectID":"/posts/pdf-reader/:0:0","tags":["go","java","docker","pdf"],"title":"Go 语言实现读取 pdf 文件内容","uri":"/posts/pdf-reader/"},{"categories":["代码技巧"],"content":"1. 背景 最近在帮朋友做一个小的程序，帮他减少一些人工繁琐的工作，将一些机器可以做的事情交给机器，提高效率他效率。 需求也相对简单，就是从大量 pdf/docx 文件内容中读取一些关键信息（这些信息有一定的规律），并输出一个 Excel 表格。听到这个需求我第一个想法是可以做的，我之前在一些招聘网站也是看过解析 pdf 内容的功能。最典型的就是在招聘网站上传个人简历时，会读出来个人信息以及其他信息。既然市面上有人这么干，那就说是肯定有一定的解决方案可以供我利用的。 接下来就是去找解决方案了，调研了一下午后，目标基本明确了。有两个项目进入了最终决赛圈，我需要分别使用，并把一些优缺点列出来就能确定最终胜者了。 这两个方案分别是: ledongthuc/pdf go语言实现的 pdf 解析库。有不少的 star 和 fork，并且从 demo 上看到确实能读取到内容。 apache/tika Java 实现的 pdf 解析库。之所以能进入决赛圈是因为网上大量人推荐并且是 apache 的项目，所以我也比较放心使用。 ","date":"2022-03-04","objectID":"/posts/pdf-reader/:1:0","tags":["go","java","docker","pdf"],"title":"Go 语言实现读取 pdf 文件内容","uri":"/posts/pdf-reader/"},{"categories":["代码技巧"],"content":"2. go 语言方案 首先对 ledongthuc/pdf 进行使用测试。先上代码： package main import ( \"bytes\" \"fmt\" \"github.com/ledongthuc/pdf\" ) func main() { pdf.DebugOn = true content, err := readPdf(\"test.pdf\") // Read local pdf file if err != nil { panic(err) } fmt.Println(content) return } func readPdf(path string) (string, error) { f, r, err := pdf.Open(path) // remember close file defer f.Close() if err != nil { return \"\", err } var buf bytes.Buffer b, err := r.GetPlainText() if err != nil { return \"\", err } buf.ReadFrom(b) return buf.String(), nil } 这是一段项目自己提供 demo，我本地确实也跑起来了。但是我当对一些我朋友提供的测试案例 pdf 文件进行解析时，遇到了问题，报了下面的错： malformed PDF: reading at offset 0: stream not present 我第一反应是可能我的 pdf 不是很标准 pdf 格式导致，然后去项目 issue 里看到类似的问题，并且作者也给了解决方案，明显比较麻烦的一种解决方案。因为需要对文件进行一些处理，这个方案我不是很能接受。不过我找一些网上标准(所谓的标准就是去找了一些权威的网站上的 pdf 文件)的 pdf 文件的时候，的确能读出内容。这个方案我先标注为 backup, 实在没辙我再回来折腾。 ","date":"2022-03-04","objectID":"/posts/pdf-reader/:2:0","tags":["go","java","docker","pdf"],"title":"Go 语言实现读取 pdf 文件内容","uri":"/posts/pdf-reader/"},{"categories":["代码技巧"],"content":"3. tika + go client 方案 apache/tika 是一个比较万能的工具集，其能力可以参考官方的说明。 apache/tika Apache Tika - a content analysis toolkit The Apache Tika™ toolkit detects and extracts metadata and text from over a thousand different file types (such as PPT, XLS, and PDF). All of these file types can be parsed through a single interface, making Tika useful for search engine indexing, content analysis, translation, and much more. 虽然说是Java 项目，我不能直接引入到我的 go 项目使用，但是人家提供了 API，这就非常的 nice 了。然后几乎同时我发现了 Google 的一个项目 go-tika, tika 的 go 语言版 client 端，也就是我不用去写调用 api 相关代码了，简直又省了我不少时间。 ","date":"2022-03-04","objectID":"/posts/pdf-reader/:3:0","tags":["go","java","docker","pdf"],"title":"Go 语言实现读取 pdf 文件内容","uri":"/posts/pdf-reader/"},{"categories":["代码技巧"],"content":"3.1. 部署 因为 tika 是个 server 端项目，那我得用的时候需要把他跑起来才行，但是我本地没有 Java 环境，我也不想去折腾（自己又不懂，瞎搞绝对浪费时间还搞不定）。所以我转向了万能的 docker 部署。找到 tika 的 docker 镜像，本地启动就可以用了。 $ docker run -d -p 9998:9998 apache/tika:latest 部署完事儿。 ","date":"2022-03-04","objectID":"/posts/pdf-reader/:3:1","tags":["go","java","docker","pdf"],"title":"Go 语言实现读取 pdf 文件内容","uri":"/posts/pdf-reader/"},{"categories":["代码技巧"],"content":"3.2. 使用 使用方面也是非常简单，有了 go-tika 项目的加成，我需要写的代码几乎很少，代码如下： package main import ( \"fmt\" \"flag\" \"context\" \"os\" \"github.com/google/go-tika/tika\" ) func main() { var filePath string flag.StringVar(\u0026filePath, \"fp\", \"\", \"pdf file path.\") flag.Parse() if filePath == \"\" { panic(\"file path must be provided\") } content, err := readPdf(filePath) // Read local pdf file if err != nil { panic(err) } fmt.Println(content) return } func readPdf(path string) (string, error) { f, err := os.Open(path) defer f.Close() if err != nil { return \"\", err } client := tika.NewClient(nil, \"http://127.0.0.1:9998\") return client.Parse(context.TODO(), f) } 响应速度完全在可接受范围内（30ms 左右），而且更让我惊喜的是，之前在用 go 语言的库解析出错的 pdf 在这里完全没问题，没有漏掉一个字的给我解析出来了。可以说是完全达到我的要求了。 ","date":"2022-03-04","objectID":"/posts/pdf-reader/:3:2","tags":["go","java","docker","pdf"],"title":"Go 语言实现读取 pdf 文件内容","uri":"/posts/pdf-reader/"},{"categories":["代码技巧"],"content":"4. 总结 从我个人的需求上面来说 tika 是完胜的，但是速度上的确没有 ledongthuc/pdf 快，但是只是快并不能解决问题。ledongthuc/pdf 读取标准的 pdf 没问题，如果你的需求是读取一些标准的 pdf ，那 ledongthuc/pdf 绝对比 tika 好用的，又快又不需要搭建一个 server 端。 如果你遇到的 pdf 不能保证来源和质量，那 tika 更适合，读取内容的概率更高，我自己测了大概 100 来份各种 pdf，成功率也很高，除非是非常模糊的 pdf，否则都能读取到内容，而且延迟 30ms 对于一般需求来说是足够的。况且是 docker 部署的无状态服务，如果你要大量的 pdf 需要处理，那就多搭建几个 server 端同时处理就好了。 其他方面的对比 ，可以参考这篇文章：https://www.jianshu.com/p/68609f51b6e7 讲得比较细，感兴趣的可以去阅读。 如果你有不同的想法，那再评论区见吧~ ","date":"2022-03-04","objectID":"/posts/pdf-reader/:4:0","tags":["go","java","docker","pdf"],"title":"Go 语言实现读取 pdf 文件内容","uri":"/posts/pdf-reader/"},{"categories":["代码技巧"],"content":" 本篇介绍一个用 go 实现的连接池，针对连接的生命周期的管理十分有帮助。本篇从连接池的设计到实现以及常用场景进行详解。 ","date":"2022-02-22","objectID":"/posts/conn-pool/:0:0","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"1. 背景 连接池 可以说是在开发中非常的常见，各类我们需要与远端保持长连接从而提高服务性能（减少建立连接过程）。 如： 数据库连接池（MySQL，Redis等） 消息队列的连接池（即producer端提前建立连接，提升消息产生速率） 与其他远端服务保持长连接 如果使用一些常见的组件，其 client 端其实已经做好连接池了，我们初始化的时候仅需要设计池子大小，空闲时间等参数就可以用了。如果我们用的客户端没有做连接池或者因为种种原因我们需要一个连接池的时候应该怎么，怎么说设计好呢？ 下面我们开始讲如何设计以及如何实现一个连接池。 ","date":"2022-02-22","objectID":"/posts/conn-pool/:1:0","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"2. 设计 我们先梳理我的需求，连接池都有什么功能，应该有哪些接口可调用呢？ Get 获取一个连接 Put 连接放回去 Close 释放/关闭连接 这两个属于是核心的能力了，我能拿到一个可用的连接，并且用完放回去或者我需要的话 这个连接能被关闭。除此之外，应该还有一个整个连接池释放的能力，程序退出是需要释放所有的连接。 那我们定义一下这个 interface: // Pool 基本方法 type Pool interface { // 获取资源 Get() (interface{}, error) // 资源放回去 Put(interface{}) error // 关闭资源 Close(interface{}) error // 释放所有资源 Release() // 返回当前池子内有效连接数量 Len() int } 这个就是一个连接池应该对外提供的能力，对于使用者来说足矣。 问 从连接池拿到的连接是从哪儿来的？ 这个问题是不是也非常重要，光有连接池还不够，需要一个工厂可以生成一个新的连接，我需要的时候能从这个工厂生成一个连接并放入连接池供使用者获取。 定义一个工厂的 interface: // ConnectionFactory 连接工厂 type ConnectionFactory interface { //生成连接的方法 Factory() (interface{}, error) //关闭连接的方法 Close(interface{}) error //检查连接是否有效的方法 Ping(interface{}) error } 工厂的定义很简单，只需要一个产生连接的方法，并且能关闭这个连接和能对这个连接进行探活的方法即可。 定义了 interface 之后好处在于，我们可以自己实现连接池和工厂，也可以只实现连接池，工厂由使用者去实现，这样我们的连接池变得更加灵活通用。 ","date":"2022-02-22","objectID":"/posts/conn-pool/:2:0","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"3. 实现 实现连接池我们需要考虑的问题比设计更全面，不能光考虑能拿/放连接就行，还得考虑管理这些连接并且支持最大连接数、最大空闲连接数、初始连接数以及连接的超时不可用等情况。 ","date":"2022-02-22","objectID":"/posts/conn-pool/:3:0","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"3.1. 结构体 下面先设计数据结构： // channelPool 存放连接信息 type channelPool struct { mu sync.RWMutex conns chan *idleConn // 存储最大空闲连接 factory ConnectionFactory // 工厂 idleTimeout, waitTimeOut time.Duration // 连接空闲超时和等待超时 maxActive int // 最大连接数 openingConns int // 活跃的连接数 connReqs []chan connReq // 连接请求缓冲区，如果无法从 conns 取到连接，则在这个缓冲区创建一个新的元素，之后连接放回去时先填充这个缓冲区 } type idleConn struct { conn interface{} t time.Time } 不难发现，没有字段去存所有的连接，仅存了最大空闲连接数，也就是拿的连接超过最大空闲连接数的时候，只会产生一个新的连接返回给使用者，但是不会在任何字段去存这个新的产生的连接。为什么这么做呢？ 答：最大空闲连接数 这个概念就很明确，我最多只会保留这么多空闲连接，超过这个数的空闲连接直接释放。但是不影响我使用更多的连接，限制最大连接数的字段是 maxActive,总连接数在这个限制之下可以一直产生使用。使用的时候可以配置最大连接数 1000，而最大空闲设置为 10，这样空闲时刻不会占用太多的资源，而使用高峰的时候又可以产生达到 1000 个的连接来用。 ","date":"2022-02-22","objectID":"/posts/conn-pool/:3:1","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"3.2. 初始化 下面实现初始化连接池相关能力： // PoolConfig 连接池相关配置 type PoolConfig struct { //连接池中拥有的最小连接数 InitialCap int //最大并发存活连接数 MaxCap int //最大空闲连接 MaxIdle int // 工厂 Factory ConnectionFactory //连接最大空闲时间，超过该事件则将失效 IdleTimeout time.Duration } // NewChannelPool 初始化连接 func NewChannelPool(poolConfig *PoolConfig) (Pool, error) { // 校验参数 if !(poolConfig.InitialCap \u003c= poolConfig.MaxIdle \u0026\u0026 poolConfig.MaxCap \u003e= poolConfig.MaxIdle \u0026\u0026 poolConfig.InitialCap \u003e= 0) { return nil, errors.New(\"invalid capacity settings\") } // 校验参数 if poolConfig.Factory == nil { return nil, errors.New(\"invalid factory interface settings\") } c := \u0026channelPool{ conns: make(chan *idleConn, poolConfig.MaxIdle), // 最大空闲连接数 factory: poolConfig.Factory, idleTimeout: poolConfig.IdleTimeout, maxActive: poolConfig.MaxCap, openingConns: poolConfig.InitialCap, } // 初始化初始连接放入 channel 中 for i := 0; i \u003c poolConfig.InitialCap; i++ { conn, err := c.factory.Factory() if err != nil { c.Release() return nil, fmt.Errorf(\"factory is not able to fill the pool: %s\", err) } c.conns \u003c- \u0026idleConn{conn: conn, t: time.Now()} } return c, nil } 上面我们定义了一个配置的结构体，方便外部传参。不仅支持了最大连接数、空闲连接数、连接超时，还支持了初始连接数，方便初始化快速使用。 ","date":"2022-02-22","objectID":"/posts/conn-pool/:3:2","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"3.3. 获取连接 现在实现获取连接的过程： // Get 从pool中取一个连接 func (c *channelPool) Get() (interface{}, error) { conns := c.getConns() if conns == nil { return nil, ErrClosed } for { select { // 优先从空闲连接缓冲取 case wrapConn := \u003c-conns: if wrapConn == nil { return nil, ErrClosed } //判断是否超时，超时则丢弃 if timeout := c.idleTimeout; timeout \u003e 0 { if wrapConn.t.Add(timeout).Before(time.Now()) { //丢弃并关闭该连接 _ = c.Close(wrapConn.conn) continue } } //判断是否失效，失效则丢弃，如果用户没有设定 ping 方法，就不检查 if err := c.Ping(wrapConn.conn); err != nil { _ = c.Close(wrapConn.conn) continue } return wrapConn.conn, nil default: // 没有空闲连接 c.mu.Lock() log.Printf(\"openConn %v %v\", c.openingConns, c.maxActive) // 判断连接数是否达到上限 if c.openingConns \u003e= c.maxActive { req := make(chan connReq, 1) // 如果达到上限，则创建一个缓冲位置，等待放回去的连接 c.connReqs = append(c.connReqs, req) c.mu.Unlock() // 判断是否有连接放回去（放回去逻辑在 put 方法内） ret, ok := \u003c-req // 如果没有连接放回去，则不能再创建新的连接了，因为达到上限了 if !ok { return nil, ErrMaxActiveConnReached } // 如果有连接放回去了 判断连接是否可用 if timeout := c.idleTimeout; timeout \u003e 0 { if ret.idleConn.t.Add(timeout).Before(time.Now()) { //丢弃并关闭该连接 // 重新尝试获取连接 _ = c.Close(ret.idleConn.conn) continue } } return ret.idleConn.conn, nil } // 到这里说明 没有空闲连接 \u0026\u0026 连接数没有达到上限 可以创建新连接 if c.factory == nil { c.mu.Unlock() return nil, ErrClosed } conn, err := c.factory.Factory() if err != nil { c.mu.Unlock() return nil, err } // 连接数+1 c.openingConns++ c.mu.Unlock() return conn, nil } } } 需要注意 if c.openingConns \u003e= c.maxActive { 这块的逻辑，当连接数达到上限时，不用马上报错，可以通过 connReqs 来复用用完还没 release 的连接，从而节约一个连接 release 然后重新建立连接的时间和资源。 ","date":"2022-02-22","objectID":"/posts/conn-pool/:3:3","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"3.4. 连接放回 设计连接池的时候设计了放回连接的接口，当使用者拿到一个连接用完后需要放回去，这个连接根据情况会立刻给新的获取连接请求使用，也可能放到空闲连接的缓存或者释放。 实现代码如下： // Put 将连接放回pool中 func (c *channelPool) Put(conn interface{}) error { if conn == nil { return errors.New(\"connection is nil. rejecting\") } c.mu.Lock() defer c.mu.Unlock() if c.conns == nil { return c.Close(conn) } // 如果有请求连接的缓冲区有等待，则按顺序有限个先来的请求分配当前放回的连接 if l := len(c.connReqs); l \u003e 0 { req := c.connReqs[0] copy(c.connReqs, c.connReqs[1:]) c.connReqs = c.connReqs[:l-1] req \u003c- connReq{ idleConn: \u0026idleConn{conn: conn, t: time.Now()}, } return nil } // 如果没有等待的缓冲则尝试放入空闲连接缓冲 select { case c.conns \u003c- \u0026idleConn{conn: conn, t: time.Now()}: return nil default: //连接池已满，直接关闭该连接 return c.Close(conn) } } ","date":"2022-02-22","objectID":"/posts/conn-pool/:3:4","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"3.5. 释放连接 释放连接有两种情况，释放单个连接和释放整个连接池，而底层只实现释放单个连接的逻辑，释放连接池则一个个调用这个释放方法即可。 实现逻辑如下： // Close 关闭单条连接 func (c *channelPool) Close(conn interface{}) error { if conn == nil { return errors.New(\"connection is nil. rejecting\") } c.mu.Lock() defer c.mu.Unlock() // 连接数减一 c.openingConns-- // 调用工厂的关闭方法 return c.factory.Close(conn) } // Release 释放连接池中所有连接 func (c *channelPool) Release() { c.mu.Lock() conns := c.conns c.conns = nil c.mu.Unlock() defer func() { c.factory = nil }() if conns == nil { return } close(conns) for wrapConn := range conns { //log.Printf(\"Type %v\\n\",reflect.TypeOf(wrapConn.conn)) _ = c.factory.Close(wrapConn.conn) } } ","date":"2022-02-22","objectID":"/posts/conn-pool/:3:5","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"3.6. 其他方法 实现连接池的 Ping 和 Len 方法也很简单 直接上代码。 // Ping 检查单条连接是否有效 func (c *channelPool) Ping(conn interface{}) error { if conn == nil { return errors.New(\"connection is nil. rejecting\") } return c.factory.Ping(conn) } // Len 连接池中已有的连接 func (c *channelPool) Len() int { return len(c.getConns()) } ","date":"2022-02-22","objectID":"/posts/conn-pool/:3:6","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"4. 实际使用场景 以一个消息队列的 producer 的连接池为例，下面说明如何使用。 ","date":"2022-02-22","objectID":"/posts/conn-pool/:4:0","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"4.1. 初始化 初始化指定配置参数即可： pc := \u0026util.PoolConfig{ InitialCap: int(cfg.ProducerConnPoolSize), MaxCap: int(cfg.ProducerConnPoolSize), MaxIdle: int(cfg.ProducerConnPoolSize), Factory: \u0026producerFactory{addr: cfg.MQURL}, // producerFactory 实现了工厂的接口 底层为创建 tcp 连接 IdleTimeout: time.Minute * 5, } pool, err := util.NewChannelPool(pc) if err != nil { return } ","date":"2022-02-22","objectID":"/posts/conn-pool/:4:1","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"4.2. 使用 使用方法如下： func (c *Client) getProducer() (p *mq.Producer, err error) { if c.producerPool == nil { err = fmt.Errorf(\"producer pool is not initialized\") return } v, err := c.producerPool.Get() if err != nil { return } p, ok := v.(*mq.Producer) if !ok { err = fmt.Errorf(\"cannot load producer from pool\") return } return p, nil } func (c *Client) putProducer(p *mq.Producer) { _ = c.producerPool.Put(p) } // Publish publish msg to topic and wait for the response. func (c *Client) Publish(topic string, body []byte) error { p, err := c.getProducer() if err != nil { return err } defer c.putProducer(p) return p.Publish(topic, body) } func (c *Client) Stop() { c.producerPool.Release() c.producerPool = nil } 注意 连接池使用完或者程序退出时，务必释放连接池资源。 ","date":"2022-02-22","objectID":"/posts/conn-pool/:4:2","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["代码技巧"],"content":"5. 总结 本篇介绍了一个 go 语言实现的连接池的设计、实现以及如何使用。连接池作为程序开发中非常常用的功能模块，即便是不需要自己实现也应该对其底层的实现有个大概的认知。 了解连接池的概念 了解连接池的主要能力 了解工厂模式 设计一个连接池 根据设计实现连接池 加入了连接数的控制 接入连接超时校验 学会使用连接池 ","date":"2022-02-22","objectID":"/posts/conn-pool/:5:0","tags":["go","连接池"],"title":"Go 语言实现连接池","uri":"/posts/conn-pool/"},{"categories":["项目经验"],"content":" 我想将自己的开发项目的经历以及过程中总结的经验或者一些小技巧整理出来，供自己和看到这篇文章的同学一个参考。内容包括但不限于，项目目录结构，模块拆分，单元测试，e2e 测试，git 的使用技巧，GitHub 的 actionflow 的使用技巧等。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:0:0","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"1. 前言 做 go 开发有几年的时间了，从最开始的只会写一些简单的代码到现在除了日常编码外会考虑一个项目的前前后后(自认为考虑的比较全)，代码管理，项目结构以及相关的工具搭配使用， 走了很多的弯路，做了很多的重复工作，到目前为止积累了一些经验。想通过这篇文章输出一个总结，方便自己以及看到这篇文章的同学们之后在做新项目的时候有一定的启发。 温馨提示 这篇文章可能存在一定的漏洞，如发现任何您认为不对的地方，希望能给我一个留言。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:1:0","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"2. 项目管理 项目管理 这个模块我想分享一些，在开发过程中需要注意的或者值得学习的知识点，从而提升开发效率减少一些出错的概率，同时提高对项目的整体的理解。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:2:0","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"2.1. 目录结构 目录结构在一些语言会有很严格的要求，但是如果你看过的 go 项目比较多你会发现，大家分目录各有各的好处各有各的理由。所以这里并没有对错，我分享几个高分的开源项目以及自己在做一些 web 项目的时候经常用的目录结构，仁者见仁智者见智吧。 2.1.1. pkg 式目录 这类分目录在开源项目内十分常见，尤其是 k8s 以及相关组件的项目结构都是这类，下面看一下大概的目录结构。 以 KEDA 为例： 其中一些目录为了尽量展示常见的目录手动补上的，实际项目中不一定存在。 ➜ tree -L 2 . ├── BUILD.md ├── CHANGELOG.md ├── CONTRIBUTING.md ├── CREATE-NEW-SCALER.md ├── LICENSE ├── Makefile ├── PROJECT ├── README.md ├── RELEASE-PROCESS.MD ├── SECURITY.md ├── api/ ## 公用的 proto api 或 全局结构定义，如 k8s 项目里 api 目录里放入所有资源对象的定义 ├── build/ ## 构建相关的脚本(Dockerfile) ├── config/ ## 配置文件 ├── cmd/ ## 程序入口 │ ├── controller ## 不同目录实现不同的功能 │ └── admin ├── docs/ ## 生成的文档 ├── go.mod ├── go.sum ├── hack/ ## 代码生成相关工具和脚本 ├── pkg/ ## 该项目的功能代码 │ ├── eventreason │ ├── generated │ ├── metrics │ ├── mock │ ├── provider │ ├── scalers │ ├── scaling │ └── util ├── tests/ ## 测试数据或测试工具（并非测试代码在这里） ├── tools/ ## 其他工具，如检查代码风格 统计代码量等 └── third_party/ ## 第三方的protobuf 文件或无法通过 go mod 拉取的代码等 最大的特点是，所有的项目功能相关的代码都在 pkg 目录下根据功能拆分。pkg 之外的目录会放测试，文档，构建，发布，配置等内容。 其中 cmd 为程序的入口，根据功能会拆子目录，每个子目录下会有一个 main 文件，启动服务。 这个目录结构的项目一般都是体量比较大，测试构建发布过程都比较复杂，所以会有大量的脚本或者工具集来自动化这些复杂的过程。如果代码量比较少或者没有过多的依赖，则这种目录结构就体现不出优势。 2.1.2. 按功能分目录 这种类型的目录结构，在各种框架类型的项目中十分常见，如 go-micro, kratos 等。 以 go-micro 为例： ➜ tree -L 1 . ├── LICENSE ├── README.md ├── api/ ## 接口定义 ├── auth/ ## 认证相关 ├── broker/ ## MQ 的 broker ├── client/ ## http/rpc client ├── cmd/ ## 项目相关工具 xxx-generator 等 或启动参数的处理 ├── codec/ ## 编码类型定义 json yaml pb 等 ├── config/ ## 配置 ├── debug/ ## debug ├── errors/ ## 错误类型定义 ├── event.go ├── examples/ ## 各个模块的使用案例 ├── go.mod ├── go.sum ├── logger/ ## 日志模块的定义 ├── metadata/ ## 元数据 ├── micro.go ## 在这里提供对外的接口和方法 ├── options.go ## 各类可选参数定义 ├── plugins/ ## 插件 一般会在这里放起码模块定义的各种实现（如基于 etcd/nacos/consul 的服务发现等） ├── registry/ ## 服务发现相关定义 ├── runtime/ ## 运行时相关代码 ├── selector/ ## 服务选择相关 ├── server/ ## 服务端相关定义 ├── service.go ## 服务启动相关 ├── service_test.go ├── store/ ## 存储相关 ├── sync/ ## 数据同步相关定义 ├── transport/ ## 网络转发相关 http/grpc └── util/ ## 其他工具类 这类目录最大的特点就是分目录分模块比较多且相互不影响。由于是框架，所以这种做法十分重要，别人引用的时候根据自己的需要选择引用其中的一部分目录。并且大部分目录都是定义接口 Interface, 并且在自己框架内都只会用接口，从而做到使用者可以自己实现并轻松替换调任意模块。作为一个框架的目录结构以及每个目录都定义接口的方式，可以说是非常的高明。这个项目可以说是对我的益处非常多，对于一个 1-3 年的程序员非常有必要好好研究一下这个项目。 2.1.3. 裸跑型 以 gin 为例，虽然也是一个框架，但是功能集中在根目录，大部分的能力都是 gin.XXX 的方式调用，所以这类项目基本不分目录，仅有一些测试和构建相关的几个目录，一般这么用的少之又少。当然你的项目很小，仅提供单个功能的时候不分目录也罢。 2.1.4. web 项目 web 项目我之前接触的比较多，参加了无数个大大小小的 web 项目，经过无数次的折腾和吸取他人经验，有了比较统一的目录结构，希望能对在这方面有需求的同学有帮助。 ➜ tree . ├── Makefile ## 聚合各类常用命令 ├── app ## 程序入口 │ ├── admin ## admin 端的入口（包含服务初始化，启动） │ └── server ## server 端（包含服务初始化 服务注册发现 监控等）如果使用任何框架，也在这里进行初始 ├── broker ## MQ 的注册和消费 ├── cmd ## 常用工具二进制文件，代码生成 文档生成等 ├── build ## 构建相关脚本 ├── config ## 配置的定义和初始化 ├── dao ## DAO 层，根据模块拆分，实现数据的增删改查 │ ├── order │ └── user ├── docs ## 文档生成目录 ├── dto ## 传输层数据定义 ├── middleware ## 中间件 ├── router ## router 为路由定义，这块目录结构比较深 但是我认为是有必要的 每一层占据路由上的一个位置 │ ├── admin ## 第一层拆分开 admin 和 server 因为该项目最终构建两个程序 分别通过管理端和服务端内容 │ │ ├── register.go ## 注册路由 │ │ ├── v1 ## 一定要加版本号，在发生重大修改时提升版本号 不要直接改掉原有的路由（除非有安全隐患） │ │ │ ├── order ## 根据模块拆目录 │ │ │ │ ├── manage ## 如果模块包含内容比较多 可以拆子目录 │ │ │ │ │ └── manage.go ## 子目录下的路由 handler，可以包含多个 handler │ │ │ │ └── statistic │ │ │ │ └── statistic.go ## 这一层的handler 的路由应该 /admin/v1/order/statistic/xxx │ │ │ ├── user ## 模块拆分 │ │ │ │ └── user.go ## 如果包含内容不多可以直接放 handler 文件 /admin/v1/user/xxx │ │ │ └── v1.go ## 注册 v1 版本路由 同时注册这一层的 middleware │ │ └── v2 ## 与 v1 类似，服务重构或与原接口发生冲突 则升级版本 │ │ ├── order │ │ ├── share │ │ ├── user │ │ └── v2.go │ └── server ## 服务端的路由注册 整体与 admin 端一致 │ ├── register.go │ ├── v1 │ │ ├── user │ │ │ ├── action.go ## /server/v1/user/xxx │ │ │ └── manage │ │ │ └── manage.go ## /server/v1/user/manage/xxx │ │ └── v1.go │ └── v2 │ ├── order │ ├── share │ ├── user │ └── v2.go ├── service ## service 内包含业务逻辑 衔接上层请求和下层数据的增删改查 │ ├── order ## 按功能拆分目录 │ │ ├── order_service.go ## 不同 dao 层方法的组合 甚至可以通过上下文传 session 的方式 支持不同dao 层方式之间的事务 │ │ └── order_statistic_service.go │ └── user │ └── user_service.go ├── tests ## 测试相关工具和脚本 ├── logger ## 日志模块 └── thrid_party ## 第三方的 protobuf 一般放这儿 整体","date":"2022-01-24","objectID":"/posts/go-project-experience/:2:1","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"2.2. 模块拆分 这个就没什么可说的，单独提出来是为了提醒自己和各位，能拆分尽量拆分，不要一个文件上千行代码，一个方法几百行代码，别人看的时候真的很痛苦。 2.2.1. 拆分方法 这个比较好拆，唯一的原则就是一个方法只做一件事儿。这个一件事儿可大可小，这是基于当前这个方法所在的层级而定。 如果处于底层操作数据库的层级，那你的方法应该只进行一个数据库操作（不管增删改查），各种组合或者事务应该上层去做。 如果你在业务逻辑层，那你的方法应该只处理一个简单的业务。比如需要下单后发送给用户一个邮件，那你应该拆开下单和发送邮件的方法，不要揉在一起，让上层去组合处理，因为你很有可能添加短信通知/公众号处理。如果揉在一起，需要加新的通知方式的时候你还得去修改完全没有发生变化 OrderAndSendMessage 这个方法（随便起了个名字），很有可能会影响到 Order 的过程。 总而言之，一个方法只做一件事儿，不要让一个不相干的修改影响到你的逻辑。 2.2.2. 拆分模块 这块的话，我觉得重点是逻辑不要掺杂在一起，不同的逻辑尽量抽象出来。dao 层就做数据相关的（MySQL，Redis）不要掺杂逻辑，service 就做业务逻辑的组合，不要在 service 层直接操作底层组件。总结起来就是不要越界，上下不要越级（service -\u003e dao），左右不要越界（用户相关逻辑不要掺杂订单的逻辑，即便是用户信息内需要包含用户的订单总金额，也要在订单模块实现查询方法去调而不是自己去实现，否则以后拆分很痛苦）。我觉得不同的功能之间相互认为是个微服务，不要有底层的依赖，不要认为代码在一个项目内就随便调用，业务升级业务拆分的时候真的非常痛苦（血的教训）。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:2:2","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"2.3. 记录任务 这块的话，我觉得是一个工具推荐了算是。不管是个人开发或者团队开发，有个任务记录和里程碑是很重要的一点。不管是 GitHub 还是 Gitlab 都有 issue 和 milestone 的功能，对于一个开发者来说个人觉得非常的好用。 issue 你可以记录你要实现的功能、你现在出现的问题、QA 同学给你提的 bug，你每解决一个问题，每实现一个新功能在 git commit 上带上 issue 号就可以关联上。不管是之后定位问题还是查看某个功能的相关的提交都非常清晰，收益很多。 milestone 也是非常好用的功能，针对你的一个大功能或者一个新的版本创建一个 milestone 并加上截止日志，之后所有跟这个版本相关的 issue 都可以挂上这个里程碑，当你一个个完成的相关 issue 后，里程碑的完成度逐渐上升，记录了你的开发进度，同时提醒/监督自己。 不管你做的项目是大是小，都应该有个开发计划并在开发前做好准备，不要盲目上手，着急上手只会让你越做越累，而且做不好。都没有计划和开发过程的记录，怎么证明项目的质量，后期怎么维护，出现问题怎么定位。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:2:3","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"3. 测试 到这里就开始聊代码相关的了。测试是考验你代码的质量和功能的一把好刀，你应该习惯并熟悉写测试代码，并且尽可能包含你代码的所有部分，确保你的项目整体都是经得起推敲的。 我常用的代码测试有以下三种： 单元测试 - 测试你的方法在功能上没有问题 e2e 测试 - end to end 你的一个完整的功能没有问题，比如测试某个接口是否返回预期结果 压测 - 测试你的某个方法或者整个系统是否存在性能问题 压测可以在发布新的版本或分支合并的时候跑一下 通过基准线即可。单元测试和e2e 测试应该在每次提交的时候本地或者远端跑一次，确保任意一次的提交都是可用的。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:3:0","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"3.1. 单元测试 单元测试作为最基础的测试方法，应该在你的项目内尽可能覆盖大部分的方法，但是需要注意以下几点 任意一个单元测试应该都是独立，不能有依赖 任意一个单元测试执行前后不应该对数据有影响，如果这个单元测试需要修改数据库数据或文件，应该在测试结束后恢复发生变化的数据 不应该长时间阻塞，不应该有 select{} 或死循环，等待手动停止的情况 最简单的单元测试示例： func Test_ParseInt(t *testing.T) { var ( input = \"12\" output int64 =12 ) v, err := strconv.ParseInt(\"12\", 10, 64) if err != nil { t.Error(err) return } if v != output { t.Errorf(\"want:%v, got:%v\\n\", output, v) return } t.Log(\"ok\") } 这是一个校验 strconv.ParseInt 方法的单元测试，可以直接运行。从上面的代码来说，有些啰嗦，而且错误信息的输出也得自己写，这里推荐比较流行的测试框架 github.com/stretchr/testify/assert,该库对错误信息的格式化和遍历对比都毕竟成熟，而且提供很多方法，来简化你的测试代码量，下面看一下改版： func Test_ParseInt(t *testing.T) { v, err := strconv.ParseInt(\"12\", 10, 64) // 如果有错误 这里会返回 false if !assert.NoError(t, err) { return } // Equal 方法支持对比所有的类型，不需要根据类型判断 // 并且报错时，日志信息也非常丰富 assert.Equal(t, int64(12), v) } 这里我不会细讲这个库，如果看过比较主流的开源库，有很多库都使用这个测试框架，非常值得学习和使用。 一般来说，测试一个方法不止一个情况，大部分情况下会有多个各种输入，多方面来确保方法的可用性，这种情况下，可以采用下面的写法： func Test_zSkipList_rank(t *testing.T) { type args struct { score float64 value string } tests := []struct { name string args args want int }{ { name: \"c\", args: args{ score: 1, value: \"clang\", }, want: 1, }, { name: \"java\", args: args{ score: 2, value: \"java\", }, want: 2, }, { name: \"w\", args: args{ score: 5, value: \"world\", }, want: 3, }, { name: \"js\", args: args{ score: 8, value: \"javascript\", }, want: 4, }, { name: \"h\", args: args{ score: 10, value: \"hello\", }, want: 5, }, { name: \"go\", args: args{ score: 12, value: \"golang\", }, want: 6, }, } zs := prepareZSetForTest() zs.zsl.print() for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { got := zs.zsl.rank(tt.args.score, tt.args.value) assert.Equal(t, tt.want, int(got)) }) } } 这里我直接从我的现成的代码里复制出来一个测试案例，这种写法特点是定义好输入输出的结构，然后批量赋值，最终一个个执行。 如果测试过程中发现，输入的数据还不够，可以随时增加n个例子，其他部分不用动，重要的是，可以在 GoLand idea 中执行任意一个测试数据，更加方便调试。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:3:1","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"3.2. e2e 测试 e2e 测试，相对于单元测试来说更加系统性测试，针对某个模块的整体功能的校验。没有一个必要的代码框架，代码量也会更多一些。比如测试用户注册流程的测试，那启动时需要准备该功能需要的环境（启动或者链接测试数据库，创建表或者创建测试数据等），调用对用的方法后，对结果进行校验，校验成功后需要删除测试用的数据和环境。 对于web项目，可能针对接口也需要写测试，这个时候就需要启动当前的服务，准备环境，进行接口调用，完成后恢复涉及到的变更。 e2e 测试更多关注当前程序的整体的功能，可以确保任意的代码改动没有对当前程序的整体能力（或核心功能）没有带来负面影响，所以e2e测试也是非常重要的。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:3:2","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"3.3. 压测 压测(Benchmark) 不同于上述两个测试，关注的是函数或程序整体的性能情况。 3.3.1. 函数的压测 这块可以通过 go 的原生能力进行测试，写压测方法也很简单，如下： /* goos: darwin goarch: amd64 pkg: github.com/yusank/godis/util cpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz BenchmarkStringConcat BenchmarkStringConcat-12 14886952 70.91 ns/op PASS */ func BenchmarkStringConcat(b *testing.B) { var slice = []string{ \"$\", \"10\", \"\\r\\n\", \"12345123456\", \"\\r\\n\", } for i := 0; i \u003c b.N; i++ { StringConcat(16, slice...) } } 这是go 提供的压测函数的写法，执行后，输出上面代码中的注释部分，可以看到当前机器的信息和执行次数以及每次执行时需要的时间。通过这种方法，可以针对我们的一些高频率调用的方法进行压测，确保这些方法不是性能的瓶颈，并且遇到瓶颈或者优化时，通过压测的方式，对比优化前后的差异。 3.3.2. 整体压测 在GitHub上有很多项目，尤其是对性能要求高的项目都会有一个性能对比的图，对自己项目进行一个压测，对程序整体的吞吐进行全方面的压测，从而体现出该程序的高性能和稳定性。 这个其实对于tcp或者web服务来说非常好做，而且有很多现成的工具，可进行压测并输出结果。 以 wrk 为例，支持指定客户端数，并发数，请求次数等，甚至支持脚本执行，定制化压测，这里贴出链接,希望看到这里的同学，花几分钟去了解一下如何使用。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:3:3","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"4. 代码规范 代码规范这块我之前写过专门的文章(点击传送)。但是实际开发过程中不能保证所有人（包括自己）都能完成的遵循代码规范的，所以需要一个自动化工具来校验/限制不规范的代码。 golangci-lint 是一个绝大多数 go 开发者都熟悉的一个工具，可以校验任何 go 项目的代码规范，能够指出不规范的部分，并且支持指定开启/关闭部分类型的校验。下面从一个简单的例子说明如何使用。 这是我在某个 main 方法里加了一些不规范的语法： var abc string func main() { var t = 0 == 0 if t == true { // do } core.Service(\":8081\") } 执行 golangci-lint 看一下结果： ➜ golangci-lint run ./... -v INFO [config_reader] Config search paths: [./ /Users/shan.yu/workspace/yusank/klyn-examp /Users/shan.yu/workspace/yusank /Users/shan.yu/workspace /Users/shan.yu /Users /] INFO [lintersdb] Active 10 linters: [deadcode errcheck gosimple govet ineffassign staticcheck structcheck typecheck unused varcheck] INFO [loader] Go packages loading at mode 575 (deps|types_sizes|compiled_files|exports_file|files|imports|name) took 429.72546ms INFO [runner/filename_unadjuster] Pre-built 0 adjustments in 3.196848ms INFO [linters context/goanalysis] analyzers took 0s with no stages INFO [runner] Issues before processing: 7, after processing: 4 INFO [runner] Processors filtering stat (out/in): skip_dirs: 7/7, exclude: 7/7, uniq_by_line: 4/7, max_per_file_from_linter: 4/4, cgo: 7/7, filename_unadjuster: 7/7, source_code: 4/4, severity-rules: 4/4, skip_files: 7/7, max_from_linter: 4/4, nolint: 7/7, diff: 4/4, max_same_issues: 4/4, path_shortener: 4/4, path_prefixer: 4/4, autogenerated_exclude: 7/7, exclude-rules: 7/7, sort_results: 4/4, path_prettifier: 7/7, identifier_marker: 7/7 INFO [runner] processing took 1.708654ms with stages: path_prettifier: 661.569µs, nolint: 614.546µs, exclude-rules: 185.12µs, identifier_marker: 110.421µs, source_code: 48.813µs, autogenerated_exclude: 48.499µs, skip_dirs: 16.894µs, cgo: 6.783µs, max_same_issues: 3.182µs, uniq_by_line: 2.679µs, filename_unadjuster: 2.498µs, path_shortener: 2.444µs, max_per_file_from_linter: 2.015µs, max_from_linter: 1.59µs, sort_results: 348ns, exclude: 291ns, diff: 268ns, skip_files: 260ns, severity-rules: 244ns, path_prefixer: 190ns INFO [runner] linters took 85.727899ms with stages: goanalysis_metalinter: 83.919309ms main.go:81:5: `abc` is unused (deadcode) var abc string ^ main.go:78:14: Error return value of `core.Service` is not checked (errcheck) core.Service(\":8081\") ^ main.go:75:5: S1002: should omit comparison to bool constant, can be simplified to `t` (gosimple) if t == true { ^ main.go:74:10: SA4000: identical expressions on the left and right side of the '==' operator (staticcheck) var t = 0 == 0 ^ INFO File cache stats: 1 entries of total size 4.3KiB INFO Memory: 7 samples, avg is 72.4MB, max is 73.1MB INFO Execution took 594.401598ms 可以看到，一开始会打印相关的一些信息后，开始一个个打印遍历到的不规范的语法，并且指出问题所在的位置。虽然说现在的代码编辑器会对代码进行扫描也会指出不规范的语法，但是不会影响我们的日常开发和提交，而 golangci-lint 比编辑器更全面且可以通过 CICD 或者命令行的方式一次性遍历整个项目代码，所有的不规范可以一次性列出来。 如果去看一些 go 语言的开源项目，你会发现很多项目的 GitHub action 里会配置 lint 的工作流，如果 lint 过不去拒绝合并到正式分支的。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:4:0","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"5. git 上面提到了一些代码规范/单元测试和 GitHub action 等概念，从这里开始讲一下如何将这些概念串联起来，让更多的工作变成自动化。 git 作为版本管理工具，除了版本管理外其他相关方面的能力也是非常的强。本地的git 可以分支开发，打 tag，git hook 等。远端（GitHub/Gitlab）可以有 issue，milestone，PR，MR，CICD 等能力。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:5:0","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"5.1. git hook git hook是git 提供的钩子方法，可以配置很多事件的回调，让我们的一些手动工作配置到对应的 hook 上自动执行。我们打开任意的一个 git 项目进行下面的命令查看支持的 hooks： ➜ ls .git/hooks applypatch-msg.sample post-update.sample pre-merge-commit.sample pre-receive.sample update.sample commit-msg.sample pre-applypatch.sample pre-push.sample prepare-commit-msg.sample fsmonitor-watchman.sample pre-commit.sample pre-rebase.sample push-to-checkout.sample 可以看到很多 pre 或 post 开头的文件，表示对应的事件前或后执行对应的脚本。如果需要启动任意一个钩子，只需要编辑对应的文件然后把后缀 .sample 去掉即可。 下面是我在自己某个项目内启用了 pre-commit hook 的例子： #!/bin/sh # # An example hook script to verify what is about to be committed. # Called by \"git commit\" with no arguments. The hook should # exit with non-zero status after issuing an appropriate message if # it wants to stop the commit. # # To enable this hook, rename this file to \"pre-commit\". # 执行 make test 命令 if ! make test; then echo \"Go test failed, please check your code.\" exit 1 fi # 执行 make lint 命令 if ! make lint; then echo \"Go lint failed, please check your code style.\" exit 1 fi 启用后我的任何一次 commit 都会先进行一次全局的测试和 lint 校验通过后，才会 commit 成功否则本次 commit 失败。这种做法对于个人或少数几个人开发者来说，实用性蛮高的，一种自我约束并且保证不会提交一些垃圾代码。最重要的是不需要自己手动去检查代码质量和规范，不通过测试和规范的提交都不会产生一次 commitID。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:5:1","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"5.2. github / gitlab 除了上述本地的一些 git hook 的限制外，GitHub 和 Gitlab 提供一套完整的 CICD 的能力，用户可以配置针对提交，MR，PR 等各种事件运行特定的脚本，在远端服务器进行校验，构建，部署等能力。 CICD 对于现在的开发者来说应该很熟悉，对于热衷于工作自动化的程序员来说，CICD 可以说是非常好用的一个手段，我个人的大部分项目都会配置远端的代码测试代码规范校验的脚本。包括本博客也在 GitHub 上自动部署的,我只管提交剩余的工作都是自动化，感兴趣可以点击查看。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:5:2","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"6. 构建与运行 项目的开发和调试过程我们免不了无数次的构建、编译、运行等过程，并且会依赖很多变量。这个时候我们应该有一个比较完善的解决方案来减少我们的敲各种命令的次数提升效率。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:6:0","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"6.1. makefile make命令是GNU的工程化编译工具，用于编译众多相互关联的源代码文件，以实现工程化的管理，提高开发效率。我们应该对 make 有一定的了解，并学会使用它，从而提升效率。 点击这里学习/复习相关知识，本篇文章不再讲述相关内容。 下面以当前博客项目的 Makefile 为例： TAG = latest help: ## Display this help @awk 'BEGIN {FS = \":.*##\"; printf \"\\nUsage:\\n make \\033[36m\u003ctarget\u003e\\033[0m\\n\"} /^[0-9A-Za-z_-]+:.*?##/ { printf \" \\033[36m%-45s\\033[0m %s\\n\", $$1, $$2 } /^##@/ { printf \"\\n\\033[1m%s\\033[0m\\n\", substr($$0, 5) } ' $(MAKEFILE_LIST) .PHONY: docker-build docker-build: ## build docker image ## change config cat config.toml \u003e config.backup.toml rm -f config.toml mv config.docker.toml config.toml docker build -t yusank/hugo_blog:$(TAG) . # recover mv config.toml config.docker.toml cat config.backup.toml \u003e config.toml rm -f config.backup.toml .PHONY: docker-run docker-run: ## run latest docker image localy docker rm -f blog docker run -d -p 8088:80 --name blog yusank/hugo_blog:$(TAG) .PHONY: docker-push docker-push: docker-build ## bulid and push newest docker image docker push docker.io/yusank/hugo_blog:$(TAG) .PHONY: docker-release docker-release: docker-push ## relaese newest version of image to aliyun ssh aliyun_d1 \"./restart.sh latest\" 可以看到定义了几个命令，并且可以发现有些命令是有依赖关系的。当执行 make docker-release 时，会先进行 docker-push的逻辑，而 docker-push 又有其依赖，以此类推。如果没有定义这个 makefile, 那为了一个 release 操作，我需要你手动敲很多命令且还得确保顺序。 这个只是一个十几行的 make 命令的示例，如果你的构建/编译的依赖更多（环境变量，上下文等），那更得写一个 Makefile 来减少人工操作次数。况且一次测好后，之后就避免人工操作失误的情况。 help 命令会打印当前 Makefile 支持的所有子命令，效果如下： ➜ make help Usage: make \u003ctarget\u003e help Display this help docker-build build docker image docker-run run latest docker image localy docker-push bulid and push newest docker image docker-release relaese newest version of image to aliyun 写好一个 Makefile 可以说是一个一劳永逸，对自己和他人都有百益无一害的事儿了。你可以有个模板，有新的项目可以直接 copy 进去修改即可用的那种。 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:6:1","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"6.2. dockerfile dockerfile 的用处很简单，就是为了构建 docker 镜像。现在大部分人的项目都在容器内运行了，二进制裸跑在物理机或虚拟机的情况很少，所以 dockerfile 也是需要我们在项目中加好，方便自动化构建镜像。 还是以本博客系统的镜像为例（引用本博客系统的文件不是因为夸自己写的好，纯属因为方便）： ARG HUGO_DIR=. ARG HUGO_ENV_ARG=production FROM klakegg/hugo:0.90.1-alpine-onbuild AS hugo FROM nginx COPY nginx.conf /etc/nginx/conf.d/nginx.conf COPY yusank.space.pem /etc/nginx/conf.d/yusank.space.pem COPY yusank.space.key /etc/nginx/conf.d/yusank.space.key COPY --from=hugo /target /usr/share/nginx/html EXPOSE 80 EXPOSE 443 典型的 go 项目的dockerfile： FROM golang:1.17-alpine as builder WORKDIR /app/godis COPY . . # args ARG build_tags RUN CGO_ENABLED=0 go build -tags \"${build_tags}\" -o godis cmd/server/main.go FROM scratch WORKDIR /app/godis COPY --from=builder /app/godis . EXPOSE 7379 CMD [\"./godis\"] ","date":"2022-01-24","objectID":"/posts/go-project-experience/:6:2","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"7. 总结 本篇文章主要讲述了我个人在开个项目过程积累的一些经验，不一定是最好的而且肯定露了很多地方，若有不正确的地方请指出。想通过这篇文章首先给自己一个总结，其次想给对项目的管理（开发方向）方面有疑惑有疑虑的同学一些启发。 本篇文章的主要内容： 介绍常见的项目目录结构，请按自己的实际情况选择 如何拆分模块 应该重视代码测试项目测试相关 介绍 git,github,gitlab 的一些好用的特性，希望能带来效率的提升 介绍 makefile,dockerfile 帮助提升开发效率 ","date":"2022-01-24","objectID":"/posts/go-project-experience/:7:0","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["项目经验"],"content":"8. 链接🔗 keda golangci-lint git hook Makefile ","date":"2022-01-24","objectID":"/posts/go-project-experience/:8:0","tags":["go","开发","维护"],"title":"Go 项目开发和维护经验之谈","uri":"/posts/go-project-experience/"},{"categories":["kubernetes"],"content":" 本篇讲述 kubernetes 的横向 pod 伸缩(HorizontalPodAutoscaler) 控制器的数据结构，逻辑处理，metrics 计算以及相关细节的源码解读。 ","date":"2022-01-20","objectID":"/posts/hpa-controller/:0:0","tags":["go","k8s","源码解读"],"title":"HPA controller 源码解读","uri":"/posts/hpa-controller/"},{"categories":["kubernetes"],"content":"1. 前言 在 k8s 环境内弹性伸缩并不是一个陌生的概念，是一个常见且不难理解的事件。就是根据特定的事件或数据来触发可伸缩资源的伸缩能力。一般有 HPA 和 VPA 两个概念，HPA 全称 Horizontal Pod Autoscaler 即横向 pod 的自动伸缩，VPA 全称 Vertical Pod Autoscaler 即纵向 pod 的自动伸缩。 ","date":"2022-01-20","objectID":"/posts/hpa-controller/:1:0","tags":["go","k8s","源码解读"],"title":"HPA controller 源码解读","uri":"/posts/hpa-controller/"},{"categories":["kubernetes"],"content":"1.1. HPA HPA 关注 pod 的数量，根据现有 pod 的数据(cpu, memory)或外部数据(metrics)来计算实际需要的 pod 数量，从而调整 pod 的总数。HPA 操作的对象需要实现 ScaleInterface 。 ScaleInterface // ScaleInterface can fetch and update scales for // resources in a particular namespace which implement // the scale subresource. type ScaleInterface interface { // Get fetches the scale of the given scalable resource. Get(ctx context.Context, resource schema.GroupResource, name string, opts metav1.GetOptions) (*autoscalingapi.Scale, error) // Update updates the scale of the given scalable resource. Update(ctx context.Context, resource schema.GroupResource, scale *autoscalingapi.Scale, opts metav1.UpdateOptions) (*autoscalingapi.Scale, error) // Patch patches the scale of the given scalable resource. Patch(ctx context.Context, gvr schema.GroupVersionResource, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions) (*autoscalingapi.Scale, error) } k8s 原生资源中 HPA 可操作性的对象是以下三个: Deployment ReplicaSet StatefulSet 一般业务场景用 HPA 能满足需求，即业务高峰增加 pod 数更好处理业务，业务低峰降低pod 数节省资源。 ","date":"2022-01-20","objectID":"/posts/hpa-controller/:1:1","tags":["go","k8s","源码解读"],"title":"HPA controller 源码解读","uri":"/posts/hpa-controller/"},{"categories":["kubernetes"],"content":"1.2. VPA VPA 关注的是 pod 的资源，根据当前资源利用率等数据为 pod 提供更多的资源（cpu，memory 等）。本人对 VPA 也是表面理解，所以这里不做详细的解读。 VPA 更适合大计算、离线计算、机器学习等场景，需要大量的 CPU，GPU，内存来进行计算。 ","date":"2022-01-20","objectID":"/posts/hpa-controller/:1:2","tags":["go","k8s","源码解读"],"title":"HPA controller 源码解读","uri":"/posts/hpa-controller/"},{"categories":["kubernetes"],"content":"2. 基础用法 通过以下命令开启对某个资源的 HPA 能力： ➜ kubectl autoscale (-f FILENAME | TYPE NAME | TYPE/NAME) [--min=MINPODS] --max=MAXPODS [--cpu-percent=CPU] [options] Warning 需要开启 metric server 才能读取到cpu 利用率，默认是不开启的。详情请看官方文档: https://github.com/kubernetes-sigs/metrics-server 实际使用如下： # 对 deployment/klyn-deploy 进行 CPU 监控，超过 10%的平均利用率即进行scale up，最大 3 个 pod 最小 1 个 ➜ kubectl autoscale deployment klyn-deploy --cpu-percent=10 --min=1 --max=3 然后可以通过 describe 命令查看创建后 HPA 的详情： ➜ kubectl describe hpa klyn-deploy Warning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+ Name: klyn-deploy Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e CreationTimestamp: Thu, 20 Jan 2022 11:43:43 +0800 Reference: Deployment/klyn-deploy Metrics: ( current / target ) resource cpu on pods (as a percentage of request): 4% (10m) / 10% # 可以看到已经读到 cpu 数据 Min replicas: 1 Max replicas: 3 Deployment pods: 1 current / 1 desired Conditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True ReadyForNewScale recommended size matches current size ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request) ScalingLimited False DesiredWithinRange the desired count is within the acceptable range Events: \u003cnone\u003e 可以从上述详情看到 HPA 资源的详细信息以及最下面的步骤信息，当进行伸缩时会同步伸缩过程和原因到 Events 字段上。 之后可以通过压测的方式将 cpu 的利用率提升然后可以到 Deployment 的 replicas 数量的提升，并压测结束一段时间(会有冷却时间)后又降到 1 个 replicas. ","date":"2022-01-20","objectID":"/posts/hpa-controller/:2:0","tags":["go","k8s","源码解读"],"title":"HPA controller 源码解读","uri":"/posts/hpa-controller/"},{"categories":["kubernetes"],"content":"3. 数据结构 HPA 资源 YAML 结构： apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: creationTimestamp: \"2022-01-20T03:43:43Z\" name: klyn-deploy namespace: default resourceVersion: \"6602029\" uid: 385f4234-025b-453d-8270-788f7ce3ced6 spec: maxReplicas: 3 metrics: - resource: name: cpu target: averageUtilization: 10 type: Utilization type: Resource minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: klyn-deploy status: conditions: - lastTransitionTime: \"2022-01-20T03:43:58Z\" message: recommended size matches current size reason: ReadyForNewScale status: \"True\" type: AbleToScale - lastTransitionTime: \"2022-01-20T03:43:58Z\" message: the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request) reason: ValidMetricFound status: \"True\" type: ScalingActive - lastTransitionTime: \"2022-01-20T03:43:58Z\" message: the desired count is within the acceptable range reason: DesiredWithinRange status: \"False\" type: ScalingLimited currentMetrics: - resource: current: averageUtilization: 3 averageValue: 9m name: cpu type: Resource currentReplicas: 1 desiredReplicas: 1 lastScaleTime: \"2022-01-20T03:51:44Z\" 上面对 HPA 的概念和如何使用有了一定的认知，从这里开始对 HPA Controller 的源码进行解读。 数据结构： // HorizontalController is responsible for the synchronizing HPA objects stored // in the system with the actual deployments/replication controllers they // control. type HorizontalController struct { // Scale 资源的读写（如 Deployment 的 Scale） scaleNamespacer scaleclient.ScalesGetter // HorizontalPodAutoscaler 资源的读写 hpaNamespacer autoscalingclient.HorizontalPodAutoscalersGetter // 用于根据类型和版本获取资源信息 mapper apimeta.RESTMapper // 计算 replica 数量 replicaCalc *ReplicaCalculator // 订阅 HPA 资源，处理资源变化 eventRecorder record.EventRecorder // 每次缩容之间等待时间 downscaleStabilisationWindow time.Duration // hpaLister is able to list/get HPAs from the shared cache from the informer passed in to // NewHorizontalController. // 用于读取 hpa 对象 hpaLister autoscalinglisters.HorizontalPodAutoscalerLister hpaListerSynced cache.InformerSynced // podLister is able to list/get Pods from the shared cache from the informer passed in to // NewHorizontalController. // 用于读取 pod 资源 podLister corelisters.PodLister podListerSynced cache.InformerSynced // Controllers that need to be synced // 只会启动一个 worker，即如果有大量的 HPA 资源时，一部分资源的扩缩容可能不那么及时 queue workqueue.RateLimitingInterface // Latest unstabilized recommendations for each autoscaler. // 记录推荐伸缩数量 recommendations map[string][]timestampedRecommendation // Latest autoscaler events // 记录伸缩事件 scaleUpEvents map[string][]timestampedScaleEvent scaleDownEvents map[string][]timestampedScaleEvent } ","date":"2022-01-20","objectID":"/posts/hpa-controller/:3:0","tags":["go","k8s","源码解读"],"title":"HPA controller 源码解读","uri":"/posts/hpa-controller/"},{"categories":["kubernetes"],"content":"4. 控制器逻辑 ","date":"2022-01-20","objectID":"/posts/hpa-controller/:4:0","tags":["go","k8s","源码解读"],"title":"HPA controller 源码解读","uri":"/posts/hpa-controller/"},{"categories":["kubernetes"],"content":"4.1. 伸缩过程 伸缩过程的触发不是实时的，而是从 queue 里消费数据，进行一次伸缩流程，再次将资源名放入 queue, 完成一次循环。 详细流程如下： 4.1.1. 从 queue 读取一条消息，根据消息中的信息，获取 hpa 对象 func (a *HorizontalController) processNextWorkItem() bool { key, quit := a.queue.Get() if quit { return false } // 处理完标记完成 defer a.queue.Done(key) // 处理函数入口 deleted, err := a.reconcileKey(key.(string)) if err != nil { utilruntime.HandleError(err) } // Add request processing HPA to queue with resyncPeriod delay. // Requests are always added to queue with resyncPeriod delay. If there's already request // for the HPA in the queue then a new request is always dropped. Requests spend resyncPeriod // in queue so HPAs are processed every resyncPeriod. // Request is added here just in case last resync didn't insert request into the queue. This // happens quite often because there is race condition between adding request after resyncPeriod // and removing them from queue. Request can be added by resync before previous request is // removed from queue. If we didn't add request here then in this case one request would be dropped // and HPA would processed after 2 x resyncPeriod. if !deleted { // 如果 hpa 没有被删除，再次放回队列里，经过默认时间间隔（15s，k8s 启动参数可配置）后再读取处理一次 a.queue.AddRateLimited(key) } return true } func (a *HorizontalController) reconcileKey(key string) (deleted bool, err error) { namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil { return true, err } // 读取 hpa 对象 `*autoscalingv1.HorizontalPodAutoscaler` hpa, err := a.hpaLister.HorizontalPodAutoscalers(namespace).Get(name) if errors.IsNotFound(err) { klog.Infof(\"Horizontal Pod Autoscaler %s has been deleted in %s\", name, namespace) delete(a.recommendations, key) delete(a.scaleUpEvents, key) delete(a.scaleDownEvents, key) return true, nil } if err != nil { return false, err } // 处理本次伸缩逻辑 return false, a.reconcileAutoscaler(hpa, key) } 4.1.2. 为了兼容老版本，读取时 hpa 对象为 v1 版本，读取后在代码里会先统一转换为 v2 版本,从而之后的逻辑统一 func (a *HorizontalController) reconcileAutoscaler(hpav1Shared *autoscalingv1.HorizontalPodAutoscaler, key string) error { // make a copy so that we never mutate the shared informer cache (conversion can mutate the object) hpav1 := hpav1Shared.DeepCopy() // then, convert to autoscaling/v2, which makes our lives easier when calculating metrics hpaRaw, err := unsafeConvertToVersionVia(hpav1, autoscalingv2.SchemeGroupVersion) if err != nil { a.eventRecorder.Event(hpav1, v1.EventTypeWarning, \"FailedConvertHPA\", err.Error()) return fmt.Errorf(\"failed to convert the given HPA to %s: %v\", autoscalingv2.SchemeGroupVersion.String(), err) } hpa := hpaRaw.(*autoscalingv2.HorizontalPodAutoscaler) ... } 4.1.3. 根据 hpa.Spec.ScaleTargetRef 信息读到需要伸缩的资源 func (a *HorizontalController) reconcileAutoscaler(hpav1Shared *autoscalingv1.HorizontalPodAutoscaler, key string) error { ... // 省略部分代码 hpa := hpaRaw.(*autoscalingv2.HorizontalPodAutoscaler) reference := fmt.Sprintf(\"%s/%s/%s\", hpa.Spec.ScaleTargetRef.Kind, hpa.Namespace, hpa.Spec.ScaleTargetRef.Name) // 解析资源 api version targetGV, err := schema.ParseGroupVersion(hpa.Spec.ScaleTargetRef.APIVersion) if err != nil { a.eventRecorder.Event(hpa, v1.EventTypeWarning, \"FailedGetScale\", err.Error()) setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionFalse, \"FailedGetScale\", \"the HPA controller was unable to get the target's current scale: %v\", err) a.updateStatusIfNeeded(hpaStatusOriginal, hpa) return fmt.Errorf(\"invalid API version in scale target reference: %v\", err) } // 资源类型 targetGK := schema.GroupKind{ Group: targetGV.Group, Kind: hpa.Spec.ScaleTargetRef.Kind, } // 查询资源对象 mappings, err := a.mapper.RESTMappings(targetGK) if err != nil { a.eventRecorder.Event(hpa, v1.EventTypeWarning, \"FailedGetScale\", err.Error()) setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionFalse, \"FailedGetScale\", \"the HPA controller was unable to get the target's current scale: %v\", err) a.updateStatusIfNeeded(hpaStatusOriginal, hpa) return fmt.Errorf(\"unable to determine resource for scale target reference: %v\", err) } // 根据 ns 资源类型 资源版本和资源名称 确定最","date":"2022-01-20","objectID":"/posts/hpa-controller/:4:1","tags":["go","k8s","源码解读"],"title":"HPA controller 源码解读","uri":"/posts/hpa-controller/"},{"categories":["kubernetes"],"content":"4.2. 计算副本数过程 可通过上面流程看到会有一个计算目标副本数的过程 a.computeReplicasForMetrics，看似简单其实内部相当丰富的一个过程，下面一起看一下如何去计算的。 4.2.1. 遍历 spec.Metrics 计算得出最大值 // computeReplicasForMetrics computes the desired number of replicas for the metric specifications listed in the HPA, // returning the maximum of the computed replica counts, a description of the associated metric, and the statuses of // all metrics computed. func (a *HorizontalController) computeReplicasForMetrics(hpa *autoscalingv2.HorizontalPodAutoscaler, scale *autoscalingv1.Scale, metricSpecs []autoscalingv2.MetricSpec) (replicas int32, metric string, statuses []autoscalingv2.MetricStatus, timestamp time.Time, err error) { /* * 省略了部分无关代码 */ if scale.Status.Selector == \"\" { errMsg := \"selector is required\" return 0, \"\", nil, time.Time{}, fmt.Errorf(errMsg) } selector, err := labels.Parse(scale.Status.Selector) if err != nil { return 0, \"\", nil, time.Time{}, fmt.Errorf(errMsg) } specReplicas := scale.Spec.Replicas statusReplicas := scale.Status.Replicas // 记录每个 metric 的结果 statuses = make([]autoscalingv2.MetricStatus, len(metricSpecs)) invalidMetricsCount := 0 var invalidMetricError error var invalidMetricCondition autoscalingv2.HorizontalPodAutoscalerCondition // 遍历计算 for i, metricSpec := range metricSpecs { // 计算单个 metric 的方法 replicaCountProposal, metricNameProposal, timestampProposal, condition, err := a.computeReplicasForMetric(hpa, metricSpec, specReplicas, statusReplicas, selector, \u0026statuses[i]) if err != nil { if invalidMetricsCount \u003c= 0 { invalidMetricCondition = condition invalidMetricError = err } invalidMetricsCount++ } // 取最大 if err == nil \u0026\u0026 (replicas == 0 || replicaCountProposal \u003e replicas) { timestamp = timestampProposal replicas = replicaCountProposal metric = metricNameProposal } } // If all metrics are invalid or some are invalid and we would scale down, // return an error and set the condition of the hpa based on the first invalid metric. // Otherwise set the condition as scaling active as we're going to scale if invalidMetricsCount \u003e= len(metricSpecs) || (invalidMetricsCount \u003e 0 \u0026\u0026 replicas \u003c specReplicas) { return 0, \"\", statuses, time.Time{}, fmt.Errorf(\"invalid metrics (%v invalid out of %v), first error is: %v\", invalidMetricsCount, len(metricSpecs), invalidMetricError) } return replicas, metric, statuses, timestamp, nil } 4.2.2. 计算单个 metric：根据类型计算 metric 名词解析 hpa 对象的 spac.metrics 中的元素有个 Type 字段,记录 metric 数据源的类型，目前支持的以下几种： 类型 说明 支持的计算类型 不支持的计算类型 Object 由 k8s 本身资源提供的数据，如 ingress 提供命中规则数量 AverageValue Value AverageUtilization Pods 由 pod 提供的除 CPU 内存之外的数据 AverageValue AverageUtilization Value Resource pod 提供的系统资源（CPU 内存） AverageUtilization AverageValue Value External 外部提供的监控指标 AverageValue Value AverageUtilization AverageValue：设定平均值，如qps，tps。计算时读取 metric 总和，与预设平均值✖️副本数进行对比，从而判断是否需要伸缩。 Value：设定固定值，如队列中消息数量，Redis 中 list 长度等。计算时直接拿 metric 和预设值进行对比。 AverageUtilization： 平均利用率，如 CPU 和内存。先计算 pod的基数的总和（totalMetric 和 totalRequest），最终计算利用率 → totalMetric/totalRequest 源码： // Computes the desired number of replicas for a specific hpa and metric specification, // returning the metric status and a proposed condition to be set on the HPA object. func (a *HorizontalController) computeReplicasForMetric(hpa *autoscalingv2.HorizontalPodAutoscaler, spec autoscalingv2.MetricSpec, specReplicas, statusReplicas int32, selector labels.Selector, status *autoscalingv2.MetricStatus) (replicaCountProposal int32, metricNameProposal string, timestampProposal time.Time, condition autoscalingv2.HorizontalPodAutoscalerCondition, err error) { switch spec.Type { case autoscalingv2.ObjectMetricSourceType: metricSelector, err := metav1.LabelSelectorAsSelector(spec.Object.Metric.Selector) if err != nil { condition := a.getUnableComputeReplicaCountCondition(hpa, \"FailedGetObjectMetric\", err) return 0, \"\", time.Time{}, condition, fmt.Errorf(\"failed to get object metric value: %v\", err) } replicaCountProposal, timestampProposal, metricNameProposal, condition, err = a.computeStatusForObjectMetric(specReplicas, statusRepli","date":"2022-01-20","objectID":"/posts/hpa-controller/:4:2","tags":["go","k8s","源码解读"],"title":"HPA controller 源码解读","uri":"/posts/hpa-controller/"},{"categories":["kubernetes"],"content":"5. 扩展用法 在实际开发环境只用 cpu, memory 来作为弹性伸缩依据是远远不够的。大多数情况可能会根据 qps, tps, 平均延迟时间, MQ 中的消息数量, Redis 中的数据量 等与业务息息相关的数据量判断伸缩的依据，这些都属于 HPA 的 External 类型的范畴。但是 External 类型的数据源需要对接 Adapter 的方式才能用到 HPA 对象内容(具体请查看: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis) ，也就是说需要额外开发量的。 这里我推荐一个知名度比较高且支持的数据量比较广泛的 Adapter 的实现 – KEDA。基于事件驱动的autoscaler,且支持从 0 到 1 1 到 0 的伸缩，也就是说业务低峰或者无流量时可以降到 0 个副本数(这个在 HPA 被认为是禁用该功能 所以 KEDA 自己实现的 0 到 1 1 到 0 的伸缩的能力，剩余的情况它叫个 HPA 处理)。 目前 KEDA 支持事件类型如下： KEDA 支持的事件 只需要创建 KEDA 的 CRD 资源，就能实现基于事件的弹性伸缩，KEDA 的 controller 会创建对应的 HPA 对象。 下面以 Prometheus 为例： KEDA.ScaledObject: apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: prom-scaledobject namespace: default spec: scaleTargetRef: name: klyn-deploy triggers: - type: prometheus metadata: serverAddress: http://10.103.255.235:9090 # Prometheus 查询接口 metricName: http_request_count # 自定义名字 query: sum(rate(http_request_count{job=\"klyn-service\"}[2m])) # 查询语句 2m 内的平均 qps threshold: '50' maxReplicaCount: 5 minReplicaCount: 1 pollingInterval: 5 apply 该 yaml 后，会创建一个 ScaledObject 和 HPA 对象。 ➜ kubectl get ScaledObject NAME SCALETARGETKIND SCALETARGETNAME MIN MAX TRIGGERS AUTHENTICATION READY ACTIVE FALLBACK AGE prom-scaledobject apps/v1.Deployment klyn-deploy 1 5 prometheus True False False 9d ## 该 HPA 对象有 KEDA 的 controller 创建的 ➜ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE keda-hpa-prom-scaledobject Deployment/klyn-deploy 0/50 (avg) 1 5 1 9d 这样一来，可以根据很多事件来控制伸缩的能力，这比单一来 cpu,内存利用率来看更灵活且及时。完全可以根据事件在服务的副本数不够用或者有一堆事件准备处理时，尽可能快速扩容，确保处理能力不会受损。 ","date":"2022-01-20","objectID":"/posts/hpa-controller/:5:0","tags":["go","k8s","源码解读"],"title":"HPA controller 源码解读","uri":"/posts/hpa-controller/"},{"categories":["kubernetes"],"content":"6. 总结 这篇文章主要讲 HPA controller 的源码和如何使用 HPA 相关内容 HPA/VPA 的解释 如何使用 HPA HPA Controller 如何处理一次伸缩事件的 HPA Controller 如何计算目标实例数的 认识和使用 KEDA – 一个基于事件驱动的 autoscaler ","date":"2022-01-20","objectID":"/posts/hpa-controller/:6:0","tags":["go","k8s","源码解读"],"title":"HPA controller 源码解读","uri":"/posts/hpa-controller/"},{"categories":["kubernetes"],"content":"7. 链接🔗 horizontal-pod-autoscale pvertical-pod-autoscaler metrics server KEDA ","date":"2022-01-20","objectID":"/posts/hpa-controller/:7:0","tags":["go","k8s","源码解读"],"title":"HPA controller 源码解读","uri":"/posts/hpa-controller/"},{"categories":["代码技巧"],"content":" 本篇分享一个分片式的 map 结构，在一些场景下该结构比原生 syncMap 更有优势，本文会对该结构的实现，原理以及时候的场景进行详细的介绍。 ","date":"2022-01-13","objectID":"/posts/shard-map/:0:0","tags":["go","map","数据结构"],"title":"go 语言中的分片 map 的实现","uri":"/posts/shard-map/"},{"categories":["代码技巧"],"content":"1. 背景 map 作为一个基础的数据结构，在编程过程中可以说是无处不在，应用场景十分广泛。在大部分场景下用原生的 map 就能解决当前的问题。如果是高并发场景可以加入 sync.RWMutex 来控制并发读写或者直接使用 sync.Map 来减少手动写锁的处理逻辑。如果作为一个应用的基础数据结构，性能可以说是非常的高了，绝大部分场景下是完全足够的。但是总有人在优化性能这块想做到极致（包括我自己），即便是原生的数据结构也会有人想优化。既然想优化 map，那首先得了解 map 在什么情况下性能会受损或者性能不够高呢？ 对 map 结构熟悉的同学应该都知道，map 又称之为哈希表，key 是按照哈希值存储的，map 在元素增多/减少的过程在 go 里叫 grow,而这个过程中有个非常关键的步骤 rehash。即会对 map 内的数据进行重新哈希计算和移动位置，在数据量比较大的时候，map 的写性能会有一定的受损的。 如果我有个对性能要求很高的程序（其实如果对性能要求极高其实可以考虑 C 或者 rust 的，这里就不考虑了），想在 map 上做进一步的优化的，应该如何优化，从什么地方入手呢？ 在我之前几篇文章都在讲用 go 语言实现 Redis server 的过程，在实际开发过程中我最开始也是直接用的 syncMap 作为底层的哈希表的，但是最后压测的时候，结果不是很满意。尤其是写入操作数据量大的时候，tps 下降比较厉害，所以就开始搜查相关的优化方案。首先遇到了一个开源库orcaman/concurrent-map,其 README 中的一句话吸引到我了 orcaman/concurrent-map.README Prior to Go 1.9, there was no concurrent map implementation in the stdlib. In Go 1.9, sync.Map was introduced. The new sync.Map has a few key differences from this map. The stdlib sync.Map is designed for append-only scenarios. So if you want to use the map for something more like in-memory db, you might benefit from using our version 这不就是在说我吗？所以马上clone 下来进行 benchmark 测试，与 syncMap 进行对比。仅对基础读写能力进行一个压测对比，结果如下： /* * goos: darwin * goarch: amd64 * pkg: github.com/yusank/godis/datastruct * cpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz * Benchmark_concurrence_map_sAdd * Benchmark_concurrence_map_sAdd-12 2619080 440.8 ns/op * Benchmark_concurrence_map_sIsMember * Benchmark_concurrence_map_sIsMember-12 13764466 77.68 ns/op * Benchmark_concurrence_map_sRem * Benchmark_concurrence_map_sRem-12 16740207 65.18 ns/op * Benchmark_sync_map_sAdd * Benchmark_sync_map_sAdd-12 2101056 765.1 ns/op * Benchmark_sync_map_sIsMember * Benchmark_sync_map_sIsMember-12 15998791 73.47 ns/op * Benchmark_sync_map_sRem * Benchmark_sync_map_sRem-12 15768998 76.62 ns/op * PASS */ 查询元素和删除元素前，提前 insert 50000 条数据进行的压测 对比结果确实让我有些惊讶，我以为这个开源库应该也就大概能达到 sync.Map 80-90% 的性能，没想到写入性能比 sync.Map 高出 60%，我决定用这个开源库替代sync.Map。 但是我在看该库的源码的时候发现让我不是很爽的一个地方，就是读取全量数据的时候会进行一次 buffer，再从 buffer 往外吐出元素，导致全量数据的读取或者遍历变得非常的慢，当然这个作者可能有自己的顾虑，但是对我来说说不可接受。所以我决定对这个库进行一个自定义，下面就把库的实现逻辑和自己自定义的部分一起分享一下。 ","date":"2022-01-13","objectID":"/posts/shard-map/:1:0","tags":["go","map","数据结构"],"title":"go 语言中的分片 map 的实现","uri":"/posts/shard-map/"},{"categories":["代码技巧"],"content":"2. 实现 ","date":"2022-01-13","objectID":"/posts/shard-map/:2:0","tags":["go","map","数据结构"],"title":"go 语言中的分片 map 的实现","uri":"/posts/shard-map/"},{"categories":["代码技巧"],"content":"2.1. 数据结构 我们先从数据结构的定义入手看看这个库为什么能做到这么高的性能。 var SHARD_COUNT = 32 // A \"thread\" safe map of type string:Anything. // To avoid lock bottlenecks this map is dived to several (SHARD_COUNT) map shards. type ConcurrentMap []*ConcurrentMapShared // A \"thread\" safe string to anything map. type ConcurrentMapShared struct { items map[string]interface{} sync.RWMutex // Read Write mutex, guards access to internal map. } // Creates a new concurrent map. func New() ConcurrentMap { m := make(ConcurrentMap, SHARD_COUNT) for i := 0; i \u003c SHARD_COUNT; i++ { m[i] = \u0026ConcurrentMapShared{items: make(map[string]interface{})} } return m } ConcurrentMap 是一个 32 个分片的 map 结构，每个分片内是一个 map-lock 的组合。这个 SHARD_COUNT 可能是为了方便后期可以通过编译过程注入的方式扩展分片大小而定义的变量。我目前没有这个需求，而且其他变量名觉得有点啰嗦，所以稍微改了一下： const shardCount = 32 // Map - // 直接定义32 长度的数组 type Map [shardCount]*Shard // Shard of Map // 分片 type Shard struct { sync.RWMutex items map[string]interface{} } func New() Map { m := Map{} for i := 0; i \u003c shardCount; i++ { m[i] = \u0026Shard{items: make(map[string]interface{})} } return m } ","date":"2022-01-13","objectID":"/posts/shard-map/:2:1","tags":["go","map","数据结构"],"title":"go 语言中的分片 map 的实现","uri":"/posts/shard-map/"},{"categories":["代码技巧"],"content":"2.2. 元素定位 从上述结构可以看出，元素分布于多个 Shard 内的 map 中，那么如何确定某个元素在哪个分片上呢？答案是： 哈希取模的方式定位元素的分片。 // GetShard returns shard under given key func (m Map) GetShard(key string) *Shard { return m[uint(fnv32(key))%uint(shardCount)] } // 哈希算法 func fnv32(key string) uint32 { hash := uint32(2166136261) const prime32 = uint32(16777619) keyLength := len(key) for i := 0; i \u003c keyLength; i++ { hash *= prime32 hash ^= uint32(key[i]) } return hash } ","date":"2022-01-13","objectID":"/posts/shard-map/:2:2","tags":["go","map","数据结构"],"title":"go 语言中的分片 map 的实现","uri":"/posts/shard-map/"},{"categories":["代码技巧"],"content":"2.3. 增改删查 func (m Map) Get(key string) (interface{}, bool) { shard := m.GetShard(key) shard.RLock() defer shard.RUnlock() v, ok := shard.items[key] return v, ok } func (m Map) Set(key string, value interface{}) { shard := m.GetShard(key) shard.Lock() defer shard.Unlock() shard.items[key] = value } func (m Map) Delete(key string) { shard := m.GetShard(key) shard.Lock() defer shard.Unlock() delete(shard.items, key) } // UpsertFunc callback for upsert // if after found oldValue and want to stop the upsert op, you can return result and true for it type UpsertFunc func(exist bool, valueInMap, newValue interface{}) (result interface{}, abort bool) // Upsert - update or insert value and support abort operation after callback func (m Map) Upsert(key string, value interface{}, f UpsertFunc) (res interface{}, abort bool) { shard := m.GetShard(key) shard.Lock() defer shard.Unlock() old, ok := shard.items[key] res, abort = f(ok, old, value) if abort { return } shard.items[key] = res return } 有了基础的方法之后，可以补充一些更方便的方法封装，如 Exists, SetIfNotExists, DeleteAndLoad 等等。 ","date":"2022-01-13","objectID":"/posts/shard-map/:2:3","tags":["go","map","数据结构"],"title":"go 语言中的分片 map 的实现","uri":"/posts/shard-map/"},{"categories":["代码技巧"],"content":"2.4. 高级用法 func (m Map) SetIfAbsent(key string, value interface{}) bool { shard := m.GetShard(key) shard.Lock() defer shard.Unlock() _, ok := shard.items[key] if !ok { shard.items[key] = value return true } return false } func (m Map) DeleteIfExists(key string) bool { shard := m.GetShard(key) shard.Lock() defer shard.Unlock() _, ok := shard.items[key] if !ok { return false } delete(shard.items, key) return true } func (m Map) LoadAndDelete(key string) (v interface{}, loaded bool) { shard := m.GetShard(key) shard.Lock() defer shard.Unlock() v, loaded = shard.items[key] if !loaded { return nil, false } delete(shard.items, key) return v, loaded } func (m Map) Delete(key string) { shard := m.GetShard(key) shard.Lock() defer shard.Unlock() delete(shard.items, key) } func (m Map) Range(f func(key string, value interface{}) bool) { for i := range m { shard := (m)[i] shard.RLock() defer shard.RUnlock() for s, v := range shard.items { if !f(s, v) { return } } } } ","date":"2022-01-13","objectID":"/posts/shard-map/:2:4","tags":["go","map","数据结构"],"title":"go 语言中的分片 map 的实现","uri":"/posts/shard-map/"},{"categories":["代码技巧"],"content":"3. 场景 \u0026 压测 ","date":"2022-01-13","objectID":"/posts/shard-map/:3:0","tags":["go","map","数据结构"],"title":"go 语言中的分片 map 的实现","uri":"/posts/shard-map/"},{"categories":["代码技巧"],"content":"3.1. 使用场景 该结构的特点就是写操作比 sync.Map 高大概 60% 左右，所以使用场景的选择的基础的在于以下两点： 对性能要求比较高，否则 sync.Map 完成足够 数据量大。在数据量比较少的情况下，该结构的优势不够明显 综上述，比较合适的使用场景的应该是 内存数据库。对性能要求高，且数据量会很大，整体性能不会因为数据量高而会下降。 ","date":"2022-01-13","objectID":"/posts/shard-map/:3:1","tags":["go","map","数据结构"],"title":"go 语言中的分片 map 的实现","uri":"/posts/shard-map/"},{"categories":["代码技巧"],"content":"3.2. 压测 压测源码： func BenchmarkShardMap_Set(b *testing.B) { m := New() for i := 0; i \u003c b.N; i++ { k := strconv.Itoa(i) m.Set(k, i) } } func BenchmarkSyncMap_Set(b *testing.B) { m := sync.Map{} for i := 0; i \u003c b.N; i++ { k := strconv.Itoa(i) m.Store(k, i) } } func BenchmarkShardMap_Get(b *testing.B) { m := New() for i := 0; i \u003c 3_000_000; i += 3 { k := strconv.Itoa(i) m.Set(k, i) } for i := 0; i \u003c b.N; i++ { k := strconv.Itoa(i) m.Get(k) } } func BenchmarkSyncMap_Get(b *testing.B) { m := sync.Map{} for i := 0; i \u003c 3_000_000; i += 3 { k := strconv.Itoa(i) m.Store(k, i) } for i := 0; i \u003c b.N; i++ { k := strconv.Itoa(i) m.Load(k) } } func BenchmarkShardMap_Del(b *testing.B) { m := New() for i := 0; i \u003c 3_000_000; i += 3 { k := strconv.Itoa(i) m.Set(k, i) } for i := 0; i \u003c b.N; i++ { k := strconv.Itoa(i) m.Delete(k) } } func BenchmarkSyncMap_Del(b *testing.B) { m := sync.Map{} for i := 0; i \u003c 3_000_000; i += 3 { k := strconv.Itoa(i) m.Store(k, i) } for i := 0; i \u003c b.N; i++ { k := strconv.Itoa(i) m.Delete(k) } } 分别对 sync.Map, ShardMap 进行大量的读写删操作,下面看看压测结果 原始结果 goos: darwin goarch: amd64 pkg: github.com/yusank/godis/lib/shard_map cpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz BenchmarkShardMap_Set BenchmarkShardMap_Set-12 2442687 481.3 ns/op BenchmarkSyncMap_Set BenchmarkSyncMap_Set-12 1442368 736.1 ns/op BenchmarkShardMap_Get BenchmarkShardMap_Get-12 2702954 385.3 ns/op BenchmarkSyncMap_Get BenchmarkSyncMap_Get-12 717051 1409 ns/op BenchmarkShardMap_Del BenchmarkShardMap_Del-12 2704998 384.8 ns/op BenchmarkSyncMap_Del BenchmarkSyncMap_Del-12 480789 2209 ns/op PASS 结果对比 结果如下(单位：ns/op)： 数据结构 Set Get Del ShardMap 481.3 385.3 384.8 sync.Map 736.1 1409 2209 不难发现，在数据量大的情况下（百万级别）sync.Map 的性能会下降很多，这个与 sync.Map 的设计和内部结构有关，感兴趣的朋友可以去阅读一下 sync.Map 的源码。 注意：这里文章最开始时的压测结果差距很大的原因是 数据量不一样，第一个压测结果是基于 50000 个元素之上进行的，所以查询和删除的性能上看上去很高。而这里的压测时基于 3000000 个元素之上进行的。 ","date":"2022-01-13","objectID":"/posts/shard-map/:3:2","tags":["go","map","数据结构"],"title":"go 语言中的分片 map 的实现","uri":"/posts/shard-map/"},{"categories":["代码技巧"],"content":"4. 总结 本篇介绍了一个基于 map 的分片式数据结构 – ShardMap。 该结构可以在数据量比较大的使用替代sync.Map 从而保持比较高的性能。我在自己的 godis 项目内也是用该结构作为基础的哈希表，但是由于godis 单进程处理数据，所以我把其中的读写锁去掉 从而获得更高的性能。 本篇主要内容： 认识 ShardMap 了解到 sync.Map 也存在性能问题 了解到 map 在数据量大的情况下，性能会因为 reshah 机制的存在而有所下降 通过分片的方式，降低单个 map 的数据量，从而减少 rehash 带来的性能的降低 实现和压测 ShardMap ","date":"2022-01-13","objectID":"/posts/shard-map/:4:0","tags":["go","map","数据结构"],"title":"go 语言中的分片 map 的实现","uri":"/posts/shard-map/"},{"categories":["代码技巧"],"content":"5. 链接🔗 orcaman/concurrent-map godis godis/shard_map ","date":"2022-01-13","objectID":"/posts/shard-map/:5:0","tags":["go","map","数据结构"],"title":"go 语言中的分片 map 的实现","uri":"/posts/shard-map/"},{"categories":null,"content":" 感谢各位大佬们的大力支持 杨鼎睿 ","date":"2022-01-07","objectID":"/links/:0:0","tags":null,"title":"友情链接","uri":"/links/"},{"categories":["Redis"],"content":" 本篇讲述 Redis 中的基础数据结构 ZSet(SortedSet) 的底层实现原理和如何通过 go 语言实现一个 ZSet 的过程以及需要注意的问题。 说明 本文章为该系列的有序集合，如果需要阅读其他相关文章， 请点击这里跳转查看 ","date":"2022-01-07","objectID":"/posts/redis-server-zset/:0:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·有序集合篇","uri":"/posts/redis-server-zset/"},{"categories":["Redis"],"content":"1 前言 使用 Redis 过程中集合这个概念出现的比较频繁，常用的 set，zset 都是集合的概念。与普通的集合不同的是，zset 的元素之间是有顺序的，而且这个顺序不是插入的顺序而是使用者插入元素时指定的 score 而定的。 zset 中的任何元素都是有score 值（浮点数）的，而且根据 score 的顺序进行读写甚至可以做到获取范围数据，这些特性给使用者带来了无数种可能性解决方案。 zset 的数据结构 从使用者的角度来说，zset 更像一个 kv 结构,元素不可以重复但是不同元素的 score 值是可以一样的，此时的排序是按元素字典排序。 通过 ZRANGEBYSCORE 命令可以列出指定score范围内的元素以及对应的 key， 其顺序为 score 的升序。 \u003e ZRANGEBYSCORE zset1 0 100 withscores 1) \"a\" 2) \"5\" 3) \"b\" 4) \"5\" 5) \"hello\" 6) \"20\" ","date":"2022-01-07","objectID":"/posts/redis-server-zset/:1:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·有序集合篇","uri":"/posts/redis-server-zset/"},{"categories":["Redis"],"content":"2 zset 支持的能力 zset 作为一个有序集合，即拥有普通集合的特性，同时又基于其有序特性衍生出更多别的特性。主要特性如下： 元素不重复 集合之间交集并集差集的操作 批量读写元素 根据 score 操作（增删改查）元素 获取score最大最小的元素 根据集合内rank（或 index）操作（增删改查）元素 ","date":"2022-01-07","objectID":"/posts/redis-server-zset/:2:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·有序集合篇","uri":"/posts/redis-server-zset/"},{"categories":["Redis"],"content":"3 zset 底层原理 注意 本篇中所有引用的 Redis 源码均基于 Redis6.2 版本。 Redis 实现的 zset 底层是跳跃表和哈希表的组合。跳跃表 用于记录和操作所有基于 score 的操作，而哈希表存的是元素值和 score 的kv关系，用于快速定位元素存不存在的情况。 typedef struct zset { dict *dict; // 哈希表，存储 元素-\u003escore zskiplist *zsl; // 跳跃表 } zset; 除了跳跃表和哈希表之外，其实还有一个不怎么出场的数据结构 – ziplist（压缩列表）。在满足以下两个条件的情况下，Redis 会使用 ziplist来替代跳跃表。 保存的元素少于128个 保存的所有元素大小都小于64字节 在了解跳跃表之前先了简单解一下 ziplist 这个数据结构的实现以及解答为什么要用 ziplist 来替代跳跃表。 ","date":"2022-01-07","objectID":"/posts/redis-server-zset/:3:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·有序集合篇","uri":"/posts/redis-server-zset/"},{"categories":["Redis"],"content":"3.1 压缩列表-ziplist ziplist 编码的有序集合对象使用压缩列表作为底层实现，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员，第二个节点保存元素的分值。并且压缩列表内的集合元素按分值从小到大的顺序进行排列。 从上述的两个条件可以看出，在数据量少且单个元素也比较小的情况下，使用 ziplist 是为了节省内存，因为在数据量少的情况下发挥不出来 skiplist 的优势且占的内存比 ziplist 大。 想更深入了解 ziplist 的实现细节，请点击这里查看 结构分布 ziplist 结构分布 area |\u003c---- ziplist header ----\u003e|\u003c----------- entries -------------\u003e|\u003c-end-\u003e| size 4 bytes 4 bytes 2 bytes ? ? ? ? 1 byte +---------+--------+-------+--------+--------+--------+--------+-------+ component | zlbytes | zltail | zllen | entry1 | entry2 | ... | entryN | zlend | +---------+--------+-------+--------+--------+--------+--------+-------+ ^ ^ ^ address | | | ZIPLIST_ENTRY_HEAD | ZIPLIST_ENTRY_END | ZIPLIST_ENTRY_TAIL ziplist 节点结构分布 area |\u003c------------------- entry --------------------\u003e| +------------------+----------+--------+---------+ component | pre_entry_length | encoding | length | content | +------------------+----------+--------+---------+ ","date":"2022-01-07","objectID":"/posts/redis-server-zset/:3:1","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·有序集合篇","uri":"/posts/redis-server-zset/"},{"categories":["Redis"],"content":"3.2 跳跃表-skiplist 3.2.1 定义 跳跃表是一个随机化的数据结构，实质就是一种可以进行二分查找的有序链表。跳跃表在原有的有序链表上面增加了多级索引，通过索引来实现快速查找。跳跃表不仅能提高搜索性能，同时也可以提高插入和删除操作的性能。 它采用随机技术决定链表中哪些节点应增加向前指针以及在该节点中应增加多少个指针。跳跃表结构的头节点需有足够的指针域，以满足可能构造最大级数的需要，而尾节点不需要指针域。 采用这种随机技术，跳跃表中的搜索、插入、删除操作的时间均为O(logn)，然而，最坏情况下时间复杂性却变成O(n)。相比之下，在一个有序数组或链表中进行插入/删除操作的时间为O(n)，最坏情况下为O(n)。 3.2.2 原理 跳跃表原理非常简单，在链表的基础上每个元素加上一个层(level)的概念，层高则是随机的, 所以每个元素的高度不一样。每一层都会指向下一个同一层的元素，查询元素时由高层向后向下的方式二级检索从而达到更高的查询效率，下面用图解的方式解析如何读写跳跃表元素的。在看图之前可以先看一下源码，尝试理解一下。 // 跳跃表结构 typedef struct zskiplist { struct zskiplistNode *header, *tail; // 记录 head 和 tail 两个节点 unsigned long length; // 记录长度 int level; // 记录当前最高 level，如果有新元素插入且其 level 大于当前最高则更新该值 } zskiplist; // 跳跃表节点 typedef struct zskiplistNode { sds ele; // 元素值 double score; // score struct zskiplistNode *backward; // 向前指向指针，用于往回跳 struct zskiplistLevel { struct zskiplistNode *forward; // 每一层都指向下一个同高度元素 unsigned long span; // 到下一个同高度元素的跨度 } level[]; // 该元素的 level 数组，index 从 0 到 N 表示从最低到最高，默认最高支持 32 层 } zskiplistNode; 如果看完源码还是没有看到 ，请看下图： 跳跃表结构 从图中可以看到， 跳跃表主要由以下部分构成： 表头（head）：负责维护跳跃表的节点指针。 跳跃表节点：保存着元素值，以及多个层。 层：保存着指向其他元素的指针。高层的指针越过的元素数量大于等于低层的指针，为了提高查找的效率，程序总是从高层先开始访问，然后随着元素值范围的缩小，慢慢降低层次。 表尾：全部由 NULL 组成，表示跳跃表的末尾。 注意 图中没有表示出来 zskiplistNode.backward 指针的指向，实际上图中每个元素都会指向前一个元素 3.2.3 查询元素 在跳跃表查询元素，总是从 head 的顶层 level 向后向下的方式取查询，以上面的示例图为例，下面讲解如何查询 score 值为 7 的元素: 初始条件： p 为初始指针，指向 head 的顶层 level 查询步骤： 判断指针 p 的 forward 元素的值 当满足条件：forward.score \u003c score 或 forward.score == score \u0026\u0026 forward.ele \u003c targetEle 时，p 向前移动，level 不变 当 p 的 forward 为 null 或者forward 元素的值大于 score 时，level 减一，但是 p 不往前移动 步骤 1，2 一直循环，指到 p 移动到 null 或者移动到目标元素为止。 跳跃表查询元素过程 源码： /* 下面就是非常常规的一个遍历查找过程 */ x = zsl-\u003eheader; // 从head 顶层level开始向下遍历 for (i = zsl-\u003elevel-1; i \u003e= 0; i--) { // 每一层判断forward元素不为空的时候是否与目标score和ele while (x-\u003elevel[i].forward \u0026\u0026 (x-\u003elevel[i].forward-\u003escore \u003c curscore || (x-\u003elevel[i].forward-\u003escore == curscore \u0026\u0026 // 这里的 sdccmp 是Redis内实现的对其 String 结构的字符串进行对比(即字典排序的对比) sdscmp(x-\u003elevel[i].forward-\u003eele,ele) \u003c 0))) { x = x-\u003elevel[i].forward; } } 然后再真正实现 zset 的时候，不会根据 value 值去遍历查询跳跃表, 而是直接从哈希表查是否存在 3.2.4 添加元素 添加元素核心有以下几点： 找到需要插入的位置，这块用上上一个小节的查询元素相关知识 在查找位置的过程中需要记录牵连到需要更新的元素 如何得到新元素的层高，真的是 [0,32) 之间随机一个数嘛？ 如果新元素的层高大于当前 skiplist 的高度，需要做哪些调整工作？ 对以上几点有了明确的认知和回答后，了解插入元素的过程就变得很简单。 添加元素过程： 定义一个 zskiplistNode *update[ZSKIPLIST_MAXLEVEL] 数组记录每次变更 level 时的节点（后期更新受影响的节点用） 定义 unsigned int rank[ZSKIPLIST_MAXLEVEL] 数组记录两次向下遍历的节点直接的跨度 与查询元素一样，从 head 的顶层开始向下向前遍历，找到插入的位置，这个位置满足 score 值介于前后的元素 在遍历的过程中，每往下移动一次(level - 1 )的时候记录当前元素update[cur_level] = cur_node 在遍历的过程中，每往前移动一次(注：每次移动只会单向 不会同时向前向下)的前记录同一个 level 内的跨度 rank[cur_level] += cur_node.level[cur_level].span (这里之所以累加是因为，同一个 level 上可能会向前移动 n 次，如上面示例图中的从 1 到 6 的过程都是在同一个 level 上进行的) // 定义变量 zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; unsigned int rank[ZSKIPLIST_MAXLEVEL]; int i, level; // 从 header 开始遍历 x = zsl-\u003eheader; // 初始位置 header 的顶层 level for (i = zsl-\u003elevel-1; i \u003e= 0; i--) { /* store rank that is crossed to reach the insert position */ // 如果当前level 为最高一层的 level 则 rank 记录 0 rank[i] = i == (zsl-\u003elevel-1) ? 0 : rank[i+1]; while (x-\u003elevel[i].forward \u0026\u0026 (x-\u003elevel[i].forward-\u003escore \u003c score || (x-\u003elevel[i].forward-\u003escore == score \u0026\u0026 sdscmp(x-\u003elevel[i].forward-\u003eele,ele) \u003c 0))) { // 向前移动时记录当前 level 移动的跨度 rank[i] += x-\u003elevel[i].span; x = x-\u003elevel[i].forward; } // level - 1 时 记录当前元素 update[i] = x; } 随机一个 level， Redis 是有一套简单的算法去生成随机的 level 跳转查看。 如果随机的 level 大于 skiplist 当前最高 level，则在 update 数组记录从当前最高到新的最高之间的level 对应的节点为 head 节点 /* we assume the element is not already inside, since we allow duplicated * scores, reinserting the same element should never happen since the * caller of zslInsert() should test in the hash table if the element is * already inside or not. */ level = zslRandomLevel(); if (level \u003e zsl-\u003elevel) { // 在 zsl-\u003elevel 到 level 之间区域补缺原来的空缺 // 原来高于 zsl-\u003elevel 的 level 均指向 null，现在需要指向到新的元素对应的 level 了 for (i = zsl-\u003elevel; i \u003c level; i++) { rank[i] = 0; // 因为最高的 level 了所以在该层不会指向下一个元素 所以对应的 rank == 0 update[i] = zsl-\u003eheader; // header 需要更新 update[i]-\u003elevel[i].span = zsl-\u003elength; // 不再","date":"2022-01-07","objectID":"/posts/redis-server-zset/:3:2","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·有序集合篇","uri":"/posts/redis-server-zset/"},{"categories":["Redis"],"content":"4 zset 实现 上面关于zset的原理和实现都理解的比较透彻了，如果还有不明白的建议看源码，结合源码上下文更好理解。现在我用 go 语言实现跳跃表。关于压缩表我在这里不会涉及到只实现跳跃表相关代码。 ","date":"2022-01-07","objectID":"/posts/redis-server-zset/:4:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·有序集合篇","uri":"/posts/redis-server-zset/"},{"categories":["Redis"],"content":"4.1 数据结构定义 数据结构定义基本与 Redis 一致，跳跃表 + map 的组合。其中 map 部分为了提高读写性能，自己实现了一个 map 结构。 // zSet is object contain skip list and map which store key-value pair type zSet struct { // smap.Map 为自己实现的原生 map 的封装，之后单独讲一下，之所以自己实现是为了提高性能 m smap.Map // store key and value // 在元素少于 100 \u0026 每个元素大小小于 64 的时候,Redis 实际上用的是 zipList 这里作为知识点提了一下 // 除非遇到性能问题,否则不准备同时支持 zipList 和 skipList zsl *zSkipList // skip list } type zSkipList struct { head, tail *zSkipListNode length int // 总长度 level int // 最大高度 } type zSkipListNode struct { value string score float64 backward *zSkipListNode levels []*zSkipListLeve } type zSkipListLeve struct { forward *zSkipListNode span uint // 当前 level 到下一个节点的跨度 } ","date":"2022-01-07","objectID":"/posts/redis-server-zset/:4:1","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·有序集合篇","uri":"/posts/redis-server-zset/"},{"categories":["Redis"],"content":"4.2 初始化 因为存在一个 header 的虚拟节点，所以初始化的时候需要把跳跃表的 header 以及其每一层都初始化。 // new skiplist func newZSkipList() *zSkipList { zsl := \u0026zSkipList{ level: 1, // 每一层为空的 ZSkipListMaxLevel 层的 head head: newZslNode(ZSkipListMaxLevel, 0, \"\"), } return zsl } // new node func newZslNode(level int, score float64, value string) *zSkipListNode { node := \u0026zSkipListNode{ value: value, score: score, levels: make([]*zSkipListLeve, level), } // 初始化每一层 for i := 0; i \u003c level; i++ { node.levels[i] = \u0026zSkipListLeve{} } return node } ","date":"2022-01-07","objectID":"/posts/redis-server-zset/:4:2","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·有序集合篇","uri":"/posts/redis-server-zset/"},{"categories":["Redis"],"content":"4.3 其他功能 增删改查的代码与上面源码解析的逻辑大致相同，我在这里给出go 语言实现的源码，可以点击查看。在这里不再讲述这些基础功能的实现， 而是给出一些特殊的方法的实现。 4.3.1 根据排名查找元素 在上面的实现里会看到到处飞的 span 这个属性，但是好像一直没有实际用上，其实在遍历过程中尤其是跟排名相关的操作里这个 span 属性是非常的有用，下面看一下实际用处： func (zsl *zSkipList) findElementByRank(rank uint) *zSkipListNode { var ( x = zsl.head // 已遍历的距离 traversed uint ) // 从 head 的顶层开始遍历 for i := zsl.level - 1; i \u003e= 0; i-- { // 如果当前level 的下一个元素的距离 + 已经走过的距离 小于 目标排名 -\u003e 向前移动 // 否则 level - 1 for x.levels[i].forward != nil \u0026\u0026 traversed+x.levels[i].span \u003c= rank { traversed += x.levels[i].span x = x.levels[i].forward } // level -1 前判断是否达到目标 rank if traversed == rank { return x } } return nil } 因为记录了与下一个元素的距离，根据排名找元素变得很简单高效，只要跳跃对应的距离即可（距离代表的就是跨越多少个元素也就是多少个排名位置） 4.3.2 zrange 实现 zrange 这个命令是使用 zset 时最常用的命令之一, 那底层是怎么实现的呢？ // start stop 支持负数 负数时表示倒数第几个 func (zsl *zSkipList) zRange(start, stop int, withScores bool) []string { if start \u003c 0 { start = start + zsl.length if start \u003c 0 { start = 0 } } if stop \u003c 0 { stop = stop + zsl.length } if start \u003e stop || start \u003e= zsl.length { return nil } if stop \u003e= zsl.length { stop = zsl.length - 1 } // 到目前为止是为了处理 start 和 stop 越界问题，并把负数换算成正数 方便下面处理 // 先用上面的方法找到遍历的第一个元素 node := zsl.findElementByRank(uint(start) + 1) var ( rangeLen = stop - start + 1 result []string ) // 从 start 元素开始遍历 for rangeLen \u003e 0 { result = append(result, node.value) if withScores { result = append(result, strconv.FormatFloat(node.score, 'g', -1, 64)) } // 跳跃表的第 0 层可以看做做是一个链表，这样遍历读取多个元素就很方便了 node = node.levels[0].forward rangeLen-- } return result } 如何逆向遍历 如果需要逆向遍历 直接把 node = node.levels[0].forward 改成 node = node.levels[0].backward 即可。 ","date":"2022-01-07","objectID":"/posts/redis-server-zset/:4:3","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·有序集合篇","uri":"/posts/redis-server-zset/"},{"categories":["Redis"],"content":"5 总结 写到这里，Redis 如何实现 zset 的原理和源码以及如何用 go 语言自己写一遍都讲完了，下面做个简单的总结。 zset 底层是两种数据结构组成（ziplist， skiplist + dict），根据存储的数据量不同从决定使用哪个 跳跃表是一个树状结构，读写时间复杂度 O(logN) 跳跃表的level 是随机算法算出来的，确保每一层是上一次的 P 倍，level 越低数据分布越密集 如果对 Redis 的源码或者跳跃表比较熟悉的话，go 语言的实现基本没有任何难度，是把理解转换成代码过程 实现过程需要注意的是一些特殊情况，包括边界问题，head 和 tail 的问题以及操作某个元素其牵连到的附近的元素 ","date":"2022-01-07","objectID":"/posts/redis-server-zset/:5:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·有序集合篇","uri":"/posts/redis-server-zset/"},{"categories":["Redis"],"content":"6 参考链接🔗 压缩列表-ziplist 跳跃表-skiplist go 语言实现 Redis ","date":"2022-01-07","objectID":"/posts/redis-server-zset/:6:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·有序集合篇","uri":"/posts/redis-server-zset/"},{"categories":["Redis"],"content":" 本篇讲述 Redis 中的基础数据结构 List 的底层实现原理和如何通过 go 语言实现一个 List 的过程以及需要注意的问题。 说明 本文章为该系列的链表，如果需要阅读其他相关文章， 请点击这里跳转查看 ","date":"2021-12-24","objectID":"/posts/redeis-server-list/:0:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·链表篇","uri":"/posts/redeis-server-list/"},{"categories":["Redis"],"content":"1 前言 众所周知，Redis 中有五大数据结构，在各种面试中也会经常遇到相关的问题，从这一篇开始，我把这个五大数据结构（string, list, set, sorted_set, hash_map）的底层原理和如何用 go 语言实现讲明白。 ","date":"2021-12-24","objectID":"/posts/redeis-server-list/:1:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·链表篇","uri":"/posts/redeis-server-list/"},{"categories":["Redis"],"content":"2 list能力 list 是一个我们常用的一个 Redis 特性，特定就是先进后出 FILO 。并且支持双端的读写，所以也可以在使用过程中也能实现基于 list 的 先进先出 FIFO 模型。 总结一下，Redis 支持的能力： 双端读写 批量读写 list 内部元素的增删改 阻塞读取 ","date":"2021-12-24","objectID":"/posts/redeis-server-list/:2:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·链表篇","uri":"/posts/redeis-server-list/"},{"categories":["Redis"],"content":"3 list 底层原理 Redis 实现 list 的是双向链表(linked-list)。这个数据结构大家应该非常的熟悉，且经常拿链表和数组进行对比。相对于数组，链表最大的优势在于写入元素时不需要考虑数组一样 grow 过程，只需要将新元素连接到链表最后即可，而数组是需要考虑扩容缩容时数组 grow 问题的。 数据结构： type List struct { // 记录头和尾 head, tail *listNode } type listNode struct { // 双向链表 // 相对于单向链表 多记录一个 prev 指向前一个元素 next, prev *listNode value string } 从数据结构来看，其实一点都不复杂，只需要记录第一个和最后一个元素，元素内部记录前一个和后一个元素的指针即可。读写都是基于修改元素内指向的指针来完成。 配合下面图看代码就更好理解了： ","date":"2021-12-24","objectID":"/posts/redeis-server-list/:3:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·链表篇","uri":"/posts/redeis-server-list/"},{"categories":["Redis"],"content":"4 list的实现 下面我们开始用 go 语言来实现 Redis 中的 list 数据结构的特性。 ","date":"2021-12-24","objectID":"/posts/redeis-server-list/:4:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·链表篇","uri":"/posts/redeis-server-list/"},{"categories":["Redis"],"content":"4.1 定义和初始化 list 定义： type List struct { length int // 记录总长度 head, tail *listNode } type listNode struct { next, prev *listNode value string } 初始化： func newList() *List{ return new(List) } func newListNode(val string) *listNode { return \u0026listNode{ value: val, } } ","date":"2021-12-24","objectID":"/posts/redeis-server-list/:4:1","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·链表篇","uri":"/posts/redeis-server-list/"},{"categories":["Redis"],"content":"4.2 增查元素 4.2.1 增加元素 // n 为 head 时调用 // 即新增一个元素并把该元素置位 head func (n *listNode) addToHead(val string) *listNode { node := newListNode(val) n.prev = node node.next = n return node } // n 为 tail 时调用 // 即新增一个元素并把该元素置位 tail func (n *listNode) addToTail(val string) *listNode { node := newListNode(val) n.next = node node.prev = n return node } 以上两个方法配合下面两个 LPush和 RPush 方法使用： func (l *List) LPush(val string) { l.length++ // 如果list 内已经有元素，则把新增元素置位 head if l.head != nil { l.head = l.head.addToHead(val) return } // 当前 list 为空，则将新元素置位 head 和 tail node := newListNode(val) l.head = node l.tail = node } // 逻辑与上面一致 sssss func (l *List) RPush(val string) { l.length++ if l.tail != nil { l.tail = l.tail.addToTail(val) return } node := newListNode(val) l.head = node l.tail = node } 4.2.2 pop元素 基础方法： // pop head 元素 并返回下一个元素 // pop current node and return next node func (n *listNode) popAndNext() *listNode { var next = n.next // 将当前节点的 next 置位空 n.next = nil if next != nil { next.prev = nil } return next } // pop tail 元素 并返回下一个元素 // pop current node and return prev node func (n *listNode) popAndPrev() *listNode { var prev = n.prev n.prev = nil if prev != nil { prev.next = nil } return prev } 下面是真正实现 LPop, RPop 方法： // left pop 从左边 pop 一个元素 func (l *List) LPop() (val string, ok bool) { if l.head == nil { return \"\", false } l.length-- val = l.head.value l.head = l.head.popAndNext() if l.head == nil { l.tail = nil } return val, true } // right pop 从右边 pop 一个元素 func (l *List) RPop() (val string, ok bool) { if l.tail == nil { return \"\", false } l.length-- val = l.tail.value l.tail = l.tail.popAndPrev() if l.tail == nil { l.head = nil } return val, true } 4.2.3 查询元素 根据 value 查询第一个元素（list 内元素值是可以重复的，所以查询第一个值相同的元素） func (l *List) findNode(val string) *listNode { var cur = l.head for cur != nil { if cur.value == val { return cur } cur = cur.next } return nil } 根绝 index 查询元素 func (l *List) lIndexNode(i int) *listNode { if l.head == nil { return nil } // 支持反向查，即如果 i 小于 0 则认为是倒数第 i 个元素，把 i 该为正数第 i 个 if i \u003c 0 { i += l.length } if i \u003e= l.length || i \u003c 0 { return nil } var ( idx int cur = l.head reverse = i \u003e l.length/2+1 ) // 为了查询效率，做一个小小的优化 // 如果 i 在前半段则从头到尾的遍历，反之从尾到头 if reverse { idx = l.length - 1 cur = l.tail } for i != idx { if reverse { idx-- cur = cur.prev } else { idx++ cur = cur.next } } return cur } 4.2.4 删除元素 已经支持通过 value/index 查询元素了，就可以删除 list 内元素了，下面以从尾部开始删除 n 个元素为例： // i 表示 index 删除第 i 个元素 func (l *List) Rem(i int) int { node := l.lIndexNode(i) if node == nil { return 0 } p := node.prev n := node.next if p != nil { p.next = nil } else { // node 是 head 所以没有 prev l.head = n } if n != nil { n.prev = nil } else { // node 是 tail 所以没有 next l.tail = p } if l.head == nil || l.tail == nil { // 删除了最后一个元素了 l.head = nil l.tail = nil } l.length-- return 1 } ","date":"2021-12-24","objectID":"/posts/redeis-server-list/:4:2","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·链表篇","uri":"/posts/redeis-server-list/"},{"categories":["Redis"],"content":"4.3 高级特性 配合上面的一些基础方法以及链表的特性，可以写出不少花样玩法. 4.3.1 某个元素前后插入新元素 场景 1 Q: 假如我想在 list 内已知的元素的前面或后面新增一个元素，仅通过增删改查是做不到，有什么好的方法呢？ A: 其实还是通过遍历链表找到插入的位置 修改前后指向指针即可。 实现源码如下： // flag 大于 0 插入到 target 后面 小于 0 插入前面 func (l *List) LInsert(target, newValue string, flag int) bool { if l.head == nil { return false } // 找到元素 node := l.findNode(target) if node == nil { return false } if flag == 0 { node.value = newValue return true } newNode := \u0026listNode{value: newValue} l.length++ // insert after if flag \u003e 0 { next := node.next node.next = newNode newNode.prev = node if next == nil { l.tail = newNode } else { newNode.next = next next.prev = newNode } return true } // insert before prev := node.prev node.prev = newNode newNode.next = node if prev == nil { l.head = newNode } else { newNode.prev = prev prev.next = newNode } return true } 4.3.2 批量删除元素 从某个元素开始往后删除 N 个或删除所有元素。 // 从head 开始遍历删除 n 个值等于 value 的元素 func (l *List) LRemCountFromHead(value string, n int) (cnt int) { var ( dumbHead = \u0026listNode{ next: l.head, } // 引入虚拟 head 是为了 防止从第一个元素删除，然后需要频繁修改 l.head 的值 // 同时为了减少过多的特殊判断 prev = dumbHead cur = l.head next = l.head.next ) // 开始遍历 for cur != nil \u0026\u0026 n \u003e 0 { // 找打元素 if cur.value == value { // cur 前后的元素连接起来 prev.next = next if next != nil { next.prev = prev } // cur 指向前后的指针置位空 cur.prev, cur.next = nil, nil cnt++ n-- } else { // 只有在没有被删除元素时才移动 prev // *注意：prev 不能每次都移动，因为不确定下一个元素是不是也是要被删除的， // 只有确保 cur 不是我们要找的元素时候 才会同时移动三个指针* prev = prev.next } // dumHead 1 2 2 4 5 // ^ ^ ^ -\u003e\u003e // prev cur next // cur 和 next 往后移动 cur = next if next != nil { next = next.next } } // remove last element if prev.next == nil { l.tail = prev } l.head = dumbHead.next if l.head != nil { // if remove first element from, dumbHead.next.prev will be point to dumbHead // // In other words, l.head.tail will be not nil l.head.prev = nil } l.length -= cnt return } 4.3.3 局部遍历 场景 2 Q: 需要读取前几个或者后几个或者从第 N 个到第 M 个元素，但是不想 pop 出来怎么办？ 局部遍历的实现： func (l *List) LRange(start, stop int) (values []string) { if l.head == nil { return nil } // 这里需要支持负数，因为 Redis lrange 是支持负数 // 负数代表倒数第 N 个 if start \u003c 0 { start = start + l.length if start \u003c 0 { start = 0 } } // 需要处理一下负数 虽然客户端表达式倒数第 N 个 但是实现的时候都是统一从头遍历到尾 // 全部转换为正数 if stop \u003c 0 { stop = stop + l.length } // start already \u003e=0 , so if stop \u003c 0 then this case is true if start \u003e stop || start \u003e l.length { return nil } if stop \u003e= l.length { stop = l.length - 1 } var ( head = l.head idx int ) for head != nil \u0026\u0026 idx \u003c= stop { if idx \u003e= start { values = append(values, head.value) } idx++ head = head.next } return } ","date":"2021-12-24","objectID":"/posts/redeis-server-list/:4:3","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·链表篇","uri":"/posts/redeis-server-list/"},{"categories":["Redis"],"content":"5 总结 目前位置除了阻塞读取外，其他数据结构特性都以全部实现或者实现了底层方法 ，上面封装即可，完整代码请看 GitHub 项目。关于阻塞这块，由于服务端和客户端是长链接，所以实现其实比较简单，而且也属于数据结构的范畴，所以不再这里细讲，下面给出思路。 起一个 goroutine, 监听对应 key 的数据 有数据之前这次请求时不响应，知道返回正确结果或者客户端主动断开连接 拿到数据后，停止 goroutine, 响应客户端。 整体下来因为数据处理都是单进程，不需要考虑进程间资源竞争问题，代码相对简洁很多，注意增删元素时前后关系以及极限情况（边界的元素的修改）。 ","date":"2021-12-24","objectID":"/posts/redeis-server-list/:5:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·链表篇","uri":"/posts/redeis-server-list/"},{"categories":["Redis"],"content":"6 项目链接🔗 https://github.com/yusank/godis ","date":"2021-12-24","objectID":"/posts/redeis-server-list/:6:0","tags":["redis","系列篇","数据结构"],"title":"[系列]Redis Server 实现·链表篇","uri":"/posts/redeis-server-list/"},{"categories":["Go"],"content":" Go 已经确实在 1.18 版本支持泛型了，预计 2022 年 2 月份发布 1.18 正式版，到目前为止泛型相关规范已确定且可以在开发分支的 go 版本中尝试使用了，这篇文章带你领略 go 的泛型. ","date":"2021-12-09","objectID":"/posts/go-generic/:0:0","tags":["go1.18","泛型"],"title":"Go 泛型提前尝试","uri":"/posts/go-generic/"},{"categories":["Go"],"content":"安装 go 开发版 没有发版我怎么提前尝试呢？ 在这里分享一个小技巧，学会了之后以后每个新版本发布前都可以提前尝试新版的特性，提前学习好。 可以通过下面的命令安装最新的 master 分支： go install golang.org/dl/gotip gotip download 现在可以把 gotip 这个命令当做 go 命令来使用了， gotip version go version devel go1.18-d6c4583 Wed Dec 8 23:38:20 2021 +0000 linux/amd64 灵活使用 bash alias gotip 敲起来麻烦，总是习惯性打 go build/run , 我就在 .bashrc 添加了一行配置 alias go18=\"gotip\" # 如果你有单独的 workspace 直接用 go=\"gotip\" 也可以 ","date":"2021-12-09","objectID":"/posts/go-generic/:1:0","tags":["go1.18","泛型"],"title":"Go 泛型提前尝试","uri":"/posts/go-generic/"},{"categories":["Go"],"content":"泛型 ","date":"2021-12-09","objectID":"/posts/go-generic/:2:0","tags":["go1.18","泛型"],"title":"Go 泛型提前尝试","uri":"/posts/go-generic/"},{"categories":["Go"],"content":"基础用法 终于可以不用为 int/uint/int8/int16/int32/int64 写一堆代码类似的代码了！ type Integer interface{ ~uint|~uint8|~uint16|~uint32|~uint64|~int|~int8|~int16|~int32|~int64 } func Max[T Integer](a, b T) T { if a \u003e b { return a } return b } func main() { fmt.Println(Max(1, 2)) // 2 fmt.Println(Max(uint(1), uint(2))) // uint(2) fmt.Println(Max(int64(1), int64(2))) // 2 } 定义支持的类型 Integer, | 表示并集， ~ 表示底层是该类型也可以(type CustomInt int 这种情况)。这个就是最基础的基于泛型的代码了，是不是看着都不太像 go 代码了，哈哈。 其实， Integer 类型不需要我们定义了，go 官方新增了 constraints 包，放了一些常用的泛型类型，后期应该也会扩展。 // Copyright 2021 The Go Authors. All rights reserved. // Use of this source code is governed by a BSD-style // license that can be found in the LICENSE file. // Package constraints defines a set of useful constraints to be used // with type parameters. package constraints // Signed is a constraint that permits any signed integer type. // If future releases of Go add new predeclared signed integer types, // this constraint will be modified to include them. type Signed interface { ~int | ~int8 | ~int16 | ~int32 | ~int64 } // Unsigned is a constraint that permits any unsigned integer type. // If future releases of Go add new predeclared unsigned integer types, // this constraint will be modified to include them. type Unsigned interface { ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64 | ~uintptr } // Integer is a constraint that permits any integer type. // If future releases of Go add new predeclared integer types, // this constraint will be modified to include them. type Integer interface { Signed | Unsigned } // Float is a constraint that permits any floating-point type. // If future releases of Go add new predeclared floating-point types, // this constraint will be modified to include them. type Float interface { ~float32 | ~float64 } // Complex is a constraint that permits any complex numeric type. // If future releases of Go add new predeclared complex numeric types, // this constraint will be modified to include them. type Complex interface { ~complex64 | ~complex128 } // Ordered is a constraint that permits any ordered type: any type // that supports the operators \u003c \u003c= \u003e= \u003e. // If future releases of Go add new ordered types, // this constraint will be modified to include them. type Ordered interface { Integer | Float | ~string } 上面的代码可以简化为如下： import ( \"constraints\" ) func Max[T constraints.Integer](a, b T) T { if a \u003e b { return a } return b } func main() { fmt.Println(Max(1, 2)) // 2 fmt.Println(Max(uint(1), uint(2))) // uint(2) fmt.Println(Max(int64(1), int64(2))) // 2 } 当然如果想扩展也是完全可以，比如上面的 Max 方法要支持 float 类型的话，自己定义一个新的 interface 即可。 type Number interface{ constraints.Integer | constraints.Float } ","date":"2021-12-09","objectID":"/posts/go-generic/:2:1","tags":["go1.18","泛型"],"title":"Go 泛型提前尝试","uri":"/posts/go-generic/"},{"categories":["Go"],"content":"花式玩法 slice 如果你写 go 的时间长的话， 应该经历过写很多长得很像排序算法，也应该羡慕过python/js的直接 string.sort()这种写法的，现在用泛型也可以玩出同样的姿势了。废话不多说直接上货： type Slice[T constraints.Ordered] []T func (s Slice[T]) Sort() { sort.Slice(s,func (i,j int) bool { return s[i] \u003c s[j] }) } func main() { // 随时定义随时调用，没有过多的方法调用或者额外的条件判断了 ss := Slice[string]{\"b\",\"d\",\"c\",\"a\"} ss.Sort() fmt.Println(ss) is := Slice[int]{2,4,1,3} is.Sort() fmt.Println(is) } 这块代码运行成功后，我反正是很爽的，以后起码少些很多长得像功能还一样的代码了，等正式发版后可以搞一波支持泛型的基础库了。 map map 的 key-value 均可以指定泛型，从而灵活的做一些处理，下面以返回某个 map 的 key 作为数组的例子： // 定义一个 key 可以做对比的类型 value 作为任意类型 // 注： any == interface{} type Map[K constraints.Ordered, V any] map[K]V func (m Map[K,V]) Keys() []K { var result []K for k := range m { result = append(result, k) } return result } func main() { sm := Map[string,int]{\"a\":1,\"b\":2} fmt.Println(sm.Keys()) // [a, b] im := Map[int,string]{1:\"a\",2:\"b\"} fmt.Println(im.Keys()) // [1, 2] } 这样减少了很多类型判断类型转换的过程了，只要初始化的时候声明好了后返回的就是对应类型的数组而不是 []interface{} . chan chan 作为在高并发异步编程中非常常用的特性，之前也面领着同样类型转换类型判断的困扰，设想一个场景，如果我有个需求，将数组转换成 channel，异步的去处理这组数据，如果数组类型是单个还好 如果我又有 string 的又有 int 的未来可能还会有别的类型，那只能一个类型一个方法或者统一 interface 然后使用时类型转换。如果用泛型实现呢？ // 类型也可以按需求限制在 Ordered 或 comparable 等范围 func convertChan[T any](slice []T) chan T { ch := make(chan T, 1) go func() { defer close(ch) for _, v := range slice { ch \u003c- v } }() return ch } func main() { s1 := []string{\"a\", \"b\", \"c\"} // s2 := []float64{1.1, 1.2, 1.3} ch := convertChan(s1) // ch =\u003e chan string for v := range ch { fmt.Println(v) } // a, b, c } ","date":"2021-12-09","objectID":"/posts/go-generic/:2:2","tags":["go1.18","泛型"],"title":"Go 泛型提前尝试","uri":"/posts/go-generic/"},{"categories":["Go"],"content":"总结 总结 整体而言，新增的泛型我觉得利大于弊的，看起来是语法复杂了很多其实并没有，定义的时候限制一些可用的基础类型或者直接用 any 来表示接受任何类型的参数即可。 对于开发者来说绝对会减少一部分重复代码的，也可以做一些更好的抽象，从长远来说对开发者还是好处很多的。 1.18是第一个支持泛型的版本，肯定会谨慎一些，之后的版本该功能说不定会得到更多的扩展和支持，所以还是很期待的。 最后 如果你对泛型有自己不一样的看法或者用法，也可以来讨论讨论。 ","date":"2021-12-09","objectID":"/posts/go-generic/:3:0","tags":["go1.18","泛型"],"title":"Go 泛型提前尝试","uri":"/posts/go-generic/"},{"categories":["Go"],"content":"参考文档 https://bitfieldconsulting.com/golang/generics https://colobu.com/2021/10/24/go-generic-eliding-interface/ ","date":"2021-12-09","objectID":"/posts/go-generic/:4:0","tags":["go1.18","泛型"],"title":"Go 泛型提前尝试","uri":"/posts/go-generic/"},{"categories":["Redis"],"content":" 这一篇主要是将如何定义一个比较完善的服务入口以及如何管理服务的生命周期、如何处理 tcp 的连接管理和请求处理等相关内容。 说明 本文章为该系列的服务管理篇，如果需要阅读其他相关文章， 请点击这里跳转查看 ","date":"2021-12-06","objectID":"/posts/redis-server-network/:0:0","tags":["redis","系列篇","网络"],"title":"[系列]Redis Server 实现·服务管理篇","uri":"/posts/redis-server-network/"},{"categories":["Redis"],"content":"定义服务 该项目作为 Redis Server, 需要定义一个 Server 对象 作为服务启动关闭及请求处理的的入口。 type Server struct { addr string // 监听 ip:port handler api.Handler // 请求处理方法 listener net.Listener // 监听入口 } func NewServer(addr string, h api.Handler) *Server { return \u0026Server{ addr: addr, handler: h, } } ","date":"2021-12-06","objectID":"/posts/redis-server-network/:1:0","tags":["redis","系列篇","网络"],"title":"[系列]Redis Server 实现·服务管理篇","uri":"/posts/redis-server-network/"},{"categories":["Redis"],"content":"启动服务 正常监听 tcp 服务，并处理连接即可。 func (s *Server) Start() error { l, err := net.Listen(\"tcp\", s.addr) if err != nil { log.Println(\"listen err:\", err) return err } log.Println(\"listen: \", l.Addr()) s.listener = l // 阻塞处理 s.handleListener() return nil } ","date":"2021-12-06","objectID":"/posts/redis-server-network/:2:0","tags":["redis","系列篇","网络"],"title":"[系列]Redis Server 实现·服务管理篇","uri":"/posts/redis-server-network/"},{"categories":["Redis"],"content":"处理连接 之后便可以 accept 连接请求，处理请求。在每建立一个新的连接的时候，启动一个 goroutine 来处理该连接，从而支持高并发的请求。 // Server 内保存 net.Listener 等服务必要参数 func (s *Server) handleListener() { for { conn, err := s.listener.Accept() if err != nil { // 如果 listener 已被关闭则退出 if errors.Is(err, net.ErrClosed) { log.Println(\"closed\") break } log.Println(\"accept err:\", err) continue } log.Println(\"new conn from:\", conn.RemoteAddr().String()) // 处理该请求 go s.handleConn(conn) } } 处理逻辑如下： // handle by a new goroutine func (s *Server) handleConn(conn net.Conn) { defer func() { _ = conn.Close() }() // 初始化 Server 时，将 handler 也注册进来 // Handle 方法的核心逻辑时，读取请求内容，根据到 Redis 协议解析内容 // 并对这次请求做出响应并返回 reply reply, err := s.handler.Handle(reader) if err == io.EOF { return } if err != nil { log.Println(\"handle err:\", err) return } if len(reply) == 0 { return } _, err = conn.Write(reply) if err != nil { log.Println(\"write err:\", err) return } } ","date":"2021-12-06","objectID":"/posts/redis-server-network/:3:0","tags":["redis","系列篇","网络"],"title":"[系列]Redis Server 实现·服务管理篇","uri":"/posts/redis-server-network/"},{"categories":["Redis"],"content":"处理请求 上述处理逻辑中比较重要的一个逻辑是 handler.Handle(reader), 这里面是如何读取请求内容并解析协议的，下面将会以简化的代码逻辑 讲述处理逻辑： func (TCPHandler) Handle(r api.Reader) ([]byte, error) { // io data to protocol msg rec, err := protocol.DecodeFromReader(r) if err != nil { return nil, err } log.Println(rec) rsp := redis.NewCommandFromReceive(rec).Execute(context.Background()) log.Println(\"rsp:\", debug.Escape(string(rsp.Encode()))) return rsp.Encode(), err } // 1. 读取内容decode协议 type Receive []string func DecodeFromReader(r api.Reader) (rec Receive, err error) { rec = make([]string, 0) // read first line b, err := r.ReadBytes('\\n') if err != nil { log.Println(\"readBytes err:\", err) return nil, err } // decode line content str, length, desc, err := decodeSingleLine(b) if err != nil { log.Println(\"init message err:\", err) return nil, err } // 如果是 bulk 或者 array 则需要往下读取 length 行 // length 从第一行内容中解析出来 if desc == DescriptionBulkStrings { temp, err1 := readBulkStrings(r, length) if err1 != nil { log.Println(\"read bulk str err:\", err1) return nil, err1 } rec = append(rec, string(temp)) return } if desc == descriptionArrays { // won't sava array element items, err1 := readArray(r, length) if err1 != nil { log.Println(\"read bulk str err:\", err1) return nil, err1 } rec = append(rec, items...) return } rec = append(rec, str) return } // 2. 处理请求（处理 Redis 命令） rsp := redis.NewCommandFromReceive(rec).Execute(context.Background()) // 3. 处理结果 encode 成 Redis 协议 return rsp.Encode(), err 小结总结 至此，一个简单的 server 端的能力基本都有了，从服务启动到监听端口、处理连接、处理请求以及响应。 但是问题也很多： 请求处理完连接会断开，需要支持长链接 服务启动后直接阻塞主线程，且没有优雅退出逻辑，导致服务关闭时可能存在请求未处理完的情况 goroutine 无限开启并不能更好的处理和管理 下面针对以上问题进行一步步优化。 ","date":"2021-12-06","objectID":"/posts/redis-server-network/:4:0","tags":["redis","系列篇","网络"],"title":"[系列]Redis Server 实现·服务管理篇","uri":"/posts/redis-server-network/"},{"categories":["Redis"],"content":"更完善的服务定义 type Server struct { addr string // 新增支持 context 从而更好的控制上下文和下游 goroutine ctx context.Context cancel context.CancelFunc handler api.Handler listener net.Listener // 新增 WaitGroup 更好控制并发和退出逻辑 wg *sync.WaitGroup } Question 单从服务定义看不出来太多的变化，即便新增几个字段又能如何解决上面的问题呢？ ","date":"2021-12-06","objectID":"/posts/redis-server-network/:5:0","tags":["redis","系列篇","网络"],"title":"[系列]Redis Server 实现·服务管理篇","uri":"/posts/redis-server-network/"},{"categories":["Redis"],"content":"更优雅的服务启停 服务启动和运行过程中感知到服务以外的一些数据才能在一些特殊情况下更从容的 handle 住问题。这个服务以外的数据一般就是系统的信号量(Signal) .除此之外还需要关心下游的 goroutine 的情况，在下游服务遇到不可控的 Fatel 事件时，上游服务需要做判断是否要关闭服务。在主 server 需要关停时，需要让下游服务感知到且给下游 goroutine 处理的时间但又得有一定的时间控制 不能无限期等待，这些都是需要考虑的问题。 ","date":"2021-12-06","objectID":"/posts/redis-server-network/:6:0","tags":["redis","系列篇","网络"],"title":"[系列]Redis Server 实现·服务管理篇","uri":"/posts/redis-server-network/"},{"categories":["Redis"],"content":"主 server 的启停 func (s *Server) Start() error { // 监听信号量 sigChan := make(chan os.Signal, 1) signal.Notify(sigChan, syscall.SIGTERM, syscall.SIGINT, syscall.SIGQUIT) l, err := net.Listen(\"tcp\", s.addr) if err != nil { log.Println(\"listen err:\", err) return err } log.Println(\"listen: \", l.Addr()) s.listener = l // 起一个 goroutine 去等待信号量或 ctx 的结束 go func() { select { case \u003c-s.ctx.Done(): log.Println(\"kill by ctx\") return case sig := \u003c-sigChan: s.Stop() log.Printf(\"kill by signal:%s\", sig.String()) return } }() //阻塞处理连接 s.handleListener() return nil } // Stop 可以被 Start 方法调用也可以被 main 的其他协程调用 func (s *Server) Stop() { s.cancel() _ = s.listener.Close() } ","date":"2021-12-06","objectID":"/posts/redis-server-network/:6:1","tags":["redis","系列篇","网络"],"title":"[系列]Redis Server 实现·服务管理篇","uri":"/posts/redis-server-network/"},{"categories":["Redis"],"content":"下游 handler 的启停 处理连接时，由sync.WaitGroup 控制 goroutine，这样可以在某个连接还未处理完成时，可以继续阻塞， 从而做到服务关闭时等待未处理的请求。 func (s *Server) handleListener() { for { conn, err := s.listener.Accept() if err != nil { if errors.Is(err, net.ErrClosed) { log.Println(\"closed\") break } log.Println(\"accept err:\", err) continue } log.Println(\"new conn from:\", conn.RemoteAddr().String()) s.wg.Add(1) go s.handleConn(conn) } // wait for unDone connections s.wg.Wait() } 在处理连接上的请求时，通过 for 循环一直读取连接上的内容，如果客户端没有写入消息则会阻塞，如何客户端主动关闭连接则会读取 EOF 错误。没处理完一次请求先判断服务是否已关闭，因为上次很有可能已经关闭且停止监听端口，等待下游剩下请求处理完成。 // handle by a new goroutine func (s *Server) handleConn(conn net.Conn) { reader := bufio.NewReader(conn) // ReceiveDataAsync 返回一个结构体包含两个 channel，实际读取数据是异步的 ar := protocol.ReceiveDataAsync(reader) loop: for { select { // ctx // 处理完上一个请求后 如果 ctx 已经被 cancel 了 则退出循环结束这个 connection case \u003c-s.ctx.Done(): break loop case \u003c-ar.ErrorChan: log.Println(\"handle err:\", err) break loop case rec := \u003c-ar.ReceiveChan: rsp := handleRequest(rec) reply := rsp.Encode() if len(reply) == 0 { continue } _, err := conn.Write(reply) if err != nil { log.Println(\"write err:\", err) break loop } } } _ = conn.Close() s.wg.Done() } func ReceiveDataAsync(r Reader) *AsyncReceive { var ar = \u0026AsyncReceive{ ReceiveChan: make(chan Receive, 1), ErrorChan: make(chan error, 1), } go func() { defer func() { close(ar.ReceiveChan) close(ar.ErrorChan) }() for { rec, err := DecodeFromReader(r) if err != nil { ar.ErrorChan \u003c- err if errors.Is(err, io.EOF) || errors.Is(err, net.ErrClosed) { return } log.Println(err) continue } ar.ReceiveChan \u003c- rec } }() return ar } 总结 到这里本篇内容结束了，总结一下讲述的内容： 作为一个 server 端，在定义和提供服务时需要注意哪些方面？ 在处理连接和请求时需要注意哪些问题？ 如何管理一个服务的生命周期，如从从上到下都能确保统一的启停，相互作用彼此感知？ 项目地址 ❤️ https://github.com/yusank/godis ","date":"2021-12-06","objectID":"/posts/redis-server-network/:6:2","tags":["redis","系列篇","网络"],"title":"[系列]Redis Server 实现·服务管理篇","uri":"/posts/redis-server-network/"},{"categories":["Markdown"],"content":" 这里我会记录一些 Markdown 和 Shortcut 的使用技巧以及常用的模块，方便后期写文章时查看和使用。 ","date":"2021-11-23","objectID":"/posts/blog_example/:0:0","tags":["技巧"],"title":"好用且实用的写文章技巧","uri":"/posts/blog_example/"},{"categories":["Markdown"],"content":"shortcuts ","date":"2021-11-23","objectID":"/posts/blog_example/:1:0","tags":["技巧"],"title":"好用且实用的写文章技巧","uri":"/posts/blog_example/"},{"categories":["Markdown"],"content":"admonition Note This is Note . Abstract This is Abstract . Info This is Info . Tip This is Tip . Success This is Success . Question This is Question . Warning This is Warning . Failure This is Failure . Danger This is Danger . Bug This is Bug . Example This is Example . Quote This is Quete . ","date":"2021-11-23","objectID":"/posts/blog_example/:1:1","tags":["技巧"],"title":"好用且实用的写文章技巧","uri":"/posts/blog_example/"},{"categories":["Markdown"],"content":"typeit markdown code Bug 这块目前发现是有 bug 的，不会换行，所以暂时不可用。 graph ","date":"2021-11-23","objectID":"/posts/blog_example/:1:2","tags":["技巧"],"title":"好用且实用的写文章技巧","uri":"/posts/blog_example/"},{"categories":["Redis"],"content":" 说明 本文章为该系列的协议篇，如果需要阅读其他相关文章， 请点击这里跳转查看 ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:0:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"前言 Redis 作为一个当下最流行的 NoSQL 或 KV 数据库，几乎嵌入到大部对性能、时效性高的项目内，变成了每个程序员尤其是后端程序必需了解的一个知识点。 我是无形在 GitHub 上发现一个 Godis 项目，是实现了大部分 Redis 服务器的功能。 然后瞬间激发了我的兴趣，既然用了 Redis 这些年了而且对核心逻辑和数据结构也是有所了解的，那我为什么不也写一个基于 Go 的 Redis 服务器代码，实现我能实现的核心功能逻辑。目的无外乎以下几点： 更全面且系统的了解Redis 核心逻辑并把自己的了解通过代码输出出来 提高自己的代码水平 提高自己的算法水平（涉及到实现 Redis 核心五大数据结构） 所以我也起了一个项目叫 Godis,一步步将 Redis 的逻辑通过Go 代码写出来，并且在性能上尽可能的追赶 Redis 的性能。 前期我更专注于最核心的 请求处理, 协议处理, 数据结构处理, 命令处理 等这些基础核心的模块一个个攻破，至于数据持久化，分布式部署，主从模式等这些高级特性，我在确保之前的功能都达到预期后再考虑，目前计划是数据持久化可能优先考虑。 ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:1:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"协议 想实现 Redis 服务器首先想到的问题是，如何通信？其实都知道是 tcp 连接，所以更准确的问题是，客户端和服务端用的什么协议去传输数据？答案是 RESP (REdis Serialization Protocol）。 该协议从内容来说还是比较简单的，对于服务端去解析也是相对友好且消耗性能很低。官方给出的优点为以下三点： 容易实现 解析快 可读性高 RESP 协议为一个 request-response 模型的协议，客户端发出请求并等待服务器响应，服务器按同样的协议返回数据。下面我们来看一下协议具体内容。 ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:2:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"基础说明 在 RESP 协议中，所有的数据的第一字符都是以下五种类型之一： + : 简单字符串. - : 错误. : : 整数. $ : 复杂字符串或大块字符串(bulk string). * : 数组. 数据结尾 任何一个类型的数据都是以 \"\\r\\n\"(CRLF)作为结束符. 本文中的大部分示例均来自官方文档。 ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:3:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"简单字符串(Simple Strings) 简单字符串以 + 符号开始，然后字符内容（不能包含 CR 或 LF，即不能有换行符），以 CRLF(\"\\r\\n\") 结尾。简单字符不适合传输数据(non binary safa)，在 Redis 内基本用于响应 \"OK\" 或成功标识。 简单字符串 “+OK\\r\\n” 为了安全起见，Redis 内传输数据会用 Bulk Strings . ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:4:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"错误(Errors) RESP 预留专门的错误类型。其实错误类型与简单字符串几乎没区别，只是第一个字符是 -, 除此之外在处理和编码过程均无区别，更多的区别在于客户端处理上。 Errors “-Error Message\\r\\n” Errors 类型只用于在处理请求遇到错误时，响应到客户端，比如命令不存在或命令与 key 的类型不符合等。客户端应该针对错误做一层处理，方便使用者感知到错误。 另一个 Errors 的例子 -ERR unknown command ‘foobar’ -WRONGTYPE Operation against a key holding the wrong kind of value 从 - 到第一个空格或新一行的单词代表返回的错误的类型。这个是 Redis 的一个响应习惯(convention)，将错误分类型，方便客户端更灵活的处理。 以上面的例子为例， ERR 是一个通用的错误，而 WRONGTYPE 是一个具体的错误类型，代表命令和 key 的类型不匹配,这个叫错误前缀Error Prefix。站在客户端角度来说，相对于一串错误字符串，拿到一个具体的错误类型无异于拿到一个 error code 一样，可以做一个具体的操作来消化这个错误。 当然如果分的错误类型很细 那客户端就得写的更复杂反而可能会导致得不偿失，抛开 Redis 的习惯从协议的角度来说，可以在非常简单的返回一个 false 来说明一个错误或者就一行错误内容，返回给用户，这个更多的取决于客户端实现的时候的取舍。 ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:5:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"整数(Integers) 该类型与上述两个类型也没有很大的区别，是为了专门传输整数而定的，以: 字符为开头，内容为整数且以\\r\\n 结尾，如 \":100\\r\\n\" 。 数字范围 有符号 64 位整数 Redis 中像 INCR, LLEN 和 LASTSAVE 等命令都返回整数。当然返回整数没有别的意义，只是这些命令的结果是数字。 除了命令结果是整数时返回 Integer 类型外，一些命令用整数 0 or 1 来表示 true or false,如 EXISTS, SISMEMBER。 还有一些命令是返回 1 表示操作真正执行(这么说是因为，一些操作因为数据已存在或已被操作所以不会再次操作数据而直接返回),否则返回 0 ，像 SADD, SREM, SETNX 等。 ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:6:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"复杂字符串(Bulk Strings) 复杂字符串为 RESP 中二进制安全的字符串类型，最大容量为 512MB。bulk string 编码方式如下： 以 $为开头写入实际字符串长度并以 CRLF 结尾 实际字符串 CRLF 结束 以 Hello 为例 “$5\\r\\nHello\\r\\n” 空字符 “$0\\r\\n\\r\\n” 前面说到简单字符串 不能包好换行符，然而 Bulk String是允许包含这类特殊字符的，因为读取 Bulk String 是根据其定义的长度来读取的，而不是根据换行符。 包含换行符的例子 “$4\\r\\nOK\\r\\n\\r\\n” 这是一个有效的复杂字符串其内容是 OK\\r\\n包含四个字符。 注意：$4\\r\\nOK\\r\\n\\r\\n, 下划线才是字符串内容 Bulk String 支持 Null 值（注意不是空字符）以此来区分数据不存在的情况。Null 的情况下不存在数据，所以长度为-1（-1 是协议定的，具体为什么是-1 I hava no idea .), 没有数据部分也没有数据最后的 CRLF（空字符是有的，这个需要注意的） Null 响应 “$-1\\r\\n” 官方原文： This is called a Null Bulk String. ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:7:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"数组(Arrays) 客户端的所有请求都是以数组的方式传到服务端的，而服务端的响应如果是多个值也都是以数组的方式响应。比如 ZRANGE, LRANGE, MGET 等。 数组(Arrays) 是以下方式编码的： 以 * 作为第一个字符，然后写入数组的长度，最后以 CRLF 结尾. 一组 RESP 类型的数据作为数组的元素. 空数组 “*0\\r\\n” 包含 foo 和 bar 两个 bulk string 作为元素的数组 “*2\\r\\n$3\\r\\nfoo\\r\\n$3\\r\\nbar\\r\\n” 为了阅读方便加上换行符： *2\\r\\n $3\\r\\n foo\\r\\n $3\\r\\n bar\\r\\n 不难发现，数组只需要声明长度即可，后面拼接元素就可以，元素之前无需有任何分隔符，元素可以是简单字符串, Errors, 整型,复杂字符串。下面例子更明显体现如何使用包含不用元素的数组： 混合元素例子 *5\\r\\n :1\\r\\n :2\\r\\n +SimpleString\\r\\n -Err Message\\r\\n $6\\r\\n foobar\\r\\n ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:8:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"Null数组 与复杂字符串一样，数组也支持表示 Null 值，在 BLPOP 命令中，如果操作超时，则返回一个 Null 数组表示结果： Null Array “*-1\\r\\n” 实现客户端时，应该考虑并区分开空数组和 Null 数组（如无数据或操作超时）应该有不同的处理方式。 ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:8:1","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"嵌套 数组是支持其元素也是数组的，如下面是一个包含两个数组作为元素的数组： 嵌套数组 *2\\r\\n *3\\r\\n :1\\r\\n :2\\r\\n :3\\r\\n *2\\r\\n +Foo\\r\\n -Bar\\r\\n 把 RESP 协议转换成可读性更高的数据后： 解码后 [[1, 2, 3], [\"Foo\", Error(\"Bar\")]] ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:8:2","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"包含 Null 元素的数组 在与 Redis 交互时, 一部分命令是操作多个 key，返回多个值的，这个时候就有个问题，其中一些操作不成功或数据不存在， 该不该影响这次请求的整体结果呢？ 以 MGET 这个命令为例，如果获取多个 key 的值，而其中有一部分 key 是不存在时，Redis 在响应中留 null 值从而不影响其他 key 的读取，协议如下： 包含 Null 元素 *3\\r\\n $3\\r\\n foo\\r\\n $-1\\r\\n $3\\r\\n bar\\r\\n 客户端针对该响应解析出来的结果应该如下： 解码后 [\"foo\", nil, \"bar\"] ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:8:3","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"客户端服务端交互 到目前为止，RESP 协议的内容全部说完了，发现其实蛮简单的。其实现在去实现一个客户端的代码比实现服务端的简单很多，因为只需要编码解码协议内容即可。 服务端和客户单交互最核心就是以下两点： 客户端向服务端发起请求均为以 Bulk Strings 作为元素的数组. 服务端向客户端响应任意合法的 RESP 数据类型. 以 LLEN mylist 为例： 一次交互过程 C: *2\\r\\n C: $4\\r\\n C: LLEN\\r\\n C: $6\\r\\n C: mylist\\r\\n S: :48293\\r\\n C: client， S:server ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:9:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"高效解析协议 该协议可读性相对高，与此同时解析效率也很高，下面给出如何解析该协议. import ( \"bytes\" \"log\" \"io\" ) func main() { var p = []byte(\"$5\\r\\nHello\\r\\n\") buf := bytes.NewBuffer(p) // read first line b, err := buf.ReadBytes('\\n') if err != nil { panic(err) } // read length ln := readBulkOrArrayLength(b) log.Println(\"string len: \",ln) // string len: 5 // read value value,err := readBulkStrings(buf, ln) if err != nil { panic(err) } log.Println(\"value: \", string(value)) // value: Hello } // 解析长度 func readBulkOrArrayLength(line []byte) int { var ( ln int ) for i := 1; line[i] != '\\r'; i++ { ln = (ln * 10) + int(line[i]-'0') } return ln } // 读取内容 func readBulkStrings(r io.Reader, ln int) (val []byte, err error) { if ln \u003c= 0 { return } // 读取时 将 \\r\\n 也读出来，保证 offset 在新的一行第一个字符 val = make([]byte, ln+2) _, err = r.Read(val) // trim last \\r\\n val = val[:ln] return } ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:10:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":"参考文献 https://redis.io/topics/protocol ","date":"2021-11-23","objectID":"/posts/redis-server-protocol/:11:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·协议篇","uri":"/posts/redis-server-protocol/"},{"categories":["Redis"],"content":" 本文系列篇Redis Server 实现的开篇文章，将列出本系列篇讲述的内容和实现的功能做简单的描述。 ","date":"2021-11-22","objectID":"/posts/redeis-server-introduction/:0:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·前言","uri":"/posts/redeis-server-introduction/"},{"categories":["Redis"],"content":"核心内容 系列篇将会分步骤讲述如何通过 Go 语言实现一个 Redis Server 并能达到可使用的地步的过程，到目前为止的想法是分 以下几个步骤实现并写出对应的总结文章： 协议篇·Redis 协议的介绍和如何解析 网络篇·如何提供一个高性能的 TCP 服务 数据结构篇·如何实现 Redis 五大数据结构（这个可能根据篇幅大小分几篇文章） 代码结构篇·如何实现和优化从请求进来到数据处理和响应过程 测试篇·如何做一个性能测试和测试结果对比 完结篇·总结开发和设计过程中遇到的问题和新的、 除以上的计划写的文章外 可能会写一些开发过程中遇到的问题结果过程或一些心得也会不定时更新 ","date":"2021-11-22","objectID":"/posts/redeis-server-introduction/:1:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·前言","uri":"/posts/redeis-server-introduction/"},{"categories":["Redis"],"content":"文章传送门 协议篇 服务管理篇 网络篇(网络模型，未发布) 数据结构·链表篇 数据结构·跳表篇 数据结构·哈希表篇 项目经验总结 待补充 ","date":"2021-11-22","objectID":"/posts/redeis-server-introduction/:2:0","tags":["redis","系列篇"],"title":"[系列]Redis Server 实现·前言","uri":"/posts/redeis-server-introduction/"},{"categories":["网关"],"content":" 分享如何在 docker 环境部署 apisix 和如何开发 lua 和 go 语言的插件以及如何使用这些自定义插件的过程，希望能帮助到你。 ","date":"2021-11-03","objectID":"/posts/apisix_plugins/:0:0","tags":["lua","go","apisix"],"title":"apisix 开发自定义插件","uri":"/posts/apisix_plugins/"},{"categories":["网关"],"content":"如何部署 # 1. 下载官方 docker compose 项目 $ git clone https://github.com/apache/apisix-docker.git $ cd apisix-docker/example # 2. run docker compose $ docker-compose -p docker-apisix up -d # check docker ps $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 629eb9d85656 627d00c649fc \"sh -c '/usr/bin/api…\" 24 seconds ago Up 20 seconds 0.0.0.0:9080-\u003e9080/tcp, :::9080-\u003e9080/tcp, 0.0.0.0:9091-9092-\u003e9091-9092/tcp, :::9091-9092-\u003e9091-9092/tcp, 0.0.0.0:9443-\u003e9443/tcp, :::9443-\u003e9443/tcp docker-apisix_apisix_1 c3cbca636e65 13afb861111c \"/run.sh\" 24 seconds ago Up 22 seconds 0.0.0.0:3000-\u003e3000/tcp, :::3000-\u003e3000/tcp docker-apisix_grafana_1 4a5cb9ad6239 5b0292a5e821 \"/usr/local/apisix-d…\" 24 seconds ago Up 22 seconds 0.0.0.0:9000-\u003e9000/tcp, :::9000-\u003e9000/tcp docker-apisix_apisix-dashboard_1 6430826c4095 8c7e00e786b8 \"/opt/bitnami/script…\" 24 seconds ago Up 21 seconds 0.0.0.0:2379-\u003e2379/tcp, :::2379-\u003e2379/tcp, 2380/tcp docker-apisix_etcd_1 c086d6e4fbd9 a618f5685492 \"/bin/prometheus --c…\" 24 seconds ago Up 22 seconds 0.0.0.0:9090-\u003e9090/tcp, :::9090-\u003e9090/tcp docker-apisix_prometheus_1 1e6ea10c008f 7d0cdcc60a96 \"/docker-entrypoint.…\" 24 seconds ago Up 21 seconds 0.0.0.0:9082-\u003e80/tcp, :::9082-\u003e80/tcp docker-apisix_web2_1 d4891bd0744e 7d0cdcc60a96 \"/docker-entrypoint.…\" 24 seconds ago Up 22 seconds 0.0.0.0:9081-\u003e80/tcp, :::9081-\u003e80/tcp docker-apisix_web1_1 部署完成，可以通过 localhost:9000 访问 dashboard。 ","date":"2021-11-03","objectID":"/posts/apisix_plugins/:1:0","tags":["lua","go","apisix"],"title":"apisix 开发自定义插件","uri":"/posts/apisix_plugins/"},{"categories":["网关"],"content":"配置 默认配置文件在 apisix-docker/example/apisix_conf 目录下。 apisix: node_listen: 9080 # APISIX listening port enable_ipv6: false allow_admin: # http://nginx.org/en/docs/http/ngx_http_access_module.html#allow - 0.0.0.0/0 # We need to restrict ip access rules for security. 0.0.0.0/0 is for test. admin_key: - name: \"admin\" key: edd1c9f034335f136f87ad84b625c8f1 # 这个值需要改的否则有安全隐患 role: admin # admin: manage all configuration data # viewer: only can view configuration data - name: \"viewer\" key: 4054f7cf07e344346cd3f287985e76a2 role: viewer enable_control: true control: ip: \"0.0.0.0\" port: 9092 etcd: host: # it's possible to define multiple etcd hosts addresses of the same etcd cluster. - \"http://etcd:2379\" # multiple etcd address prefix: \"/apisix\" # apisix configurations prefix timeout: 30 # 30 seconds plugin_attr: prometheus: export_addr: ip: \"0.0.0.0\" port: 9091 ","date":"2021-11-03","objectID":"/posts/apisix_plugins/:2:0","tags":["lua","go","apisix"],"title":"apisix 开发自定义插件","uri":"/posts/apisix_plugins/"},{"categories":["网关"],"content":"插件 ","date":"2021-11-03","objectID":"/posts/apisix_plugins/:3:0","tags":["lua","go","apisix"],"title":"apisix 开发自定义插件","uri":"/posts/apisix_plugins/"},{"categories":["网关"],"content":"lua 官方开发教程 示例插件： local ngx = ngx local core = require(\"apisix.core\") local plugin = require(\"apisix.plugin\") local upstream = require(\"apisix.upstream\") -- 定义配置，即使用插件时 配置一些自定义字段，如鉴权的 key，需要校验的 header 之类的 local schema = { type = \"object\", properties = { value = {type = \"array\", minItems = 1} }, required = {\"value\"} } -- 这个需要了解一下干什么的 local metadata_schema = { type = \"object\", properties = { ikey = {type = \"number\", minimum = 0}, skey = {type = \"string\"} }, required = {\"ikey\", \"skey\"} } local plugin_name = \"block_by_lua\" local _M = { version = 0.1, priority = 0, name = plugin_name, schema = schema, metadata_schema = metadata_schema } function _M.check_schema(conf, schema_type) if schema_type == core.schema.TYPE_METADATA then return core.schema.check(metadata_schema, conf) end return core.schema.check(schema, conf) end function _M.init() -- call this function when plugin is loaded core.log.info(plugin_name, \"loaded!\") end function _M.destroy() -- call this function when plugin is unloaded end -- uri 重写阶段 如果不需要就不用定义这个方法 --[[ function _M.rewrite(conf, ctx) core.log.warn(\"plugin rewrite phase, conf: \", core.json.encode(conf)) core.log.warn(\"conf_type: \", ctx.conf_type) core.log.warn(\"conf_id: \", ctx.conf_id) core.log.warn(\"conf_version: \", ctx.conf_version) end --]] --命中服务 \u0026 调服务前 function _M.access(conf, ctx) core.log.warn(\"plugin access phase, conf: \", core.json.encode(conf)) core.log.warn(\"plugin access phase, ctx: \", core.json.encode(ctx, true)) -- return 200, {message = \"hit example plugin\"} -- 1. extract from header local pass = core.request.header(ctx, \"X-Block-Pass\") if not pass then core.response.set_header(\"X-Block-Flag\", \"Block By Lua Ext\") -- 返回 http 状态码 则这次请求截止到当前插件，不会往下走 return 403, {message = \"Missing pass value in header\"} end for _, val in pairs(conf.value) do if val == pass then -- return 空表示通过 return end end core.response.set_header(\"X-Block-Flag\", \"Block By Lua Ext\") return 403, {message = \"Invalid pass value in header.\"} end local function hello() local args = ngx.req.get_uri_args() if args[\"json\"] then return 200, {msg = \"world\"} else return 200, \"world\\n\" end end function _M.control_api() return { { -- 注册 controller api 用于探测插件是否插入成功，也可以用于内部一些返回 token 之类的用处 methods = {\"GET\"}, uris = {\"/v1/plugin/example-plugin/hello\"}, handler = hello } } end return _M 如何安装： 创建目录 ├── example │ └── apisix │ ├── plugins │ │ └── 3rd-party.lua │ └── stream │ └── plugins │ └── 3rd-party.lua 配置文件(config.yaml)添加插件目录 apisix: ... extra_lua_path: \"/path/to/example/?.lua\" 开启插件(config.yaml) apisix: ... plugins: # 从 config-default.yaml 文件复制出来，然后加上自己的插件 ... - your-plugin Q:如何在dashboard 看到自己的插件？ A: 目前自定义插件不支持自动同步到 dashboard，需要手动添加，步骤如下： 在 apisix 机器上执行如下命令获取最新 json scheme： $ curl 127.0.0.1:9092/v1/schema \u003e scheme.json 2. 将 `scheme.json` 复制到 dashboard 机器上 `conf` 目录下与原有的文件替换，重启 dashboard 服务。 ","date":"2021-11-03","objectID":"/posts/apisix_plugins/:3:1","tags":["lua","go","apisix"],"title":"apisix 开发自定义插件","uri":"/posts/apisix_plugins/"},{"categories":["网关"],"content":"go 官方开发教程 示例代码： package plugins import ( \"encoding/json\" \"net/http\" pkgHTTP \"github.com/apache/apisix-go-plugin-runner/pkg/http\" \"github.com/apache/apisix-go-plugin-runner/pkg/log\" \"github.com/apache/apisix-go-plugin-runner/pkg/plugin\" ) // 初始化 func init() { // 注册插件 err := plugin.RegisterPlugin(\u0026BlockReq{}) if err != nil { log.Fatalf(\"failed to register plugin block-req: %s\", err) } } // LimitReq is a demo for a real world plugin type BlockReq struct { } // 与 lua 插件内 scheme 一样 type BlockReqConf struct { Key string `json:\"key\"` Value []string `json:\"value\"` } func (p *BlockReq) Name() string { return \"blcok-req\" } // ParseConf is called when the configuration is changed. And its output is unique per route. func (p *BlockReq) ParseConf(in []byte) (interface{}, error) { conf := BlockReqConf{} err := json.Unmarshal(in, \u0026conf) return conf, err } // Filter is called when a request hits the route func (p *BlockReq) Filter(conf interface{}, w http.ResponseWriter, r pkgHTTP.Request) { b := conf.(BlockReqConf) val := r.Header().Get(b.Key) for _, v := range b.Value { if val == v { r.Header().Set(\"X-Block-Value\", v) return } } // block request // 只要写 response 的 header 或body，请求将停在这里不会往下传递，直接响应回去 w.Header().Add(\"X-Block-Req\", \"Block by Go ext.\") w.WriteHeader(http.StatusForbidden) } 编译部署 用官方提供的 Makefile 进行 build（注意编译环境和apisix 运行的环境，指定对应的 GOOS，GOARCH） 将编译好的二进制文件打包到 apisix 的容器内 修改配置文件 ext-plugin: cmd: [\"/path/to/apisix-go-plugin-runner/go-runner\", \"run\"] 注意：一个 go-runner 内可以注册多个插件，所以不需要拥有多个 go-runner ,所有的插件在一个项目里 然后统一编译部署即可 使用 非 lua 插件都运行在各自的 runner 内，所以使用的时候不能直接在 dashboard 中使用自定义的插件（lua 的自定义插件是可以的），需要在 ext-plugin-pre-req, ext-plugin-post-req 两个插件内配置使用，这两插件只有运行时间不一样，一个在所有插件之前 一个在所有插件之后。使用时配置如下: \"plugins\": { \"ext-plugin-pre-req\": { \"conf\": [ { \"name\": \"blcok-req\", // 注册的 go 插件名字 \"value\": \"{\\\"key\\\":\\\"pass\\\", \\\"value\\\":[\\\"word\\\",\\\"port\\\"]}\" // 该插件的 conf，这里需要将 json 进行转义 } ], \"disable\": false } } ","date":"2021-11-03","objectID":"/posts/apisix_plugins/:3:2","tags":["lua","go","apisix"],"title":"apisix 开发自定义插件","uri":"/posts/apisix_plugins/"},{"categories":["网关"],"content":"wasm apisix 开始支持 wasm 插件，但是官方给出的示例和文档还不够完善，这块还在研究中，之后会补齐。 ","date":"2021-11-03","objectID":"/posts/apisix_plugins/:3:3","tags":["lua","go","apisix"],"title":"apisix 开发自定义插件","uri":"/posts/apisix_plugins/"},{"categories":["网关"],"content":" 本文简单介绍如何通过 lua 脚本和 ngx_shared_dict 在 nginx 中动态加载后端服务配置以及动态更新服务配置. ","date":"2021-09-17","objectID":"/posts/nginx-lua-plugins/:0:0","tags":["nginx","lua"],"title":"nginx 中使用 lua 动态加载服务配置","uri":"/posts/nginx-lua-plugins/"},{"categories":["网关"],"content":"nginx ","date":"2021-09-17","objectID":"/posts/nginx-lua-plugins/:1:0","tags":["nginx","lua"],"title":"nginx 中使用 lua 动态加载服务配置","uri":"/posts/nginx-lua-plugins/"},{"categories":["网关"],"content":"加载 lua 脚本 在 Nginx 中需要引入和加载 lua 脚本，从而在路由转发时运行 lua 脚本进行我们的逻辑。初始化代码如下： http { lua_shared_dict endpoints_data 5m; #定义upstream共享内存空间 lua_shared_dict cache 1m; #定义计数共享空间 access_log nginx_access.log; lua_package_path \"/etc/nginx/lua/?.lua;;\"; init_by_lua_block { collectgarbage(\"collect\") local ok, res # 加载脚本 configuration.lua ok, res = pcall(require, \"configuration\") if not ok then error(\"require failed: \" .. tostring(res)) else configuration = res end } # 执行脚本内初始化方法，这里为可选项，如果没有可初始化的代码部分 这里可以不要 init_worker_by_lua_block { configuration.prepare() } } ","date":"2021-09-17","objectID":"/posts/nginx-lua-plugins/:1:1","tags":["nginx","lua"],"title":"nginx 中使用 lua 动态加载服务配置","uri":"/posts/nginx-lua-plugins/"},{"categories":["网关"],"content":"执行 lua 脚本 如何在 Nginx 配置中执行 lua 脚本，从而实现一些特殊逻辑？这里给出一个简单的示例: server { # 执行最简单的 lua 脚本 location /hello { default_type 'text/plain'; content_by_lua 'ngx.say(\"hello, lua\")'; } # 配置接口 # 这里是执行加载的 lua 脚本中方法 location /configuration { client_max_body_size 5m; client_body_buffer_size 1m; proxy_buffering off; content_by_lua_block { configuration.call() # 调用 call() 方法 } } # 执行较为复杂的 lua 逻辑 location /lua { default_type 'text/plain'; # 读取请求中的 path 参数 并从共享 dict 中查询这个值， # 返回查询到的结果 content_by_lua ' local path = ngx.req.get_uri_args()[\"path\"] if path == nil then ngx.say(\"path not found\") return end local data = ngx.shared.endpoints_data:get(\"/\"..path) if not data then ngx.say(\"unkonw path\") return end ngx.say(\"paths: \"..data) '; } } lua 的语法相对简单好上手，实现一些简单的逻辑也很方便，非常值得学习。 ","date":"2021-09-17","objectID":"/posts/nginx-lua-plugins/:1:2","tags":["nginx","lua"],"title":"nginx 中使用 lua 动态加载服务配置","uri":"/posts/nginx-lua-plugins/"},{"categories":["网关"],"content":"完整配置 先给出 Nginx 的完整配置，里面包括动态配置后端服务列表和动态加载服务转发的逻辑，然后再给出 lua 部分详细实现的代码。 user nginx; worker_processes 1; pid /var/run/nginx.pid; error_log nginx_error.log; events { worker_connections 1024; } http { lua_shared_dict endpoints_data 5m; #定义upstream共享内存空间 lua_shared_dict cache 1m; #定义计数共享空间 access_log nginx_access.log; lua_package_path \"/etc/nginx/lua/?.lua;;\"; init_by_lua_block { collectgarbage(\"collect\") local ok, res ok, res = pcall(require, \"configuration\") if not ok then error(\"require failed: \" .. tostring(res)) else configuration = res end } # 执行脚本内初始化方法，这里为可选项，如果没有可初始化的代码部分 这里可以不要 init_worker_by_lua_block { configuration.prepare() } include /etc/nginx/mime.types; default_type application/octet-stream; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; server { # 执行最简单的 lua 脚本 location /hello { default_type 'text/plain'; content_by_lua 'ngx.say(\"hello, lua\")'; } # 配置接口 # 这里是执行加载的 lua 脚本中方法 location /configuration { client_max_body_size 5m; client_body_buffer_size 1m; proxy_buffering off; content_by_lua_block { configuration.call() # 调用 call() 方法 } } # 执行较为复杂的 lua 逻辑 location /lua { default_type 'text/plain'; # 读取请求中的 path 参数 并从共享 dict 中查询这个值， # 返回查询到的结果 content_by_lua ' local path = ngx.req.get_uri_args()[\"path\"] if path == nil then ngx.say(\"path not found\") return end local data = ngx.shared.endpoints_data:get(\"/\"..path) if not data then ngx.say(\"unkonw path\") return end ngx.say(\"paths: \"..data) '; } # other path location / { set $load_ups \"\"; # 动态设置当前 upstream, 未找到返回404 rewrite_by_lua ' local ups = configuration.getEndpoints() if ups ~= nil then ngx.log(ngx.ERR,\"got upstream\", ups) ngx.var.load_ups = ups return end ngx.status = ngx.HTTP_NOT_FOUND ngx.exit(ngx.status) '; proxy_pass http://$load_ups$uri; add_header X-Upstream $upstream_addr always; # 添加 backend ip } } } ","date":"2021-09-17","objectID":"/posts/nginx-lua-plugins/:1:3","tags":["nginx","lua"],"title":"nginx 中使用 lua 动态加载服务配置","uri":"/posts/nginx-lua-plugins/"},{"categories":["网关"],"content":"lua ","date":"2021-09-17","objectID":"/posts/nginx-lua-plugins/:2:0","tags":["nginx","lua"],"title":"nginx 中使用 lua 动态加载服务配置","uri":"/posts/nginx-lua-plugins/"},{"categories":["网关"],"content":"定义变量 因为需要用到 shared_dict 特性，在 lua 和 Nginx 之间公用内存块 从而实现数据的同步共享，所以需要预定义一些变量。 -- 引入变量 local io = io local ngx = ngx local table = table -- 当前包的对象，类似 go 语言的定义结构体 让给这个结构体实现方法 local _M = {} -- 与 Nginx 共享的空间 可读写 local Endpoints = ngx.shared.endpoints_data ","date":"2021-09-17","objectID":"/posts/nginx-lua-plugins/:2:1","tags":["nginx","lua"],"title":"nginx 中使用 lua 动态加载服务配置","uri":"/posts/nginx-lua-plugins/"},{"categories":["网关"],"content":"动态更新服务列表 服务列表是通过被调接口实现，即有别的服务区监听服务节点(endpoint)的变化,然后调用/configuration/backends 接口，被 Nginx 配置的 /configuration 规则命中后调用 configuration.call() 方法，我们看一下这个 call 方法的实现。 -- call called by ngx function _M.call() -- 只处理 GET 和 POST if ngx.var.request_method ~= \"POST\" and ngx.var.request_method ~= \"GET\" then ngx.status = ngx.HTTP_BAD_REQUEST ngx.print(\"Only POST and GET requests are allowed!\") return end -- 目前只处理后端服务的配置 所以判断路由 if ngx.var.request_uri == \"/configuration/backends\" then -- 调用内部方法 handle_backends() return end -- 非法请求 返回 404 ngx.status = ngx.HTTP_NOT_FOUND ngx.print(\"Not found!\") end 多说一句，调用 /configuration/backends 时传参是在请求 body 里，格式为 json 所以需要引入第三方的 json 解析包。handle_backends 方法的实现： -- handle_backends . local function handle_backends() if ngx.var.request_method == \"GET\" then ngx.status = ngx.HTTP_OK -- 返回查询的服务列表 local path = ngx.req.get_uri_args()[\"path\"] ngx.print(Endpoints:get(\"path\")) return end -- 读取请求 body local obj = fetch_request_body() if not obj then ngx.log(ngx.ERR, \"dynamic-configuration: unable to read valid request body\") ngx.status = ngx.HTTP_BAD_REQUEST return end -- 通过 第三方包 json 解析 body到 lua table local rule, err = json.decode(obj) if not rule then ngx.log(ngx.ERR, \"could not parse backends data: \", err) return end ngx.log(ngx.ERR, \"decoed rule\", obj) -- 清空共享空间 Endpoints:flush_all() -- 遍历并写入 for _, new_rule in ipairs(rule.rules) do -- 更新 -- 将数组合并 local succ, err1, forcible = Endpoints:set(new_rule.path, table.concat(new_rule.upstreams, \",\")) ngx.log(ngx.ERR, \"set result\", succ, err1,forcible) end ngx.status = ngx.HTTP_CREATED ngx.say(\"ok\") end -- 读取请求 body 部分 local function fetch_request_body() ngx.req.read_body() local body = ngx.req.get_body_data() if not body then -- request body might've been written to tmp file if body \u003e client_body_buffer_size local file_name = ngx.req.get_body_file() local file = io.open(file_name, \"rb\") if not file then return nil end body = file:read(\"*all\") file:close() end return body end 请求 body 的 json 结构如下： type NginxRuleConf struct { Rules []struct{ Path string `json:\"path\"` ServiceName string `json:\"serviceName\"` Port int32 `json:\"-\"` Upstreams []string `json:\"upstreams\"` } `json:\"rules\"` } ","date":"2021-09-17","objectID":"/posts/nginx-lua-plugins/:2:2","tags":["nginx","lua"],"title":"nginx 中使用 lua 动态加载服务配置","uri":"/posts/nginx-lua-plugins/"},{"categories":["网关"],"content":"动态读取后端服务 上面已经通过接口的方式动态更新服务节点列表并写入到共享空间 endpoints_data 内，我们现在实现读取服务列表并选择其中一个节点进行接口转发。 代码如下： -- 轮顺的方式取节点 function _M.getEndpoints() local cache = ngx.shared.cache local path = ngx.var.request_uri local eps = Endpoints:get(path) if not eps then return nil end local tab = split(eps,\",\") local index = cache:get(path) if index == nil or index \u003e #tab then index = 1 end -- 加一 cache:set(path,index+1) return tab[index] end ","date":"2021-09-17","objectID":"/posts/nginx-lua-plugins/:2:3","tags":["nginx","lua"],"title":"nginx 中使用 lua 动态加载服务配置","uri":"/posts/nginx-lua-plugins/"},{"categories":["网关"],"content":"结论 至此实现的效果是，可以动态配置多个后端服务和后端服务节点列表，外部服务请求 Nginx 时，会尝试从已有的服务中匹配转发，如果服务有多个节点则轮顺的方法去转发。如有服务信息发生变化，则通过调用 Nginx 中配置的 configuration 接口更新即可，无需修改 Nginx 配置。 ","date":"2021-09-17","objectID":"/posts/nginx-lua-plugins/:3:0","tags":["nginx","lua"],"title":"nginx 中使用 lua 动态加载服务配置","uri":"/posts/nginx-lua-plugins/"},{"categories":["kubernetes"],"content":"本文介绍本地或服务器上搭建单节点的 k8s 集群和 webUI 以及启用ingress，可以用作开发和测试环境。 ","date":"2021-09-07","objectID":"/posts/deploy-k8s-cluster/:0:0","tags":["k8s","docker"],"title":"部署单机 k8s 集群","uri":"/posts/deploy-k8s-cluster/"},{"categories":["kubernetes"],"content":"准备工作 所需工具： docker minkube kubectl 如何安装 docker 就不再这里撰述。 ","date":"2021-09-07","objectID":"/posts/deploy-k8s-cluster/:1:0","tags":["k8s","docker"],"title":"部署单机 k8s 集群","uri":"/posts/deploy-k8s-cluster/"},{"categories":["kubernetes"],"content":"安装 minikube 官方文档 Mac $ brew install minkube linux $ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \\ \u0026\u0026 chmod +x minikube 将 Minikube 可执行文件添加至 PATH： sudo mkdir -p /usr/local/bin/ sudo install minikube /usr/local/bin/ 也可以在 GitHub 上下载系统对应的二级制文件 ","date":"2021-09-07","objectID":"/posts/deploy-k8s-cluster/:1:1","tags":["k8s","docker"],"title":"部署单机 k8s 集群","uri":"/posts/deploy-k8s-cluster/"},{"categories":["kubernetes"],"content":"安装 kubectl 官方文档 Mac $ brew install kubernetes-cli Linux $ sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - $ echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list $ sudo apt-get update $ sudo apt-get install -y kubectl ","date":"2021-09-07","objectID":"/posts/deploy-k8s-cluster/:1:2","tags":["k8s","docker"],"title":"部署单机 k8s 集群","uri":"/posts/deploy-k8s-cluster/"},{"categories":["kubernetes"],"content":"启动\u0026检查 启动 $ minikube start --vm-driver=docker 检查 $ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured 至此集群已经部署成功，可以通过 kubectl 命令查看状态 $ kubectl cluster-info Kubernetes control plane is running at https://xxx.xxx.xx.xx:8443 CoreDNS is running at https://xxx.xxx.xx.xx:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy ","date":"2021-09-07","objectID":"/posts/deploy-k8s-cluster/:2:0","tags":["k8s","docker"],"title":"部署单机 k8s 集群","uri":"/posts/deploy-k8s-cluster/"},{"categories":["kubernetes"],"content":"停止\u0026清理 停止 $ minkube stop 清理 $ minikube delete ","date":"2021-09-07","objectID":"/posts/deploy-k8s-cluster/:3:0","tags":["k8s","docker"],"title":"部署单机 k8s 集群","uri":"/posts/deploy-k8s-cluster/"},{"categories":["kubernetes"],"content":"webUI 安装 k8s 管理 dashboard。 $ minikube dashboard --url 🤔 Verifying dashboard health ... 🚀 Launching proxy ... 🤔 Verifying proxy health ... http://127.0.0.1:35983/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ minikube 会安装 dashboard 并返回可访问的 url, 如果是本地则直接访问即可。 如果是服务器上，则需要执行以下命令： $ kubectl proxy --address='0.0.0.0' --disable-filter=true W0907 17:47:12.246841 591818 proxy.go:162] Request filter disabled, your proxy is vulnerable to XSRF attacks, please be cautious Starting to serve on [::]:8001 并通过http://serverIP:8001/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/访问 dashboard 。 ","date":"2021-09-07","objectID":"/posts/deploy-k8s-cluster/:4:0","tags":["k8s","docker"],"title":"部署单机 k8s 集群","uri":"/posts/deploy-k8s-cluster/"},{"categories":["kubernetes"],"content":"Ingress 启动 ingress 也是需要通过 minikube 命令执行。 $ minikube addons enable ingress minikube 会开启 ingress 并安装 ingress-nginx, 我们只需要写 ingress 规则即可。然后通过 kubectl 命令查看可访问的虚拟 ip。 $ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE goapp-ingress \u003cnone\u003e * 192.168.49.2 80 2d $ curl 192.168.49.2/ping \"pong\" 可以访问的通的。 相关连接: https://v1-18.docs.kubernetes.io/zh/docs/tasks/tools/install-minikube/ https://kubernetes.io/zh/docs/tasks/access-application-cluster/ingress-minikube/ https://stackoverflow.com/questions/47173463/how-to-access-local-kubernetes-minikube-dashboard-remotely ","date":"2021-09-07","objectID":"/posts/deploy-k8s-cluster/:5:0","tags":["k8s","docker"],"title":"部署单机 k8s 集群","uri":"/posts/deploy-k8s-cluster/"},{"categories":["Docker"],"content":"dlv 作为程序调试工具功能非常强大，日常开发和测试中几乎离不开 debug 调试。但是有的时候由于本地环境与线上环境不一致或有些问题在本地无法复现的时候，我们需要在线上/测试环境做 debug，同时希望 debug 体验能与本地 debug 体验一致。dlv 其实是支持这种需求的，线上运行线本地 debug。以下是基于 docker 环境的远程调试步骤，希望能对遇到这种情况的码友们友帮助。 所需工具： docker goland dlv ","date":"2021-09-06","objectID":"/posts/docker-dlv-debugging/:0:0","tags":["go","docker"],"title":"如何在 docker 环境下进行远程 dlv 调试","uri":"/posts/docker-dlv-debugging/"},{"categories":["Docker"],"content":"docker file # Compile stage FROM golang:1.13.8 AS build-env # Build Delve RUN go get github.com/go-delve/delve/cmd/dlv ADD . /dockerdev WORKDIR /dockerdev # 编译需要 debug 的程序 RUN go build -gcflags=\"all=-N -l\" -o /server # Final stage FROM debian:buster # 分别暴露 server 和 dlv 端口 EXPOSE 8000 40000 WORKDIR / COPY --from=build-env /go/bin/dlv / COPY --from=build-env /server / CMD [\"/dlv\", \"--listen=:40000\", \"--headless=true\", \"--api-version=2\", \"--accept-multiclient\", \"exec\", \"/server\"] ","date":"2021-09-06","objectID":"/posts/docker-dlv-debugging/:1:0","tags":["go","docker"],"title":"如何在 docker 环境下进行远程 dlv 调试","uri":"/posts/docker-dlv-debugging/"},{"categories":["Docker"],"content":"启动 docker 镜像 $ docker run -d -p 8000:8000 -p 40000:40000 --privileged --name=dlv-debug $(ImageName):$(ImageVersion) ","date":"2021-09-06","objectID":"/posts/docker-dlv-debugging/:2:0","tags":["go","docker"],"title":"如何在 docker 环境下进行远程 dlv 调试","uri":"/posts/docker-dlv-debugging/"},{"categories":["Docker"],"content":"goland 配置 在 Goland -\u003e Run -\u003e Edit Configuration 添加 Go Remote 配置 docker 镜像的 ip:port, 本地的 docker 环境则 localhost:40000 即可。 现在就可以在本地 Goland 环境下启动配置 debug 就可以，本地 debug 远程程序，与本地 debug 毫无区别。 这种方式在一些特定环境（test 环境、远程办公等）非常方便。 参考文献： https://blog.jetbrains.com/go/2020/05/06/debugging-a-go-application-inside-a-docker-container/ ","date":"2021-09-06","objectID":"/posts/docker-dlv-debugging/:3:0","tags":["go","docker"],"title":"如何在 docker 环境下进行远程 dlv 调试","uri":"/posts/docker-dlv-debugging/"},{"categories":["proto"],"content":"前言 如果大家接触过 grpc 和 protobuf ，那对 protoc 这个命令应该不陌生。 protoc 为基于 proto buffer 文件生成不同语言代码的工具，在日常业务开发中能经常用到。那先抛出一个问题，你有没有基于 pb 文件生成满足自己特殊要求的需求？比如生成对应的 http 代码或校验参数等。 我个人需求为，除了生成正常的 grpc 代码外，需要生成一套对应的 http 代码，而且最好是能直接在 gin/iris 这种主流 web 框架内注册使用。 其实 golang/protobuf 包支持自定义插件的，而且还提供很多好用的方法，方便我们读写 pb 文件。我们写好自己的插件安装到 $GOPATH/bin 下，然后在调用 protoc 命令时，指定我们自己的插件名和输出位置即可。 关于这个插件：我现有的需求然后一直找不到比较好的解决方案，直到看到 kratos 项目的 http 代码生成插件后豁然开朗，基于 kratos 的逻辑实现的自己需求，感谢 kratos 作者们。 ","date":"2021-07-08","objectID":"/posts/go-protoc-http/:1:0","tags":["go","grpc","protoc"],"title":"如何自定义 protoc 插件","uri":"/posts/go-protoc-http/"},{"categories":["proto"],"content":"效果 先看原始 pb 文件。 test.proto syntax = \"proto3\"; package hello.service.v1; option go_package = \"api/hello/service/v1;v1\"; // 下载 `github.com/googleapis/googleapis` 至`GOPATH`, 生成 http 代码需要。 import \"google/api/annotations.proto\"; service Hello { rpc Add(AddRequest) returns (AddResponse) { option (google.api.http) = { post: \"/api/hello/service/v1/add\" body: \"*\" }; } rpc Get(GetRequest) returns (GetResponse) { option (google.api.http) = { get: \"/api/hello/service/v1/get\" }; } } message AddRequest { uint32 id = 1; string name = 2; } message AddResponse { uint32 id = 1; string name = 2; } message GetRequest { uint32 id = 1; } message GetResponse { uint32 id = 1; string name = 2; float score = 3; bytes bs = 4; map\u003cstring, string\u003e m = 5; } 因为我需要生成 http 代码，所以定义 rpc 时，http 路由和method 需要在 pb 文件指定。 我实现的插件起码叫 protoc-gen-go-http, 必须以 protoc-gen 开头否则 protoc 不认。 执行命令： # --go-http 为我自己的插件 # 其中参数是 key=v,key2=v2 方式传，最后冒号后面写输出目录 protoc -I$GOPATH/src/github.com/googleapis/googleapis --proto_path=$GOPATH/src:. --go_out=. --go-http_out=router=gin:. --micro_out=. test.proto 执行完命令后，会生成三个文件分别为 test.pb.go,test.pb.micro.go和test.http.pb.go， 生成的文件名是可以自定义的。 test.pb.micro.go 文件是由 go-micro 提供的工具生成 grpc 代码文件。 看一下 test.http.pb.go 文件 // Code generated by protoc-gen-go-http. DO NOT EDIT. // versions: // protoc-gen-go-http v0.0.9 package v1 import ( context \"context\" gin \"github.com/gin-gonic/gin\" ) // This is a compile-time assertion to ensure that this generated file // is compatible with the galaxy package it is being compiled against. var _ context.Context const _ = gin.Version type HelloHTTPHandler interface { Add(context.Context, *AddRequest, *AddResponse) error Get(context.Context, *GetRequest, *GetResponse) error } // RegisterHelloHTTPHandler define http router handle by gin. func RegisterHelloHTTPHandler(g *gin.RouterGroup, srv HelloHTTPHandler) { g.POST(\"/api/hello/service/v1/add\", _Hello_Add0_HTTP_Handler(srv)) g.GET(\"/api/hello/service/v1/get\", _Hello_Get0_HTTP_Handler(srv)) } func _Hello_Add0_HTTP_Handler(srv HelloHTTPHandler) func(c *gin.Context) { return func(c *gin.Context) { var ( in AddRequest out AddResponse ) if err := c.ShouldBind(\u0026in); err != nil { c.AbortWithStatusJSON(400, gin.H{\"err\": err.Error()}) return } err := srv.Add(context.Background(), \u0026in, \u0026out) if err != nil { c.AbortWithStatusJSON(500, gin.H{\"err\": err.Error()}) return } c.JSON(200, \u0026out) } } func _Hello_Get0_HTTP_Handler(srv HelloHTTPHandler) func(c *gin.Context) { return func(c *gin.Context) { var ( in GetRequest out GetResponse ) if err := c.ShouldBind(\u0026in); err != nil { c.AbortWithStatusJSON(400, gin.H{\"err\": err.Error()}) return } err := srv.Get(context.Background(), \u0026in, \u0026out) if err != nil { c.AbortWithStatusJSON(500, gin.H{\"err\": err.Error()}) return } c.JSON(200, \u0026out) } } 重点是 RegisterHelloHTTPHandler 方法，这样我就注册一个 gin.RouterGroup 和 HelloHTTPHandler 就可以直接提供一个 http 服务 HelloHTTPHandler 接口里方法的签名与go-micro生成的 grpc 方法保持了一致， 这样我只需要实现 grpc 的代码里对应的 Interface{} 接口，就可以服用，完全不会产生多余代码。 go-micro 生成的 pb 代码片段： type HelloHandler interface { Add(context.Context, *AddRequest, *AddResponse) error Get(context.Context, *GetRequest, *GetResponse) error } func RegisterHelloHandler(s server.Server, hdlr HelloHandler, opts ...server.HandlerOption) error {} 我在 main 函数注册的时候也只需要多注册一次 http handler 即可， main.go // 它实现了 HelloHandler type implHello struct{} RegisterHelloHandler(micro.Server, \u0026implHello) g := gin.New() // implHello 实现HelloHandler 那就是实现了HelloHTTPHandler RegisterHelloHTTPHandler(g.Group(\"/\"), \u0026implHello) 所以我就很容易通过 http 接口调试 grpc 方法，甚至可以对外提供服务，一举两得。 ","date":"2021-07-08","objectID":"/posts/go-protoc-http/:2:0","tags":["go","grpc","protoc"],"title":"如何自定义 protoc 插件","uri":"/posts/go-protoc-http/"},{"categories":["proto"],"content":"如何实现 ","date":"2021-07-08","objectID":"/posts/go-protoc-http/:3:0","tags":["go","grpc","protoc"],"title":"如何自定义 protoc 插件","uri":"/posts/go-protoc-http/"},{"categories":["proto"],"content":"程序入口 main.go package main import ( \"flag\" \"google.golang.org/protobuf/compiler/protogen\" \"google.golang.org/protobuf/types/pluginpb\" ) // protoc-gen-go-http 工具版本 // 与 GalaxyMicroVersion 保持一致 const version = \"v0.0.12\" func main() { // 1. 传参定义 // 即 插件是支持自定义参数的，这样我们可以更加灵活，针对不同的场景生成不同的代码 var flags flag.FlagSet // 是否忽略没有指定 google.api 的方法 omitempty := flags.Bool(\"omitempty\", true, \"omit if google.api is empty\") // 我这里同时支持了 gin 和 iris 可以通过参数指定生成 routerEngine := flags.String(\"router\", \"gin\", \"http router engine, choose between gin and iris\") // 是否生校验代码块 // 发现了一个很有用的插件 github.com/envoyproxy/protoc-gen-validate // 可以在 pb 的 message 中设置参数规则，然后会生成一个 validate.go 的文件 针对每个 message 生成一个 Validate() 方法 // 我在每个 handler 处理业务前做了一次参数校验判断，通过这个 flag 控制是否生成这段校验代码 genValidateCode := flags.Bool(\"validate\", false, \"add validate request params in handler\") // 生成代码时参数 这么传：--go-http_out=router=iris,validate=true:. gp := \u0026GenParam{ Omitempty: omitempty, RouterEngine: routerEngine, GenValidateCode: genValidateCode, } // 这里就是入口，指定 option 后执行 Run 方法 ，我们的主逻辑就是在 Run 方法 protogen.Options{ ParamFunc: flags.Set, }.Run(func(gen *protogen.Plugin) error { gen.SupportedFeatures = uint64(pluginpb.CodeGeneratorResponse_FEATURE_PROTO3_OPTIONAL) for _, f := range gen.Files { if !f.Generate { continue } // 这里是我们的生成代码方法 generateFile(gen, f, gp) } return nil }) } type GenParam struct { Omitempty *bool RouterEngine *string GenValidateCode *bool } ","date":"2021-07-08","objectID":"/posts/go-protoc-http/:3:1","tags":["go","grpc","protoc"],"title":"如何自定义 protoc 插件","uri":"/posts/go-protoc-http/"},{"categories":["proto"],"content":"读取 pb 文件定义 http.go import ( \"fmt\" \"strings\" \"google.golang.org/genproto/googleapis/api/annotations\" \"google.golang.org/protobuf/compiler/protogen\" \"google.golang.org/protobuf/proto\" \"google.golang.org/protobuf/types/descriptorpb\" ) const ( contextPackage = protogen.GoImportPath(\"context\") ginPackage = protogen.GoImportPath(\"github.com/gin-gonic/gin\") irisPackage = protogen.GoImportPath(\"github.com/kataras/iris/v12\") ) var methodSets = make(map[string]int) // generateFile generates a _http.pb.go file containing gin/iris handler. func generateFile(gen *protogen.Plugin, file *protogen.File, gp *GenParam) *protogen.GeneratedFile { if len(file.Services) == 0 || (*gp.Omitempty \u0026\u0026 !hasHTTPRule(file.Services)) { return nil } // 这里我们可以自定义文件名 filename := file.GeneratedFilenamePrefix + \".pb.http.go\" g := gen.NewGeneratedFile(filename, file.GoImportPath) // 写入一些警告之类的 告诉用户不要修改 g.P(\"// Code generated by protoc-gen-go-http. DO NOT EDIT.\") g.P(\"// versions:\") g.P(fmt.Sprintf(\"// protoc-gen-go-http %s\", version)) g.P() g.P(\"package \", file.GoPackageName) g.P() generateFileContent(gen, file, g, gp) return g } // generateFileContent generates the _http.pb.go file content, excluding the package statement. func generateFileContent(gen *protogen.Plugin, file *protogen.File, g *protogen.GeneratedFile, gp *GenParam) { if len(file.Services) == 0 { return } // import // 这里有个插曲：其实 import 相关的代码我们这么不需要特殊指定，protogen 包会帮我们处理， // 但是import 的 path 前的别名默认取 path 最后一个 `/` 之后的字符， // 比如：github.com/kataras/iris/v12 被处理成 v12 \"github.com/kataras/iris/v12\" // 这个我不太愿意接受 所以自己写入 import g.P(\"// This imports are custom by galaxy micro framework.\") g.P(\"import (\") switch *gp.RouterEngine { case \"gin\": g.P(\"gin\", \" \", ginPackage) case \"iris\": g.P(\"iris\", \" \", irisPackage) } g.P(\")\") // 注： 我们难免有一些 _ \"my/package\" 这种需求，这其实不用自己写 直接调 g.Import(\"my/package\") 就可以 // 这里定义一堆变量是为了程序编译的时候确保这些包是正确的，如果包不存在或者这些定义的包变量不存在都会编译失败 g.P(\"// This is a compile-time assertion to ensure that this generated file\") g.P(\"// is compatible with the galaxy package it is being compiled against.\") // 只要调用这个 Ident 方法 就会自动写入到 import 中 ，所以如果对 import 的包名没有特殊要求，那就直接使用 Ident g.P(\"var _ \", contextPackage.Ident(\"Context\")) // 像我自己自定义 import 的包就不要使用 Ident 方法，否则生成的代码文件里有两个同一个包的引入导致语法错误 switch *gp.RouterEngine { case \"gin\": g.P(\"const _ = \", \"gin.\", \"Version\") case \"iris\": g.P(\"const _ = \", \"iris.\", \"Version\") } g.P() // 到这里我们就把包名 import 和变量写入成功了，剩下的就是针对 rpc service 生成对应的 handler for _, service := range file.Services { genService(gen, file, g, service, gp) } } // rpc service 信息 type serviceDesc struct { ServiceType string // Greeter ServiceName string // helloworld.Greeter Metadata string // api/helloworld/helloworld.proto GenValidate bool Methods []*methodDesc MethodSets map[string]*methodDesc } // rpc 方法信息 type methodDesc struct { // method Name string Num int Request string Reply string // http_rule Path string Method string CamelCaseMethod string HasVars bool HasBody bool Body string ResponseBody string } // 生成 service 相关代码 func genService(gen *protogen.Plugin, file *protogen.File, g *protogen.GeneratedFile, service *protogen.Service, gp *GenParam) { if service.Desc.Options().(*descriptorpb.ServiceOptions).GetDeprecated() { g.P(\"//\") g.P(deprecationComment) } // HTTP Server. // 服务的主要变量，比如服务名 服务类型等 sd := \u0026serviceDesc{ ServiceType: service.GoName, ServiceName: string(service.Desc.FullName()), Metadata: file.Desc.Path(), GenValidate: *gp.GenValidateCode, } // 开始遍历服务的方法 for _, method := range service.Methods { // 不处理 if method.Desc.IsStreamingClient() || method.Desc.IsStreamingServer() { continue } // annotations 这个就是我们在 rpc 方法里 option 里定义的 http 路由 rule, ok := proto.GetExtension(method.Desc.Options(), annotations.E_Http).(*annotations.HttpRule) if rule != nil \u0026\u0026 ok { for _, bind := range rule.AdditionalBindings { // 拿到 option里定义的路由， http method等信息 sd.Methods = append(sd.Methods, buildHTTPRule(g, method, bind)) } sd.Methods = append(sd.Methods, buildHTTPRule(g, method, rule)) } else if !*gp.Omitempty { path := fmt.Sprint","date":"2021-07-08","objectID":"/posts/go-protoc-http/:3:2","tags":["go","grpc","protoc"],"title":"如何自定义 protoc 插件","uri":"/posts/go-protoc-http/"},{"categories":["proto"],"content":"模板渲染 // execute 方法实现也其实不复杂，总起来就是 go 的 temple 包的使用 // 提前写好模板文件，然后拿到所有需要的变量，进行模板渲染，写入文件 func (s *serviceDesc) execute(routerEngine string) string { var ( name = routerEngine tmp string ) switch routerEngine { case \"gin\": tmp = ginTemplate case \"iris\": tmp = irisTemplate default: panic(\"unknown http engine\") } s.MethodSets = make(map[string]*methodDesc) for _, m := range s.Methods { s.MethodSets[m.Name] = m } buf := new(bytes.Buffer) tmpl, err := template.New(name).Parse(strings.TrimSpace(tmp)) if err != nil { panic(err) } if err = tmpl.Execute(buf, s); err != nil { panic(err) } return strings.Trim(buf.String(), \"\\r\\n\") } ","date":"2021-07-08","objectID":"/posts/go-protoc-http/:3:3","tags":["go","grpc","protoc"],"title":"如何自定义 protoc 插件","uri":"/posts/go-protoc-http/"},{"categories":["proto"],"content":"模板内容 var ginTemplate = ` {{$svrType := .ServiceType}} {{$svrName := .ServiceName}} {{$validate := .GenValidate}} // 这里定义 handler interface type {{.ServiceType}}HTTPHandler interface { {{- range .MethodSets}} {{.Name}}(context.Context, *{{.Request}}, *{{.Reply}}) error {{- end}} } // Register{{.ServiceType}}HTTPHandler define http router handle by gin. // 注册路由 handler func Register{{.ServiceType}}HTTPHandler(g *gin.RouterGroup, srv {{.ServiceType}}HTTPHandler) { {{- range .Methods}} g.{{.Method}}(\"{{.Path}}\", _{{$svrType}}_{{.Name}}{{.Num}}_HTTP_Handler(srv)) {{- end}} } // 定义 handler // 遍历之前解析到所有 rpc 方法信息 {{range .Methods}} func _{{$svrType}}_{{.Name}}{{.Num}}_HTTP_Handler(srv {{$svrType}}HTTPHandler) func(c *gin.Context) { return func(c *gin.Context) { var ( in = new({{.Request}}) out = new({{.Reply}}) ctx = middleware.GetContextFromGinCtx(c) ) if err := c.ShouldBind(in{{.Body}}); err != nil { c.AbortWithStatusJSON(400, gin.H{\"err\": err.Error()}) return } // 这里就是最开始提到的判断是否启用 validate // 其中这个 api.Validator 接口只有一个方法 Validate() error // 所以需要在一个统一的地方定义好引入使用，建议不要在生成的时候写入，因为这个是通用的 interface{} {{if $validate -}} // check param if v, ok := interface{}(in).(api.Validator);ok { if err := v.Validate();err != nil { c.AbortWithStatusJSON(400, gin.H{\"err\": err.Error()}) return } } {{end -}} // 执行方法 err := srv.{{.Name}}(ctx, in, out) if err != nil { c.AbortWithStatusJSON(500, gin.H{\"err\": err.Error()}) return } c.JSON(200, out) } } {{end}} ` iris 的模板基本类似。 到这里代码部分完全结束，做一个简单的总结： 构思需求，即我需要什么样的插件，它需要给我生成什么的代码块？ 根据需求先自己写一个预期代码，然后把这份代码拆解成一个模板，提取里面的可以渲染的变量。 模板里可以有逻辑，也就是可以做一些参数校验的方式，生成不同的代码，比如针对不同的 http 方法，做不同的处理，针对不同的插件参数生成不同的代码块。 程序入口到渲染文件前这段代码，基本都用 protogen 包提供的方法，可以对这个包做一些调研阅读文档，看看它都提供什么能力, 说不定可以少走很多弯路。 基本就这些了，我也是各种琢磨琢磨出来的，建议大家多动手，只要不写永远学不到精髓。 ","date":"2021-07-08","objectID":"/posts/go-protoc-http/:3:4","tags":["go","grpc","protoc"],"title":"如何自定义 protoc 插件","uri":"/posts/go-protoc-http/"},{"categories":["Go"],"content":"关于如何用 go 语言编写一个命令行工具。这里会基于 cobra 开源库进行开发。cobra 作为一个非常有名的命令行工具库，被很多开源项目引入使用，很多命令行工具都能看到 cobra 的身影。cobra 提供一个完整的命令行的工具的所需的功能，包括命令定义、命令扩展、读取参数等。下面我们以开发一个命令行工具的流程一步步学习如何使用 cobra 开发一个自己的命令行工具。 ","date":"2021-06-30","objectID":"/posts/go-cobra/:0:0","tags":["go","cobra"],"title":"如何编写自己的第一个命令行工具","uri":"/posts/go-cobra/"},{"categories":["Go"],"content":"创建根命令 我们项目暂且就叫 myCmd, 我们本地创建一个go项目就叫 myCmd。 $ mkdir myCmd $ cd myCmd $ touch main.go $ go mod init myCmd main.go package main import ( \"log\" \"github.com/spf13/cobra\" ) var ( // 定义主命令 rootCmd = \u0026cobra.Command{ Use: \"myCmd\", Short: \"这里是对命令的简短介绍\", Long: `这里可以放对命令的详细介绍。 可以多行`, Example: \"myCmd help\", // 使用示例 Version: \"v0.0.1\", // 定义版本 } dirPath string ) func init() { // 定义参数，即从命令行读取的参数变量 // 除了 PersistentFlags 外，也可以用 Flags()，区别是 前一个可以在其子命令也可以用，后一个不能。即PersistentFlags是一个全局的flag注册。 rootCmd.PersistentFlags().StringVarP(\u0026dirPath, \"dir\", \"d\", \".\", \"文件路径\") } func main() { if err := rootCmd.Execute(); err != nil { log.Fatal(err) } } 这样我们就创建了一个属于的自己的命令，执行看一下效果。 # 直接执行,会打印 字段Long的值， ➜./myCmd 这里可以放对命令的详细介绍。 可以多行 # 打印版本 ➜./myCmd -v myCmd version v0.0.1 # 输入未知 flag ➜./myCmd -x Error: unknown shorthand flag: 'x' in -x Usage: Examples: myCmd help Flags: -d, --dir string 文件路径 (default \".\") -h, --help help for myCmd -v, --version version for myCmd 2021/07/04 15:18:59 unknown shorthand flag: 'x' in -x 不难发现，版本处理，未知参数处理等情况 cobra已经做了相对完善的处理，我们不需要做太多的错误处理。 目前未知，我们的的命令只是定义了命令，并没有执行任何指令，下面我们添加一个简单的执行函数。cobra.Command 有很多参数可以定义执行函数的，我们以最常用的的 Run，RunE 为例，分别是不返回错误和返回错误的函数定义。 假如我们的主命令执行一个打印 d 参数传值的目录的信息。 // rootCmd RunE: printDirInfo, /* ... */ func printDirInfo(cmd *cobra.Command, args []string) error { info, err := os.Stat(dirPath) if err != nil { return err } fmt.Printf(\"name:%s, size:%d modTime:%v \\n\", info.Name(), info.Size(), info.ModTime()) return nil } 执行一下命令： # 查看一下 main 文件的信息 ➜./myCmd -d main.go name:main.go, size:759 modTime:2021-07-04 15:26:43.311399368 +0800 CST # 查看一个不存在的文件 ➜./myCmd -d main.go1 Error: stat main.go1: no such file or directory Usage: myCmd [flags] Examples: myCmd help Flags: -d, --dir string 命令执行目录 (default \".\") -h, --help help for myCmd -v, --version version for myCmd 2021/07/04 15:30:01 stat main.go1: no such file or directory # 不仅打印出错误，如何使用命令也会同时打印出来 下面我们就添加我们的子命令。 ","date":"2021-06-30","objectID":"/posts/go-cobra/:1:0","tags":["go","cobra"],"title":"如何编写自己的第一个命令行工具","uri":"/posts/go-cobra/"},{"categories":["Go"],"content":"添加子命令 我们现在添加一个子命令，这个子命令的功能是统计当前目录下的所有文件信息，我们就起名叫 stat。同时，为了方便全局变量的在不同包内读取，创建一个 variable 的目录，里面存放全局的一些变量，包内变量就放到各自包内。 ➜ mkdir stat ➜ mkdir variable ➜ touch stat/stat.go ➜ touch variable/variable.go 下面是stat文件的内容。 stat.go package stat import ( \"fmt\" \"myCmd/variable\" \"os\" \"path/filepath\" \"github.com/spf13/cobra\" ) var ( StatCmd = \u0026cobra.Command{ Use: \"stat\", Short: \"统计目录\", RunE: statDir, } isStatDir bool ) func init() { // 这里使用 Flags 只在我这个命令内解析和读取 StatCmd.Flags().BoolVarP(\u0026isStatDir, \"stat_dir\", \"s\", false, \"是否统计目录信息\") } func statDir(cmd *cobra.Command, args []string) error { return filepath.Walk(variable.DirPath, func(path string, info os.FileInfo, err error) error { if err != nil { return err } if info.IsDir() \u0026\u0026 !isStatDir { // 不统计 return nil } fmt.Printf(\"path:%s, size:%d, modTime:%v\", path, info.Size(), info.ModTime()) return nil }) } 然后将该子命令注册的主命令下。 main.go package main import ( \"fmt\" \"log\" \"myCmd/stat\" \"myCmd/variable\" \"os\" \"github.com/spf13/cobra\" ) var ( rootCmd = \u0026cobra.Command{ Use: \"myCmd\", Short: \"这里是对命令的简短介绍\", Long: `这里可以放对命令的详细介绍。 可以多行`, Example: \"myCmd help\", // 使用示例 Version: variable.Version, // 全局变量常量都移到 variable 目录下 RunE: printDirInfo, } ) func init() { rootCmd.PersistentFlags().StringVarP(\u0026variable.DirPath, \"dir\", \"d\", \".\", \"文件路径\") // 注册命令 rootCmd.AddCommand(stat.StatCmd) } func printDirInfo(cmd *cobra.Command, args []string) error { info, err := os.Stat(variable.DirPath) if err != nil { return err } fmt.Printf(\"name:%s, size:%d modTime:%v \\n\", info.Name(), info.Size(), info.ModTime()) return nil } func main() { if err := rootCmd.Execute(); err != nil { log.Fatal(err) } } 再次执行 help 查看我们的命令。 ➜./myCmd -h 这里可以放对命令的详细介绍。 可以多行 Usage: myCmd [flags] myCmd [command] Examples: myCmd help Available Commands: help Help about any command stat 统计目录 Flags: -d, --dir string 文件路径 (default \".\") -h, --help help for myCmd -v, --version version for myCmd Use \"myCmd [command] --help\" for more information about a command. # 查看子命令help ➜./myCmd stat -h 统计目录 Usage: myCmd stat [flags] Flags: -h, --help help for stat -s, --stat_dir 是否统计目录信息 Global Flags: -d, --dir string 文件路径 (default \".\") # 统计 ➜./myCmd stat -d . path:go.mod, size:61, modTime:2021-07-04 15:03:33.339495852 +0800 CST path:go.sum, size:56568, modTime:2021-07-04 15:03:33.339185898 +0800 CST path:main.go, size:929, modTime:2021-07-04 15:55:07.206300444 +0800 CST path:myCmd, size:4344056, modTime:2021-07-04 16:01:12.930132286 +0800 CST path:stat/stat.go, size:691, modTime:2021-07-04 16:01:09.353487727 +0800 CST path:variable/variable.go, size:73, modTime:2021-07-04 15:48:18.345134258 +0800 CST 不难发现，这个子命令可以无限嵌套，我们可以拥有二级三级子命令，能满足我们各种各样奇葩的需求，子命令可以复用其上级目录的 flag参数。 ","date":"2021-06-30","objectID":"/posts/go-cobra/:2:0","tags":["go","cobra"],"title":"如何编写自己的第一个命令行工具","uri":"/posts/go-cobra/"},{"categories":["Go"],"content":"自主更新 假如我们开发命令，已经发布到 GitHub 上，别人可以简单的 go get 命令就能安装使用我们的命令。但是我要是发布一个新版本，希望使用的人能知道我的命令工具有新版了而且要是能方便的更新到最新的版本是不是一个非常人性化的设计呢？ 其实实现起来也不难，这里抛出个思路。假如我们命令每次执行的时候，我做一次版本检查（但是强烈不建议每次都检查，最好本地做一个上次检查时间的缓存，最多一天检查一次，否则用户体验非常不好），如果有新的版本我就提醒用户，甚至我可以检查的时候拉过来新版本的 feature 展现给用户，，然后提供一个 update 的子命令，自我更新，这体验是不是听起来就很不错呀。 至于 update 这个子命令实现也很简单，尝试执行一次 go get -u \u003cmyCmdRemoteURL\u003e 即可，虽然看起来是对 go get 的一次封装，但是对于用户来说就很简单方便。 ","date":"2021-06-30","objectID":"/posts/go-cobra/:3:0","tags":["go","cobra"],"title":"如何编写自己的第一个命令行工具","uri":"/posts/go-cobra/"},{"categories":["Go"],"content":"小彩蛋 到这里我们一个小命令行工具也有模有样了，但是缺一个灵魂，是什么呢？ 当然是 命令的炫酷的logo！！！ 先看效果图： # 普通版本 __ __ __ __ _____ __ __ _____ | \\/ | \\ \\ / / / ____| | \\/ | | __ \\ | \\ / | \\ \\_/ / | | | \\ / | | | | | | |\\/| | \\ / | | | |\\/| | | | | | | | | | | | | |____ | | | | | |__| | |_| |_| |_| \\_____| |_| |_| |_____/ # 斜体 /| //| | \\\\ / / // ) ) /| //| | // ) ) //| // | | \\\\ / / // //| // | | // / / // | // | | \\\\/ / // // | // | | // / / // | // | | / / // // | // | | // / / // |// | | / / ((____/ / // |// | | //____/ / # 夸张版本 _____ _____ _____ _____ _____ /\\ \\ |\\ \\ /\\ \\ /\\ \\ /\\ \\ /::\\____\\ |:\\____\\ /::\\ \\ /::\\____\\ /::\\ \\ /::::| | |::| | /::::\\ \\ /::::| | /::::\\ \\ /:::::| | |::| | /::::::\\ \\ /:::::| | /::::::\\ \\ /::::::| | |::| | /:::/\\:::\\ \\ /::::::| | /:::/\\:::\\ \\ /:::/|::| | |::| | /:::/ \\:::\\ \\ /:::/|::| | /:::/ \\:::\\ \\ /:::/ |::| | |::| | /:::/ \\:::\\ \\ /:::/ |::| | /:::/ \\:::\\ \\ /:::/ |::|___|______ |::|___|______ /:::/ / \\:::\\ \\ /:::/ |::|___|______ /:::/ / \\:::\\ \\ /:::/ |::::::::\\ \\ /::::::::\\ \\ /:::/ / \\:::\\ \\ /:::/ |::::::::\\ \\ /:::/ / \\:::\\ ___\\ /:::/ |:::::::::\\____\\ /::::::::::\\____\\/:::/____/ \\:::\\____\\/:::/ |:::::::::\\____\\/:::/____/ \\:::| | \\::/ / ~~~~~/:::/ / /:::/~~~~/~~ \\:::\\ \\ \\::/ /\\::/ / ~~~~~/:::/ /\\:::\\ \\ /:::|____| \\/____/ /:::/ / /:::/ / \\:::\\ \\ \\/____/ \\/____/ /:::/ / \\:::\\ \\ /:::/ / /:::/ / /:::/ / \\:::\\ \\ /:::/ / \\:::\\ \\ /:::/ / /:::/ / /:::/ / \\:::\\ \\ /:::/ / \\:::\\ /:::/ / /:::/ / \\::/ / \\:::\\ \\ /:::/ / \\:::\\ /:::/ / /:::/ / \\/____/ \\:::\\ \\ /:::/ / \\:::\\/:::/ / /:::/ / \\:::\\ \\ /:::/ / \\::::::/ / /:::/ / \\:::\\____\\ /:::/ / \\::::/ / \\::/ / \\::/ / \\::/ / \\::/____/ \\/____/ \\/____/ \\/____/ ~~ 我随机选了几个作为演示，点击这里跳转制作自己工具的logo，然后再主命令注册一个 PreRun 的函数，在该函数内打印我们的logo。这样在主逻辑执行前会打印我们的logo，辨识度一下子提高很多。 实际效果： ➜./myCmd -d main.go __ __ __ __ _____ __ __ _____ | \\/ | \\ \\ / / / ____| | \\/ | | __ \\ | \\ / | \\ \\_/ / | | | \\ / | | | | | | |\\/| | \\ / | | | |\\/| | | | | | | | | | | | | |____ | | | | | |__| | |_| |_| |_| \\_____| |_| |_| |_____/ name:main.go, size:1320 modTime:2021-07-04 16:28:51.339520282 +0800 CST 暂且就这么多，感谢 spf13/cobra 的作者，提供这么高质量的开源库。 ","date":"2021-06-30","objectID":"/posts/go-cobra/:4:0","tags":["go","cobra"],"title":"如何编写自己的第一个命令行工具","uri":"/posts/go-cobra/"},{"categories":["microservice"],"content":"go-micro 作为比较流行的微服务框架，其良好的接口设计为后期扩展使用带来了非常好的便利性。本文章主要讲在 go-micro 中用 nacos 作为服务注册中心和配置中心。 ","date":"2021-06-23","objectID":"/posts/use-nacos-with-go-micro/:0:0","tags":["go","go-micro"],"title":"Go-Micro 中使用Nacos","uri":"/posts/use-nacos-with-go-micro/"},{"categories":["microservice"],"content":"注册中心 先看一下 go-micro 定义的服务注册接口。 registry.go // 服务注册接口 type Registry interface { // 初始化 Init(...Option) error // 返回可选参数 Options() Options // 服务注册 Register(*Service, ...RegisterOption) error // 服务注销 Deregister(*Service, ...DeregisterOption) error // 查询服务 GetService(string, ...GetOption) ([]*Service, error) // 列出服务 ListServices(...ListOption) ([]*Service, error) // 监听服务 Watch(...WatchOption) (Watcher, error) String() string } 只要基于任意一个服务注册服务实现以上接口，即可在 go-micro 中作为注册中心使用。假如我用一个 customRegistry 实现接口后，在 go-micro 初始化的时候或服务启动时候通过启动参数指定实现接口的接口的 String() string方法的返回值接口。 如： // 假如该结构体已实现 Registry 接口 type customRegistry struct {} func (c *customRegistry) String() string { return \"custom\" } // 代码中指定 func main() { micro.NewService(micro.Registry(\u0026customRegistry{})) } // 启动参数指定 ./myApp -- registry custom 如此一看，发现非常方便和好扩展，接下来贴出如何使用nacos 实现该 Registry 接口。 直接列出关键代码块： registry.go import ( \"errors\" \"fmt\" \"net\" \"strconv\" \"time\" \"github.com/asim/go-micro/v3/cmd\" \"github.com/asim/go-micro/v3/registry\" \"github.com/nacos-group/nacos-sdk-go/v2/clients\" \"github.com/nacos-group/nacos-sdk-go/v2/clients/naming_client\" \"github.com/nacos-group/nacos-sdk-go/v2/common/constant\" \"github.com/nacos-group/nacos-sdk-go/v2/common/logger\" \"github.com/nacos-group/nacos-sdk-go/v2/vo\" ) type nacosRegistry struct { // nacos sdk 的client client naming_client.INamingClient // 可选参数，初始化的时候可以通过 registry.Option 方法指定配置 opts registry.Options } func init() { // 设置为默认配置 cmd.DefaultRegistries[\"nacos\"] = NewRegistry } // NewRegistry NewRegistry func NewRegistry(opts ...registry.Option) registry.Registry { n := \u0026nacosRegistry{ opts: registry.Options{}, } if err := configure(n, opts...); err != nil { panic(err) } return n } // 这个方法总结下来就是干了一件事：配置初始化 func configure(n *nacosRegistry, opts ...registry.Option) error { // set opts for _, o := range opts { o(\u0026n.opts) } clientConfig := constant.ClientConfig{} serverConfigs := make([]constant.ServerConfig, 0) contextPath := \"/nacos\" cfg, ok := n.opts.Context.Value(configKey{}).(constant.ClientConfig) if ok { clientConfig = cfg } addrs, ok := n.opts.Context.Value(addressKey{}).([]string) if !ok { addrs = []string{\"127.0.0.1:8848\"} // 默认连接本地 } for _, addr := range addrs { // check we have a port host, port, err := net.SplitHostPort(addr) if err != nil { return err } p, err := strconv.ParseUint(port, 10, 64) if err != nil { return err } serverConfigs = append(serverConfigs, constant.ServerConfig{ // Scheme: \"go.micro\", IpAddr: host, Port: p, ContextPath: contextPath, }) } if n.opts.Timeout == 0 { n.opts.Timeout = time.Second * 1 } clientConfig.TimeoutMs = uint64(n.opts.Timeout.Milliseconds()) // 创建客户端 client, err := clients.CreateNamingClient(map[string]interface{}{ constant.KEY_SERVER_CONFIGS: serverConfigs, constant.KEY_CLIENT_CONFIG: clientConfig, }) if err != nil { return err } n.client = client return nil } func (n *nacosRegistry) Init(opts ...registry.Option) error { _ = configure(n, opts...) return nil } func (n *nacosRegistry) Options() registry.Options { return n.opts } func (n *nacosRegistry) Register(s *registry.Service, opts ...registry.RegisterOption) error { var options registry.RegisterOptions for _, o := range opts { o(\u0026options) } withContext := false // 处理参数 param := vo.RegisterInstanceParam{} if options.Context != nil { if p, ok := options.Context.Value(\"register_instance_param\").(vo.RegisterInstanceParam); ok { param = p withContext = ok } } if !withContext { host, port, err := getNodeIPPort(s) if err != nil { return err } s.Nodes[0].Metadata[\"version\"] = s.Version param.Ip = host param.Port = uint64(port) param.Metadata = s.Nodes[0].Metadata param.ServiceName = s.Name param.Enable = true param.Healthy = true param.Weight = 1.0 param.Ephemeral = true } // 注册节点 _, err := n.client.RegisterInstance(param) return err } func (n *nacosRegistry) Deregister(s *registry.Service, opts ...registry.DeregisterOption) error { var options registry.DeregisterOptions for _, o := range opts { o(\u0026options) } withContext := false param := vo.DeregisterInst","date":"2021-06-23","objectID":"/posts/use-nacos-with-go-micro/:1:0","tags":["go","go-micro"],"title":"Go-Micro 中使用Nacos","uri":"/posts/use-nacos-with-go-micro/"},{"categories":["microservice"],"content":"关于如何使用go的微服务框架 go-micro/v3 的使用和其插件的自定义。第一部分将框架的架构大致了解一遍。 ","date":"2021-06-11","objectID":"/posts/go-micro-1/:0:0","tags":["go","go-micro"],"title":"Go-Micro 的架构及其使用（一）","uri":"/posts/go-micro-1/"},{"categories":["microservice"],"content":"架构 以 v3.5.1 分支为例 go-micor 项目的目录结构如下： $ tree -L 2 . ├── LICENSE ├── README.md ├── _config.yml ├── api // api 接口的定义，包括http、grpc、router等 ├── auth // 账号认证接口的定义 ├── broker // 消息队列接口定义及默认实现 ├── client // 客户端相关接口定义和实现 ├── cmd // 可执行命令（包括生成protobuf的命令实现） ├── codec // code encoder ├── config // 动态配置的接口定义 ├── debug // debug 模式 ├── errors // 错误处理 ├── examples // 各个模块的示例代码 ├── logger // 日志模块接口定义 ├── metadata // 原数据 ├── plugins // 各个模块定义的接口的不同实现 ├── registry // 服务注册接口定义 ├── selector // 负载均衡 ├── server // 服务端接口定义 ├── store // 数据存储接口定义 ├── sync ├── transport // 请求转发 └── util // 工具类 下面按目录将 go-micro 的主要核心模块过一遍。 ","date":"2021-06-11","objectID":"/posts/go-micro-1/:1:0","tags":["go","go-micro"],"title":"Go-Micro 的架构及其使用（一）","uri":"/posts/go-micro-1/"},{"categories":["microservice"],"content":"API api 层为定义和实现基于http/gRPC的api service。即http请求处理 路由处理 路由注册等。 接口定义： type Api interface { // Initialise options Init(...Option) error // Get the options Options() Options // Register a http handler Register(*Endpoint) error // Register a route Deregister(*Endpoint) error // Implemenation of api String() string } 目录结构： $ tree . ├── api.go ├── api_test.go ├── handler // 接口处理方法 │ ├── api // 实现 http.ServerHTTP() 方法 │ ├── event // 基于消息队列的实现 │ ├── handler.go // 接口定义 │ ├── http // 基于http的实现 │ ├── options.go │ ├── rpc // 基于rpc的实现 │ └── web // 支持websocket的实现 ├── proto │ ├── api.pb.go │ ├── api.pb.micro.go │ └── api.proto // 数据结构定义 ├── resolver // 解析请求及路由 │ ├── grpc │ ├── host │ ├── options.go │ ├── path │ ├── resolver.go │ └── vpath ├── router // 路由定义和注册 │ ├── options.go │ ├── registry │ ├── router.go │ ├── static │ └── util └── server // 服务定义和启动 ├── acme ├── cors ├── http ├── options.go └── server.go ","date":"2021-06-11","objectID":"/posts/go-micro-1/:1:1","tags":["go","go-micro"],"title":"Go-Micro 的架构及其使用（一）","uri":"/posts/go-micro-1/"},{"categories":["microservice"],"content":"Config config 作为动态配置中心的接口定义和实现。支持动态加载、插件式配置源、配置合并和观察配置变化。 接口定义： // Config is an interface abstraction for dynamic configuration // 配置接口定义 type Config interface { // provide the reader.Values interface // 读取到的配置的reader reader.Values // Init the config Init(opts ...Option) error // Options in the config Options() Options // Stop the config loader/watcher Close() error // Load config sources // 可以加载多个Source Load(source ...source.Source) error // Force a source changeset sync // 同步配置变化 Sync() error // Watch a value for changes // 订阅配置变化 Watch(path ...string) (Watcher, error) } // Watcher is the config watcher type Watcher interface { Next() (reader.Value, error) Stop() error } // Source is the source from which config is loaded // Source 就是配置来源 go-micro 已实现基于consul，etcd，file等多种配置来源，也可以自己实现下面接口来使用 type Source interface { Read() (*ChangeSet, error) Write(*ChangeSet) error Watch() (Watcher, error) String() string } // Reader is an interface for merging changesets // 用于配置合并 // go-micro 实现了基于json的Reader,默认用json作为解析配置内容，并在插件目录内实现了 toml yaml xml等格式的Encoder可以按需求替换 type Reader interface { Merge(...*source.ChangeSet) (*source.ChangeSet, error) Values(*source.ChangeSet) (Values, error) String() string } // Values is returned by the reader // 用于读写配置，读取的配置会返回 Value type Values interface { Bytes() []byte Get(path ...string) Value Set(val interface{}, path ...string) Del(path ...string) Map() map[string]interface{} Scan(v interface{}) error } // Value represents a value of any type // Value 为拿到的配置，可以通过其方法转到基础类型。 type Value interface { Bool(def bool) bool Int(def int) int String(def string) string Float64(def float64) float64 Duration(def time.Duration) time.Duration StringSlice(def []string) []string StringMap(def map[string]string) map[string]string Scan(val interface{}) error Bytes() []byte } 目录结构： $ tree -L 2 . ├── README.md ├── config.go // Config 接口定义 ├── default.go // 默认实现的Config ├── default_test.go ├── encoder // encoder 解析配置内容 │ ├── encoder.go │ └── json // json实现 ├── loader // 加载配置 │ ├── loader.go │ └── memory // 基于内存的加载，即启动时会将配置加载到内存 ├── options.go ├── reader // 定义和实现Reader，内部依赖Encoder │ ├── json │ ├── options.go │ ├── preprocessor.go │ ├── preprocessor_test.go │ └── reader.go ├── secrets // 定义和实现需要加解密的配置 │ ├── box │ ├── secretbox │ └── secrets.go ├── source // 配置来源 │ ├── changeset.go │ ├── cli │ ├── env // 基于环境变量的实现 │ ├── file // 基于本地文件实现 │ ├── flag // 基于启动参数flag实现 │ ├── memory // 基于内存实现 │ ├── noop.go │ ├── options.go │ └── source.go └── value.go plugins/config/encoder 目录: 实现Encoder接口 $ tree plugins/config/encoder ├── cue ├── hcl ├── toml ├── xml └── yaml plugins/config/source 目录： 实现Source接口 $ tree plugins/config/source ├── configmap ├── consul ├── etcd ├── grpc ├── mucp ├── pkger ├── runtimevar ├── url └── vault ","date":"2021-06-11","objectID":"/posts/go-micro-1/:1:2","tags":["go","go-micro"],"title":"Go-Micro 的架构及其使用（一）","uri":"/posts/go-micro-1/"},{"categories":["microservice"],"content":"Logger Logger 包为全局日志库，默认实现了一套，并在plugins 内实现了基于 logrus，zap的个主流的日志的实现。 接口定义： // Logger is a generic logging interface type Logger interface { // Init initialises options Init(options ...Option) error // The Logger options Options() Options // Fields set fields to always be logged Fields(fields map[string]interface{}) Logger // Log writes a log entry Log(level Level, v ...interface{}) // Logf writes a formatted log entry Logf(level Level, format string, v ...interface{}) // String returns the name of logger String() string } 若需要自己定义日志格式和日志库，可以实现上面接口，并初始化的时候指定即可。 ","date":"2021-06-11","objectID":"/posts/go-micro-1/:1:3","tags":["go","go-micro"],"title":"Go-Micro 的架构及其使用（一）","uri":"/posts/go-micro-1/"},{"categories":["microservice"],"content":"plugins 该目录作为插件目录，实现了大部分预定义的接口，方便使用的时候替换成默认实现的模块代码。 该目录下所有子目录均可以作为go mod package 导入使用 在之后讲如何使用是 同时演示如何使用插件 目录结构： $ tree plugins ├── LICENSE ├── README.md ├── auth // 用户认真 │ └── jwt // 实现基于jwt的auth接口 ├── broker // 支持了市面上大部分消息队列 │ ├── gocloud │ ├── googlepubsub │ ├── grpc │ ├── http │ ├── kafka │ ├── memory │ ├── mqtt │ ├── nats │ ├── nsq │ ├── proxy │ ├── rabbitmq │ ├── redis │ ├── segmentio │ ├── snssqs │ ├── sqs │ ├── stan │ └── stomp ├── client // 支持了grpc http 等方式的客户端实现 │ ├── grpc │ ├── http │ ├── mock │ └── mucp ├── codec // 消息的编码解码的实现 │ ├── bsonrpc │ ├── json-iterator │ ├── jsonrpc2 │ ├── msgpackrpc │ └── segmentio ├── config // 配置 │ ├── encoder // 配置编码解码 │ ├── cue │ ├── hcl │ ├── toml │ ├── xml │ └── yaml │ └── source // 配置数据源 │ ├── configmap │ ├── consul │ ├── etcd │ ├── grpc │ ├── mucp │ ├── pkger │ ├── runtimevar │ ├── url │ └── vault ├── logger // 日志库 │ ├── apex │ ├── logrus │ ├── zap │ └── zerolog ├── plugin.go ├── proxy │ └── http ├── registry // 服务发现服务注册 │ ├── cache │ ├── consul │ ├── etcd │ ├── eureka │ ├── gossip │ ├── kubernetes │ ├── mdns │ ├── memory │ ├── multi │ ├── nats │ ├── proxy │ └── zookeeper ├── release.sh ├── selector // 负载均衡 │ ├── dns │ ├── label │ ├── registry │ ├── shard │ └── static ├── server // 后端服务 │ ├── grpc │ ├── http │ └── mucp ├── store // 数据存储的实现 │ ├── cockroach │ ├── consul │ ├── file │ ├── memcached │ ├── memory │ ├── mysql │ └── redis ├── sync // 数据同步 │ ├── etcd │ └── memory ├── template.go ├── transport // 服务之间通讯模块 │ ├── grpc │ ├── http │ ├── memory │ ├── nats │ ├── quic │ ├── rabbitmq │ ├── tcp │ └── utp └── wrapper // 自定义组件 比如监控、限流、熔断、追踪等 ├── README.md ├── breaker // 熔断 ├── endpoint // 指定服务节点 ├── monitoring // 监控 ├── ratelimiter // 限流 ├── select // 负载均衡 ├── service ├── trace // 链路追踪 └── validator // 参数校验（处理请求时 可以统一参数校验等工作） ","date":"2021-06-11","objectID":"/posts/go-micro-1/:1:4","tags":["go","go-micro"],"title":"Go-Micro 的架构及其使用（一）","uri":"/posts/go-micro-1/"},{"categories":["microservice"],"content":"Registry 服务发现/服务注册相关逻辑均在 registry 包内实现。 核心接口定义： // The registry provides an interface for service discovery // and an abstraction over varying implementations // {consul, etcd, zookeeper, ...} type Registry interface { Init(...Option) error Options() Options // 服务注册 Register(*Service, ...RegisterOption) error // 服务注销 Deregister(*Service, ...DeregisterOption) error // 查询服务 GetService(string, ...GetOption) ([]*Service, error) // 列出服务列表 ListServices(...ListOption) ([]*Service, error) // 监控服务 Watch(...WatchOption) (Watcher, error) String() string } // Watcher is an interface that returns updates // about services within the registry. type Watcher interface { // Next is a blocking call Next() (*Result, error) Stop() } ","date":"2021-06-11","objectID":"/posts/go-micro-1/:1:5","tags":["go","go-micro"],"title":"Go-Micro 的架构及其使用（一）","uri":"/posts/go-micro-1/"},{"categories":["microservice"],"content":"Selector 负载均衡逻辑，即客户端请求其他服务时如何选取服务节点都是在该包内实现。可以通过option指定策略，随机，轮询等。 接口定义： // Selector builds on the registry as a mechanism to pick nodes // and mark their status. This allows host pools and other things // to be built using various algorithms. type Selector interface { Init(opts ...Option) error Options() Options // Select returns a function which should return the next node Select(service string, opts ...SelectOption) (Next, error) // Mark sets the success/error against a node Mark(service string, node *registry.Node, err error) // Reset returns state back to zero for a service Reset(service string) // Close renders the selector unusable Close() error // Name of the selector String() string } // Next is a function that returns the next node // based on the selector's strategy type Next func() (*registry.Node, error) ","date":"2021-06-11","objectID":"/posts/go-micro-1/:1:6","tags":["go","go-micro"],"title":"Go-Micro 的架构及其使用（一）","uri":"/posts/go-micro-1/"},{"categories":["microservice"],"content":"Server server 包为定义和实现管理服务相关逻辑。 server的定义： // Server is a simple micro server abstraction type Server interface { // Initialise options Init(...Option) error // Retrieve the options Options() Options // Register a handler Handle(Handler) error // Create a new handler NewHandler(interface{}, ...HandlerOption) Handler // Create a new subscriber NewSubscriber(string, interface{}, ...SubscriberOption) Subscriber // Register a subscriber Subscribe(Subscriber) error // Start the server Start() error // Stop the server Stop() error // Server implementation String() string } // Router handle serving messages type Router interface { // ProcessMessage processes a message // 处理消息队列消息 ProcessMessage(context.Context, Message) error // ServeRequest processes a request to completion // 处理 http/rpc 请求 ServeRequest(context.Context, Request, Response) error } 默认实现了rpc和消息队列，http服务 可以使用plugins/server/http 包。 ","date":"2021-06-11","objectID":"/posts/go-micro-1/:1:7","tags":["go","go-micro"],"title":"Go-Micro 的架构及其使用（一）","uri":"/posts/go-micro-1/"},{"categories":["microservice"],"content":"Store 该包定义了数据存储的接口。 接口定义： // Store is a data storage interface type Store interface { // Init initialises the store. It must perform any required setup on the backing storage implementation and check that it is ready for use, returning any errors. Init(...Option) error // Options allows you to view the current options. Options() Options // Read takes a single key name and optional ReadOptions. It returns matching []*Record or an error. Read(key string, opts ...ReadOption) ([]*Record, error) // Write() writes a record to the store, and returns an error if the record was not written. Write(r *Record, opts ...WriteOption) error // Delete removes the record with the corresponding key from the store. Delete(key string, opts ...DeleteOption) error // List returns any keys that match, or an empty list with no error if none matched. List(opts ...ListOption) ([]string, error) // Close the store Close() error // String returns the name of the implementation. String() string } 具体使用数据库类型，在plugins/store 内初始化对应的实例。 ","date":"2021-06-11","objectID":"/posts/go-micro-1/:1:8","tags":["go","go-micro"],"title":"Go-Micro 的架构及其使用（一）","uri":"/posts/go-micro-1/"},{"categories":["microservice"],"content":"Sync sync 包为定义分布式选举和分布式锁的定义。 接口定义： // Sync is an interface for distributed synchronization type Sync interface { // Initialise options Init(...Option) error // Return the options Options() Options // Elect a leader // 选举 Leader(id string, opts ...LeaderOption) (Leader, error) // Lock acquires a lock // 上锁 Lock(id string, opts ...LockOption) error // Unlock releases a lock // 释放锁 Unlock(id string) error // Sync implementation String() string } // Leader provides leadership election // 提供分布式选举 type Leader interface { // resign leadership // 辞职 即放弃Leader状态 Resign() error // status returns when leadership is lost // 在leader 状态失去时，channel内可读取 Status() chan bool } ","date":"2021-06-11","objectID":"/posts/go-micro-1/:1:9","tags":["go","go-micro"],"title":"Go-Micro 的架构及其使用（一）","uri":"/posts/go-micro-1/"},{"categories":["面试"],"content":"经历了2个月的面试折磨，拿到offer总算结束了这段时间。主要是想记录面试中遇到的问题，列出来的问题不一定有答案，有答案也不一定是最佳答案，所以还是看问题为主，答案自行解决。 知识架构 ","date":"2021-05-31","objectID":"/posts/go-interview/:0:0","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"数据库 ","date":"2021-05-31","objectID":"/posts/go-interview/:1:0","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"MySQL 索引 B+tree 索引 数据存储位置-在叶子节点 相邻节点是链表结构 这样可以实现 range 查询 联合索引 最左原则 索引不能是表达式的一部分 否则不走索引 为什么主键是递增的，随机会怎么样？ 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置，此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。 哈希索引 - 精准查询 聚簇索引和非聚簇索引 事务 事务的四种特性： 原子性：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 隔离性：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 事务的隔离级别： 未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 提交读(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 隔离级别 脏读（Dirty Read） 不可重复读（NonRepeatable Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能（innodb 不存在） 可串行化（Serializable ） 不可能 不可能 不可能 分库分表 水平： 水平分表 – 表之间结构相同 表之间数据不相同 所有表的数据并集是总数据（单表数据量很大，影响sql性能） 水平分库 – 库之间表结构相同 库之间数据不相同 所有库数据的并集是总数据（并发量很高，cpu 网络扛不住，分库缓解压力） 垂直： 垂直分表 – 表之间结构不相同，数据根据某个字段关联，缓解io性能 垂直分库 – 库之前的表之间结构不相同，服务压力很高 可以考虑拆出去做单独服务了 方案： 方案一（水平扩容库） 采用双倍扩容策略，避免数据迁移。扩容前每个节点的数据，有一半要迁移至一个新增节点中，对应关系比较简单。 具体操作如下(假设已有 2 个节点 A/B，要双倍扩容至 A/A2/B/B2 这 4 个节点)： 无需停止应用服务器； 新增两个数据库 A2/B2 作为从库，设置主从同步关系为：A=\u003eA2、B=\u003eB2，直至主从数据同步完毕(早期数据可手工同步)； 调整分片规则并使之生效： 原 ID%2=0 =\u003e A 改为 ID%4=0 =\u003e A, ID%4=2 =\u003e A2； 原 ID%2=1 =\u003e B 改为 ID%4=1 =\u003e B, ID%4=3 =\u003e B2。 解除数据库实例的主从同步关系，并使之生效； 此时，四个节点的数据都已完整，只是有冗余(多存了和自己配对的节点的那部分数据)，择机清除即可(过后随时进行，不影响业务)。 方案二（水平扩容表-双写） 第一步：（同步双写）修改应用配置和代码，加上双写，部署 第二步：（同步双写）将老库中的老数据复制到新库中 第三步：（同步双写）以老库为准校对新库中的老数据 第四步：（同步双写）修改应用配置和代码，去掉双写，部署； ","date":"2021-05-31","objectID":"/posts/go-interview/:1:1","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"Redis 数据结构 String 简单动态字符串 编码方式不同会有什么影响 Set 底层哈希表 ZSet member存在哈希表中 score 存在跳表里 查询插入时间复杂 logn 为什么用跳表 List 双向链表结构 Hmap 哈希表 性能 为什么这么快 为什么这么快2 数据均存在内存（引发出持久化问题） 高效的数据结构 单线程，省去线程间上下文切换的时间 以及不需要考虑锁 网络io 多路复用 可以让单个线程处理多个请求连接 减少网络io Redis采用自己实现的事件分离器，效率比较高，内部采用非阻塞的执行方式，吞吐能力比较大。 持久化 两种持久化： RDB持久化 即内存数据定时dump到磁盘上。 fork 一个子进程 将数据写入一个临时文件 写入成功后 替换源文件。 快照的数据是截止fork命令执行的那一刻 AOF 将Redis的操作日志以追加的方式写入文件。 将每一个写、删操作记录下来。默认配置时每秒同步一次。 RDB存在哪些优势呢？ 1). 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 2). 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 3). 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。 4). 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 RDB又存在哪些劣势呢？ 1). 如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。 2). 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。 AOF的优势有哪些呢？ 1). 该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。 2). 由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。 3). 如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。 4). AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。 AOF的劣势有哪些呢？ 1). 对于相同数量的数据集而言，AOF文件通常要大于RDB文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 2). 根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。 二者选择的标准，就是看系统是愿意牺牲一些性能，换取更高的缓存一致性（aof），还是愿意写操作频繁的时候，不启用备份来换取更高的性能，待手动运行save的时候，再做备份（rdb）。rdb这个就更有些 eventually consistent的意思了。 内存模型 内存模型 ","date":"2021-05-31","objectID":"/posts/go-interview/:1:2","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"MongoDB 待补充 ","date":"2021-05-31","objectID":"/posts/go-interview/:1:3","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"缓存 常见缓存策略 一致性哈希 解决某个缓存节点宕机的情况。 缓存穿透 描述：缓存穿透是指缓存和数据库中都没有的数据，而用户不断发起请求，如发起为id为“-1”的数据或id为特别大不存在的数据。这时的用户很可能是攻击者，攻击会导致数据库压力过大。 解决方案： 接口层增加校验，如用户鉴权校验，id做基础校验，id\u003c=0的直接拦截。 从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击。 布隆过滤器：将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。 布隆过滤器原理：对key进行多个(n)hash算法 并将其值与 bitArray 长度m 进行取模 并对应的位置置位1，当一个新的key进行查询时 先查询其n个hash算法后的各个位置是否为1 如果都为1 则这个key可能存在 如果有任意一个位置不是1 则这个key 一定不存在。 缓存击穿 描述：缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力 解决方案： 热点数据不做过期 互斥锁。如果数据缓存不存在 则先进行上锁读数据写缓存释放锁 缓存雪崩 描述：缓存雪崩是指缓存中数据大批量到过期时间，而查询数据量巨大，引起数据库压力过大甚至down机。和缓存击穿不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。 解决方案： 过期时间加随机数 热点数据不过期 分布式缓存 将热点数据拆分到不同的实例 ","date":"2021-05-31","objectID":"/posts/go-interview/:1:4","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"语言特性 ","date":"2021-05-31","objectID":"/posts/go-interview/:2:0","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"GMP 调度 调度2 概念 G：代表一个goroutine对象，每次go调用的时候，都会创建一个G对象，它包括栈、指令指针以及对于调用goroutines很重要的其它信息，比如阻塞它的任何channel，其主要数据结构： type g struct { stack stack // 描述了真实的栈内存，包括上下界 m *m // 当前的m sched gobuf // goroutine切换时，用于保存g的上下文 param unsafe.Pointer // 用于传递参数，睡眠时其他goroutine可以设置param，唤醒时该goroutine可以获取 atomicstatus uint32 stackLock uint32 goid int64 // goroutine的ID waitsince int64 // g被阻塞的大体时间 lockedm *m // G被锁定只在这个m上运行 } M: 代表内核线程(Pthread)，它本身就与一个内核线程进行绑定，goroutine 运行在M上。 type m struct { /* 1. 所有调用栈的Goroutine,这是一个比较特殊的Goroutine。 2. 普通的Goroutine栈是在Heap分配的可增长的stack,而g0的stack是M对应的线程栈。 3. 所有调度相关代码,会先切换到该Goroutine的栈再执行。 */ g0 *g curg *g // M当前绑定的结构体G // SP、PC寄存器用于现场保护和现场恢复 vdsoSP uintptr vdsoPC uintptr // 省略…} P：P(Processor)是一个抽象的概念，并不是真正的物理CPU。所以当P有任务时需要创建或者唤醒一个系统线程来执行它队列里的任务。所以P/M需要进行绑定，构成一个执行单元。 P决定了同时可以并发任务的数量，可通过GOMAXPROCS限制同时执行用户级任务的操作系统线程。可以通过runtime.GOMAXPROCS进行指定。在Go1.5之后GOMAXPROCS被默认设置可用的核数，而之前则默认为1。 // 自定义设置GOMAXPROCS数量 func GOMAXPROCS(n int) int { /* 1. GOMAXPROCS设置可执行的CPU的最大数量,同时返回之前的设置。 2. 如果n \u003c 1,则不更改当前的值。 */ ret := int(gomaxprocs) stopTheWorld(\"GOMAXPROCS\") // startTheWorld启动时,使用newprocs。 newprocs = int32(n) startTheWorld() return ret } // 默认P被绑定到所有CPU核上 // P == cpu.cores func getproccount() int32 { const maxCPUs = 64 * 1024 var buf [maxCPUs / 8]byte // 获取CPU Core r := sched_getaffinity(0, unsafe.Sizeof(buf), \u0026buf[0]) n := int32(0) for _, v := range buf[:r] { for v != 0 { n += int32(v \u0026 1) v \u003e\u003e= 1 } } if n == 0 { n = 1 } return n } // 一个进程默认被绑定在所有CPU核上,返回所有CPU core。 // 获取进程的CPU亲和性掩码系统调用 // rax 204 ; 系统调用码 // system_call sys_sched_getaffinity; 系统调用名称 // rid pid ; 进程号 // rsi unsigned int len // rdx unsigned long *user_mask_ptr sys_linux_amd64.s: TEXT runtime·sched_getaffinity(SB),NOSPLIT,$0 MOVQ pid+0(FP), DI MOVQ len+8(FP), SI MOVQ buf+16(FP), DX MOVL $SYS_sched_getaffinity, AX SYSCALL MOVL AX, ret+24(FP) RET 调度过程 首先创建一个G对象，G对象保存到P本地队列或者是全局队列。P此时去唤醒一个M。P继续执行它的执行序。M寻找是否有空闲的P，如果有则将该G对象移动到它本身。接下来M执行一个调度循环(调用G对象-\u003e执行-\u003e清理线程→继续找新的Goroutine执行)。 M执行过程中，随时会发生上下文切换。当发生上线文切换时，需要对执行现场进行保护，以便下次被调度执行时进行现场恢复。Go调度器M的栈保存在G对象上，只需要将M所需要的寄存器(SP、PC等)保存到G对象上就可以实现现场保护。当这些寄存器数据被保护起来，就随时可以做上下文切换了，在中断之前把现场保存起来。如果此时G任务还没有执行完，M可以将任务重新丢到P的任务队列，等待下一次被调度执行。当再次被调度执行时，M通过访问G的vdsoSP、vdsoPC寄存器进行现场恢复(从上次中断位置继续执行)。 多个线程下如何调度 抛出一个问题：每个P里面的G执行时间是不可控的，如果多个P同时在执行，会不会出现有的P里面的G执行不完，有的P里面几乎没有G可执行呢？ 这就要从M的自循环过程中如何获取G、归还G的行为说起了 有两种途径：1.借助全局队列 sched.runq 作为中介，本地P里的G太多的话就放全局里，G太少的话就从全局取。 2.全局列表里没有的话直接从P1里偷取(steal)。(更多M在执行的话，同样的原理，这里就只拿2个来举例) 调度循环中如何让出CPU 正常完成让出CPU 主动让出CPU time.Sleep(),IO阻塞等 抢占让出CPU 抢占式调度 概念：枚举所有的P 如果P在系统调用中(_Psyscall), 且经过了一次sysmon循环(20us~10ms), 则抢占这个P， 调用handoffp解除M和P之间的关联， 如果P在运行中(_Prunning), 且经过了一次sysmon循环并且G运行时间超过forcePreemptNS(10ms), 则抢占这个P 并设置g.preempt = true，g.stackguard0 = stackPreempt。 为什么设置了stackguard就可以实现抢占? 因为这个值用于检查当前栈空间是否足够, go函数的开头会比对这个值判断是否需要扩张栈。 newstack函数判断g.stackguard0等于stackPreempt, 就知道这是抢占触发的, 这时会再检查一遍是否要抢占。 抢占机制保证了不会有一个G长时间的运行导致其他G无法运行的情况发生。 主动让出CPU time.Sleep() timeSleep 函数里通过 addtimerLocked 把定时器加入到 timer 管理器（timer 通过最小堆的数据结构存放每个定时器，在这不做详细说明）后，再通过 goparkunlock 实现把当前G休眠，这里看到了上面提到的 gopark 方法进行调度循环的上下文切换。 在 addtimerLocked 方法的最下面有个逻辑在运行期间开启了’全局时间事件驱动器’timerproc,该方法会全程遍历最小堆，寻找最早进入 timer 管理器的定时器，然后唤醒。他是怎么找到要唤醒哪个G的？回头看下 timeSleep 方法里把当时正在执行的G以及唤醒方法 goroutineReady 带到了每个定时器里，而在 timerproc 则通过找到期的定时器执行f(arg, seq) 即通过 goroutineReady 方法唤醒。方法调用过程: goroutineReady() -\u003e ready() // runtime/time.go func timeSleep(ns int64) { if ns \u003c= 0 { return } t := getg().timer if t == nil { t = new(timer) getg().timer = t } *t = timer{} // 每个定时任务都创建一个timer t.when = nanotime() + ns t.f = goroutineReady // 记录唤醒该G的方法,唤醒时通过该方法执行唤醒 t.arg = getg() // 把timer与当前G关联,时间到了唤醒时通过该参数找到所在的G lock(\u0026timers.lock) addtimerLocked(t) // 把timer添加到最小堆里 goparkunlock(\u0026timers.lock, \"sleep\", traceEvGoSleep, 2) // 切到G0让出CPU,进入休眠 } 总结：time.Sleep 想要进入阻塞(休眠)状态，其实是通过 gopark 方法给自己标记个_Gwaiting 状态，然后把自己所占用的CPU线程资源给释放出来，继续执行调度任务，调度其它的G来运行。而唤醒是通过把G更改回_Grunnable 状态后，然后把G放入到P的待运行队列里等待执行。通过这点还可以看出休眠中的G其实并不占用 CPU 资源，最多是占用内存，是个很轻量级的阻塞。 Mutex Mutex.Lock 方法通过调用 runtime_SemacquireMutex 最终还是调用 goparkunlock 实现把G进入到休眠状态。在进入休眠","date":"2021-05-31","objectID":"/posts/go-interview/:2:1","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"垃圾回收 三色标记 哪些情况下不被垃圾回收？ 强三色和弱三色 写屏障 ","date":"2021-05-31","objectID":"/posts/go-interview/:2:2","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"channel ","date":"2021-05-31","objectID":"/posts/go-interview/:2:3","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"slice ","date":"2021-05-31","objectID":"/posts/go-interview/:2:4","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"map 怎么解决读写并发（除了锁） sync.Map 了解一下 ","date":"2021-05-31","objectID":"/posts/go-interview/:2:5","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"http库 ","date":"2021-05-31","objectID":"/posts/go-interview/:2:6","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"interface iface 有方法的interface{} eface 没有方法的interface{} ","date":"2021-05-31","objectID":"/posts/go-interview/:2:7","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"其他 变量逃逸 ","date":"2021-05-31","objectID":"/posts/go-interview/:2:8","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"项目经验 ","date":"2021-05-31","objectID":"/posts/go-interview/:3:0","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"微服务 ","date":"2021-05-31","objectID":"/posts/go-interview/:3:1","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"服务限流限速熔断 服务熔断 ","date":"2021-05-31","objectID":"/posts/go-interview/:3:2","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"提现个人能力的点 引入 etcd，基于 etcd 开发服务间通信的基础库 解决服务间通信和服务选举问题 引入nsq 修改源码 支持延迟消息持久化，二次开发 SDK 支持连接池 开发项目基础架构，一键生成新项目 开发基础 lib 包，wechat 包 ，common 包， eventbus-lib ，htlog-go 提高开发效率 规范化项目开发+上线流程，规范化架构 开发统一的内部消息服务，规范内服飞书/企业微信消息的发送 后台服务单点登录功能 服务拆解 向微服务方向改进 敏感词 建立词库结构（B-tree）黑白名单的缓存 自我介绍： 部门内的定位：后端开发+基础服务搭建 关注新技术 表现出持续学习 岗位匹配度 ","date":"2021-05-31","objectID":"/posts/go-interview/:3:3","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"系统设计 系统设计 ","date":"2021-05-31","objectID":"/posts/go-interview/:3:4","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"算法 github项目 ","date":"2021-05-31","objectID":"/posts/go-interview/:4:0","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"堆 最小堆/最大堆 topK 算法的实现： hash 加 小顶堆 堆排序 ","date":"2021-05-31","objectID":"/posts/go-interview/:4:1","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"链表 单向链表/双向链表 链表找环（快慢指针） 链表局部/全部旋转（即修改方向） 问题：查找倒数第 K 个节点 ","date":"2021-05-31","objectID":"/posts/go-interview/:4:2","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"二叉树 二叉树的五种遍历 前序 根-左-右 中序 左-根-右 后序 左-右-根 层次遍历 一层一层从左到右 锯齿遍历（s型遍历）每一层换方向 二叉树查找 查找最近路劲 查找共同祖先（最近祖先） 二叉树的转换 左右转换 其他 打印右视图左视图（即打印每一层的最右边或最左边） ","date":"2021-05-31","objectID":"/posts/go-interview/:4:3","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"回溯法递归法 理解回溯递归的每一层堆栈情况，学会什么情况下使用回溯/递归 ","date":"2021-05-31","objectID":"/posts/go-interview/:4:4","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"动态规划（DP） 找路线数量，爬台阶，背包问题 数组等和分组问题 ","date":"2021-05-31","objectID":"/posts/go-interview/:4:5","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"计算机基础 ","date":"2021-05-31","objectID":"/posts/go-interview/:5:0","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"基础概念 问题：查看当前服务器的性能\u0026查看 go 开的线程数？ 问题：进程线程协程的区别？ 问题：select poll epoll 的区别？ select 有大小限制 效率低 对 socket 是线性扫描 同步多路复用 O(n) poll 与 select 类似 但是用的链表结构 所以没有大小限制 同步多路复用 O(n) epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知我们。所以我们说epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。（复杂度降低到了O(1)） 相关链接 ","date":"2021-05-31","objectID":"/posts/go-interview/:5:1","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"tcp/udp tcp必备 问题：tcp time await 发生在哪端？ A: 发生在四次挥手时客户端，最后会等2MSL。 问题：为什么客户端最后还要等待2MSL？ 答：MSL（Maximum Segment Lifetime），TCP允许不同的实现可以设置不同的MSL值。第一，保证客户端发送的最后一个ACK报文能够到达服务器，因为这个ACK报文可能丢失。站在服务器的角度看来，我已经发送了FIN+ACK报文请求断开了，客户端还没有给我回应，应该是我发送的请求断开报文它没有收到，于是服务器又会重新发送一次，而客户端就能在这个2MSL时间段内收到这个重传的报文，接着给出回应报文，并且会重启2MSL计时器。如果客户端收到服务端的FIN+ACK报文后，发送一个ACK给服务端之后就“自私”地立马进入CLOSED状态，可能会导致服务端无法确认收到最后的ACK指令，也就无法进入CLOSED状态，这是客户端不负责任的表现。第二，防止失效请求。防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。客户端发送完最后一个确认报文后，在这个2MSL时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样新的连接中不会出现旧连接的请求报文。 在TIME_WAIT状态无法真正释放句柄资源，在此期间，Socket中使用的本地端口在默认情况下不能再被使用。该限制对于客户端机器来说是无所谓的，但对于高并发服务器来说，会极大地限制有效连接的创建数量，称为性能瓶颈。所以建议将高并发服务器TIME_WAIT超时时间调小。RFC793中规定MSL为2分钟。但是在当前的高速网络中，2分钟的等待时间会造成资源的极大浪费，在高并发服务器上通常会使用更小的值。 ","date":"2021-05-31","objectID":"/posts/go-interview/:5:2","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"http 谈谈HTTP https 相关 ","date":"2021-05-31","objectID":"/posts/go-interview/:5:3","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"grpc ","date":"2021-05-31","objectID":"/posts/go-interview/:5:4","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["面试"],"content":"其他QA Q: 解释graceful 平滑重启？ Q： ","date":"2021-05-31","objectID":"/posts/go-interview/:6:0","tags":["面试","经验"],"title":"Go 面试总结","uri":"/posts/go-interview/"},{"categories":["代码规范"],"content":"语言篇，提出常见的开发上的不好的、不规范的写法，并给出更好的写法。 Uber 是一家美国硅谷的科技公司，也是 Go 语言的早期 adopter。其开源了很多 golang 项目，诸如被 Gopher 圈熟知的 zap、jaeger 等。2018 年年末 Uber 将内部的 Go 风格规范 开源到 GitHub，经过一年的积累和更新，该规范已经初具规模，并受到广大 Gopher 的关注。本文是该规范的中文版本，并加以作者个人的一些看法，非 uber 官方的建议和看法 本人会加以标注。 ","date":"2020-11-25","objectID":"/posts/go-standard/:0:0","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"介绍 样式 (style) 是支配我们代码的惯例。术语样式有点用词不当，因为这些约定涵盖的范围不限于由 gofmt 替我们处理的源文件格式。 本指南的目的是通过详细描述在 Uber 编写 Go 代码的注意事项来管理这种复杂性。这些规则的存在是为了使代码库易于管理，同时仍然允许工程师更有效地使用 Go 语言功能。 该指南最初由 Prashant Varanasi 和 Simon Newton 编写，目的是使一些同事能快速使用 Go。多年来，该指南已根据其他人的反馈进行了修改。 本文档记录了我们在 Uber 遵循的 Go 代码中的惯用约定。其中许多是 Go 的通用准则，而其他扩展准则依赖于下面外部的指南： Effective Go The Go common mistakes guide 所有代码都应该通过golint和go vet的检查并无错误。我们建议您将编辑器设置为： 保存时运行 goimports 运行 golint 和 go vet 检查错误 您可以在以下 Go 编辑器工具支持页面中找到更为详细的信息： https://github.com/golang/go/wiki/IDEsAndTextEditorPlugins ","date":"2020-11-25","objectID":"/posts/go-standard/:1:0","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"指导原则 ","date":"2020-11-25","objectID":"/posts/go-standard/:2:0","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"指向 interface 的指针 您几乎不需要指向接口类型的指针。您应该将接口作为值进行传递，在这样的传递过程中，实质上传递的底层数据仍然可以是指针。 接口实质上在底层用两个字段表示： 一个指向某些特定类型信息的指针。您可以将其视为\"type\"。 数据指针。如果存储的数据是指针，则直接存储。如果存储的数据是一个值，则存储指向该值的指针。 如果希望接口方法修改基础数据，则必须使用指针传递(将对象指针赋值给接口变量)。 type F interface { f() } type S1 struct{} func (s S1) f() {} type S2 struct{} func (s *S2) f() {} // f1.f()无法修改底层数据 // f2.f() 可以修改底层数据,给接口变量f2赋值时使用的是对象指针 var f1 F:= S1{} var f2 F:= \u0026S2{} ","date":"2020-11-25","objectID":"/posts/go-standard/:2:1","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"Interface 合理性验证 在编译时验证接口的符合性。这包括： 将实现特定接口的导出类型作为接口API 的一部分进行检查 实现同一接口的(导出和非导出)类型属于实现类型的集合 任何违反接口合理性检查的场景,都会终止编译,并通知给用户 补充:上面3条是编译器对接口的检查机制, 大体意思是错误使用接口会在编译期报错. 所以可以利用这个机制让部分问题在编译期暴露. Bad Good // 如果Handler没有实现http.Handler,会在运行时报错 type Handler struct { // ... } func (h *Handler) ServeHTTP( w http.ResponseWriter, r *http.Request, ) { ... } type Handler struct { // ... } // 用于触发编译期的接口的合理性检查机制 // 如果Handler没有实现http.Handler,会在编译期报错 var _ http.Handler = (*Handler)(nil) func (h *Handler) ServeHTTP( w http.ResponseWriter, r *http.Request, ) { // ... } 如果 *Handler 与 http.Handler 的接口不匹配, 那么语句 var _ http.Handler = (*Handler)(nil) 将无法编译通过. 赋值的右边应该是断言类型的零值。 对于指针类型（如 *Handler）、切片和映射，这是 nil； 对于结构类型，这是空结构。 type LogHandler struct { h http.Handler log *zap.Logger } var _ http.Handler = LogHandler{} func (h LogHandler) ServeHTTP( w http.ResponseWriter, r *http.Request, ) { // ... } ","date":"2020-11-25","objectID":"/posts/go-standard/:2:2","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"接收器 (receiver) 与接口 使用值接收器的方法既可以通过值调用，也可以通过指针调用。 带指针接收器的方法只能通过指针或 addressable values调用. 例如， type S struct { data string } func (s S) Read() string { return s.data } func (s *S) Write(str string) { s.data = str } sVals := map[int]S{1: {\"A\"}} // 你只能通过值调用 Read sVals[1].Read() // 这不能编译通过： // sVals[1].Write(\"test\") sPtrs := map[int]*S{1: {\"A\"}} // 通过指针既可以调用 Read，也可以调用 Write 方法 sPtrs[1].Read() sPtrs[1].Write(\"test\") 类似的,即使方法有了值接收器,也同样可以用指针接收器来满足接口. type F interface { f() } type S1 struct{} func (s S1) f() {} type S2 struct{} func (s *S2) f() {} s1Val := S1{} s1Ptr := \u0026S1{} s2Val := S2{} s2Ptr := \u0026S2{} var i F i = s1Val i = s1Ptr i = s2Ptr // 下面代码无法通过编译。因为 s2Val 是一个值，而 S2 的 f 方法中没有使用值接收器 // i = s2Val Effective Go 中有一段关于 pointers vs. values 的精彩讲解。 补充: 一个类型可以有值接收器方法集和指针接收器方法集 值接收器方法集是指针接收器方法集的子集,反之不是 规则 值对象只可以使用值接收器方法集 指针对象可以使用 值接收器方法集 + 指针接收器方法集 接口的匹配(或者叫实现) 类型实现了接口的所有方法,叫匹配 具体的讲,要么是类型的值方法集匹配接口,要么是指针方法集匹配接口 具体的匹配分两种: 值方法集和接口匹配 给接口变量赋值的不管是值还是指针对象,都ok,因为都包含值方法集 指针方法集和接口匹配 只能将指针对象赋值给接口变量,因为只有指针方法集和接口匹配 如果将值对象赋值给接口变量,会在编译期报错(会触发接口合理性检查机制) 为啥 i = s2Val 会报错,因为值方法集和接口不匹配. ","date":"2020-11-25","objectID":"/posts/go-standard/:2:3","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"零值 Mutex 是有效的 零值 sync.Mutex 和 sync.RWMutex 是有效的。所以指向 mutex 的指针基本是不必要的。 BadGood mu := new(sync.Mutex) mu.Lock() var mu sync.Mutex mu.Lock() 如果你使用结构体指针，mutex 可以非指针形式作为结构体的组成字段，或者更好的方式是直接嵌入到结构体中。 如果是私有结构体类型或是要实现 Mutex 接口的类型，我们可以使用嵌入 mutex 的方法： type smap struct { sync.Mutex // only for unexported types（仅适用于非导出类型） data map[string]string } func newSMap() *smap { return \u0026smap{ data: make(map[string]string), } } func (m *smap) Get(k string) string { m.Lock() defer m.Unlock() return m.data[k] } type SMap struct { mu sync.Mutex // 对于导出类型，请使用私有锁 data map[string]string } func NewSMap() *SMap { return \u0026SMap{ data: make(map[string]string), } } func (m *SMap) Get(k string) string { m.mu.Lock() defer m.mu.Unlock() return m.data[k] } 为私有类型或需要实现互斥接口的类型嵌入。 对于导出的类型，请使用专用字段。 ","date":"2020-11-25","objectID":"/posts/go-standard/:2:4","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"在边界处拷贝 Slices 和 Maps slices 和 maps 包含了指向底层数据的指针，因此在需要复制它们时要特别注意。 接收 Slices 和 Maps 请记住，当 map 或 slice 作为函数参数传入时，如果您存储了对它们的引用，则用户可以对其进行修改。 Bad Good func (d *Driver) SetTrips(trips []Trip) { d.trips = trips } trips := ... d1.SetTrips(trips) // 你是要修改 d1.trips 吗？ trips[0] = ... func (d *Driver) SetTrips(trips []Trip) { d.trips = make([]Trip, len(trips)) copy(d.trips, trips) } trips := ... d1.SetTrips(trips) // 这里我们修改 trips[0]，但不会影响到 d1.trips trips[0] = ... 返回 slices 或 maps 同样，请注意用户对暴露内部状态的 map 或 slice 的修改。 BadGood type Stats struct { mu sync.Mutex counters map[string]int } // Snapshot 返回当前状态。 func (s *Stats) Snapshot() map[string]int { s.mu.Lock() defer s.mu.Unlock() return s.counters } // snapshot 不再受互斥锁保护 // 因此对 snapshot 的任何访问都将受到数据竞争的影响 // 影响 stats.counters snapshot := stats.Snapshot() type Stats struct { mu sync.Mutex counters map[string]int } func (s *Stats) Snapshot() map[string]int { s.mu.Lock() defer s.mu.Unlock() result := make(map[string]int, len(s.counters)) for k, v := range s.counters { result[k] = v } return result } // snapshot 现在是一个拷贝 snapshot := stats.Snapshot() ","date":"2020-11-25","objectID":"/posts/go-standard/:2:5","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"使用 defer 释放资源 使用 defer 释放资源，诸如文件和锁。 BadGood p.Lock() if p.count \u003c 10 { p.Unlock() return p.count } p.count++ newCount := p.count p.Unlock() return newCount // 当有多个 return 分支时，很容易遗忘 unlock p.Lock() defer p.Unlock() if p.count \u003c 10 { return p.count } p.count++ return p.count // 更可读 Defer 的开销非常小，只有在您可以证明函数执行时间处于纳秒级的程度时，才应避免这样做。使用 defer 提升可读性是值得的，因为使用它们的成本微不足道。尤其适用于那些不仅仅是简单内存访问的较大的方法，在这些方法中其他计算的资源消耗远超过 defer。 ","date":"2020-11-25","objectID":"/posts/go-standard/:2:6","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"Channel 的 size 要么是 1，要么是无缓冲的 channel 通常 size 应为 1 或是无缓冲的。默认情况下，channel 是无缓冲的，其 size 为零。任何其他尺寸都必须经过严格的审查。我们需要考虑如何确定大小，考虑是什么阻止了 channel 在高负载下和阻塞写时的写入，以及当这种情况发生时系统逻辑有哪些变化。(翻译解释：按照原文意思是需要界定通道边界，竞态条件，以及逻辑上下文梳理) BadGood // 应该足以满足任何情况！ c := make(chan int, 64) // 大小：1 c := make(chan int, 1) // 或者 // 无缓冲 channel，大小为 0 c := make(chan int) ","date":"2020-11-25","objectID":"/posts/go-standard/:2:7","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"枚举从 1 开始 在 Go 中引入枚举的标准方法是声明一个自定义类型和一个使用了 iota 的 const 组。由于变量的默认值为 0，因此通常应以非零值开头枚举。 BadGood type Operation int const ( Add Operation = iota Subtract Multiply ) // Add=0, Subtract=1, Multiply=2 type Operation int const ( Add Operation = iota + 1 Subtract Multiply ) // Add=1, Subtract=2, Multiply=3 在某些情况下，使用零值是有意义的（枚举从零开始），例如，当零值是理想的默认行为时。 type LogOutput int const ( LogToStdout LogOutput = iota LogToFile LogToRemote ) // LogToStdout=0, LogToFile=1, LogToRemote=2 ","date":"2020-11-25","objectID":"/posts/go-standard/:2:8","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"使用 time 处理时间 时间处理很复杂。关于时间的错误假设通常包括以下几点。 一天有 24 小时 一小时有 60 分钟 一周有七天 一年 365 天 还有更多 例如，1 表示在一个时间点上加上 24 小时并不总是产生一个新的日历日。 因此，在处理时间时始终使用 \"time\" 包，因为它有助于以更安全、更准确的方式处理这些不正确的假设。 使用 time.Time 表达瞬时时间 在处理时间的瞬间时使用 time.time，在比较、添加或减去时间时使用 time.Time 中的方法。 BadGood func isActive(now, start, stop int) bool { return start \u003c= now \u0026\u0026 now \u003c stop } func isActive(now, start, stop time.Time) bool { return (start.Before(now) || start.Equal(now)) \u0026\u0026 now.Before(stop) } 使用 time.Duration 表达时间段 在处理时间段时使用 time.Duration . BadGood func poll(delay int) { for { // ... time.Sleep(time.Duration(delay) * time.Millisecond) } } poll(10) // 是几秒钟还是几毫秒? func poll(delay time.Duration) { for { // ... time.Sleep(delay) } } poll(10*time.Second) 回到第一个例子，在一个时间瞬间加上 24 小时，我们用于添加时间的方法取决于意图。如果我们想要下一个日历日(当前天的下一天)的同一个时间点，我们应该使用 Time.AddDate。但是，如果我们想保证某一时刻比前一时刻晚 24 小时，我们应该使用 Time.Add。 newDay := t.AddDate(0 /* years */, 0, /* months */, 1 /* days */) maybeNewDay := t.Add(24 * time.Hour) 对外部系统使用 time.Time 和 time.Duration 尽可能在与外部系统的交互中使用 time.Duration 和 time.Time 例如 : Command-line 标志: flag 通过 time.ParseDuration 支持 time.Duration JSON: encoding/json 通过其 UnmarshalJSON method 方法支持将 time.Time 编码为 RFC 3339 字符串 SQL: database/sql 支持将 DATETIME 或 TIMESTAMP 列转换为 time.Time，如果底层驱动程序支持则返回 YAML: gopkg.in/yaml.v2 支持将 time.Time 作为 RFC 3339 字符串，并通过 time.ParseDuration 支持 time.Duration。 当不能在这些交互中使用 time.Duration 时，请使用 int 或 float64，并在字段名称中包含单位。 例如，由于 encoding/json 不支持 time.Duration，因此该单位包含在字段的名称中。 BadGood // {\"interval\": 2} type Config struct { Interval int `json:\"interval\"` } // {\"intervalMillis\": 2000} type Config struct { IntervalMillis int `json:\"intervalMillis\"` } 当在这些交互中不能使用 time.Time 时，除非达成一致，否则使用 string 和 RFC 3339 中定义的格式时间戳。默认情况下，Time.UnmarshalText 使用此格式，并可通过 time.RFC3339 在 Time.Format 和 time.Parse 中使用。 尽管这在实践中并不成问题，但请记住，\"time\" 包不支持解析闰秒时间戳（8728），也不在计算中考虑闰秒（15190）。如果您比较两个时间瞬间，则差异将不包括这两个瞬间之间可能发生的闰秒。 ","date":"2020-11-25","objectID":"/posts/go-standard/:2:9","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"错误类型 Go 中有多种声明错误（Error) 的选项： errors.New 对于简单静态字符串的错误 fmt.Errorf 用于格式化的错误字符串 实现 Error() 方法的自定义类型 用 \"pkg/errors\".Wrap 的 Wrapped errors 返回错误时，请考虑以下因素以确定最佳选择： 这是一个不需要额外信息的简单错误吗？如果是这样，errors.New 足够了。 客户需要检测并处理此错误吗？如果是这样，则应使用自定义类型并实现该 Error() 方法。 您是否正在传播下游函数返回的错误？如果是这样，请查看本文后面有关错误包装 section on error wrapping 部分的内容。 否则 fmt.Errorf 就可以了。 如果客户端需要检测错误，并且您已使用创建了一个简单的错误 errors.New，请使用一个错误变量。 BadGood // package foo func Open() error { return errors.New(\"could not open\") } // package bar func use() { if err := foo.Open(); err != nil { if err.Error() == \"could not open\" { // handle } else { panic(\"unknown error\") } } } // package foo var ErrCouldNotOpen = errors.New(\"could not open\") func Open() error { return ErrCouldNotOpen } // package bar if err := foo.Open(); err != nil { if err == foo.ErrCouldNotOpen { // handle } else { panic(\"unknown error\") } } 如果您有可能需要客户端检测的错误，并且想向其中添加更多信息（例如，它不是静态字符串），则应使用自定义类型。 BadGood func open(file string) error { return fmt.Errorf(\"file %q not found\", file) } func use() { if err := open(\"testfile.txt\"); err != nil { if strings.Contains(err.Error(), \"not found\") { // handle } else { panic(\"unknown error\") } } } type errNotFound struct { file string } func (e errNotFound) Error() string { return fmt.Sprintf(\"file %q not found\", e.file) } func open(file string) error { return errNotFound{file: file} } func use() { if err := open(\"testfile.txt\"); err != nil { if _, ok := err.(errNotFound); ok { // handle } else { panic(\"unknown error\") } } } 直接导出自定义错误类型时要小心，因为它们已成为程序包公共 API 的一部分。最好公开匹配器功能以检查错误。 // package foo type errNotFound struct { file string } func (e errNotFound) Error() string { return fmt.Sprintf(\"file %q not found\", e.file) } func IsNotFoundError(err error) bool { _, ok := err.(errNotFound) return ok } func Open(file string) error { return errNotFound{file: file} } // package bar if err := foo.Open(\"foo\"); err != nil { if foo.IsNotFoundError(err) { // handle } else { panic(\"unknown error\") } } ","date":"2020-11-25","objectID":"/posts/go-standard/:2:10","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"错误包装 (Error Wrapping) 一个（函数/方法）调用失败时，有三种主要的错误传播方式： 如果没有要添加的其他上下文，并且您想要维护原始错误类型，则返回原始错误。 添加上下文，使用 \"pkg/errors\".Wrap 以便错误消息提供更多上下文 ,\"pkg/errors\".Cause 可用于提取原始错误。 如果调用者不需要检测或处理的特定错误情况，使用 fmt.Errorf。 建议在可能的地方添加上下文，以使您获得诸如“调用服务 foo：连接被拒绝”之类的更有用的错误，而不是诸如“连接被拒绝”之类的模糊错误。 在将上下文添加到返回的错误时，请避免使用“failed to”之类的短语以保持上下文简洁，这些短语会陈述明显的内容，并随着错误在堆栈中的渗透而逐渐堆积： BadGood s, err := store.New() if err != nil { return fmt.Errorf( \"failed to create new store: %s\", err) } s, err := store.New() if err != nil { return fmt.Errorf( \"new store: %s\", err) } failed to x: failed to y: failed to create new store: the error x: y: new store: the error 但是，一旦将错误发送到另一个系统，就应该明确消息是错误消息（例如使用err标记，或在日志中以”Failed”为前缀）。 另请参见 Don’t just check errors, handle them gracefully. 不要只是检查错误，要优雅地处理错误 ","date":"2020-11-25","objectID":"/posts/go-standard/:2:11","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"处理类型断言失败 type assertion 的单个返回值形式针对不正确的类型将产生 panic。因此，请始终使用“comma ok”的惯用法。 BadGood t := i.(string) t, ok := i.(string) if !ok { // 优雅地处理错误 } ","date":"2020-11-25","objectID":"/posts/go-standard/:2:12","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"不要 panic 在生产环境中运行的代码必须避免出现 panic。panic 是 cascading failures 级联失败的主要根源 。如果发生错误，该函数必须返回错误，并允许调用方决定如何处理它。 BadGood func run(args []string) { if len(args) == 0 { panic(\"an argument is required\") } // ... } func main() { run(os.Args[1:]) } func run(args []string) error { if len(args) == 0 { return errors.New(\"an argument is required\") } // ... return nil } func main() { if err := run(os.Args[1:]); err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } } panic/recover 不是错误处理策略。仅当发生不可恢复的事情（例如：nil 引用）时，程序才必须 panic。程序初始化是一个例外：程序启动时应使程序中止的不良情况可能会引起 panic。 var _statusTemplate = template.Must(template.New(\"name\").Parse(\"_statusHTML\")) 即使在测试代码中，也优先使用t.Fatal或者t.FailNow而不是 panic 来确保失败被标记。 BadGood // func TestFoo(t *testing.T) f, err := ioutil.TempFile(\"\", \"test\") if err != nil { panic(\"failed to set up test\") } // func TestFoo(t *testing.T) f, err := ioutil.TempFile(\"\", \"test\") if err != nil { t.Fatal(\"failed to set up test\") } ","date":"2020-11-25","objectID":"/posts/go-standard/:2:13","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"使用 go.uber.org/atomic 使用 sync/atomic 包的原子操作对原始类型 (int32, int64等）进行操作，因为很容易忘记使用原子操作来读取或修改变量。 go.uber.org/atomic 通过隐藏基础类型为这些操作增加了类型安全性。此外，它包括一个方便的atomic.Bool类型。 BadGood type foo struct { running int32 // atomic } func (f* foo) start() { if atomic.SwapInt32(\u0026f.running, 1) == 1 { // already running… return } // start the Foo } func (f *foo) isRunning() bool { return f.running == 1 // race! } type foo struct { running atomic.Bool } func (f *foo) start() { if f.running.Swap(true) { // already running… return } // start the Foo } func (f *foo) isRunning() bool { return f.running.Load() } ","date":"2020-11-25","objectID":"/posts/go-standard/:2:14","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"避免可变全局变量 使用选择依赖注入方式避免改变全局变量。 既适用于函数指针又适用于其他值类型 BadGood // sign.go var _timeNow = time.Now func sign(msg string) string { now := _timeNow() return signWithTime(msg, now) } // sign.go type signer struct { now func() time.Time } func newSigner() *signer { return \u0026signer{ now: time.Now, } } func (s *signer) Sign(msg string) string { now := s.now() return signWithTime(msg, now) } // sign_test.go func TestSign(t *testing.T) { oldTimeNow := _timeNow _timeNow = func() time.Time { return someFixedTime } defer func() { _timeNow = oldTimeNow }() assert.Equal(t, want, sign(give)) } // sign_test.go func TestSigner(t *testing.T) { s := newSigner() s.now = func() time.Time { return someFixedTime } assert.Equal(t, want, s.Sign(give)) } ","date":"2020-11-25","objectID":"/posts/go-standard/:2:15","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"避免在公共结构中嵌入类型 这些嵌入的类型泄漏实现细节、禁止类型演化和模糊的文档。 假设您使用共享的 AbstractList 实现了多种列表类型，请避免在具体的列表实现中嵌入 AbstractList。 相反，只需手动将方法写入具体的列表，该列表将委托给抽象列表。 type AbstractList struct {} // 添加将实体添加到列表中。 func (l *AbstractList) Add(e Entity) { // ... } // 移除从列表中移除实体。 func (l *AbstractList) Remove(e Entity) { // ... } BadGood // ConcreteList 是一个实体列表。 type ConcreteList struct { *AbstractList } // ConcreteList 是一个实体列表。 type ConcreteList struct { list *AbstractList } // 添加将实体添加到列表中。 func (l *ConcreteList) Add(e Entity) { return l.list.Add(e) } // 移除从列表中移除实体。 func (l *ConcreteList) Remove(e Entity) { return l.list.Remove(e) } Go 允许 类型嵌入 作为继承和组合之间的折衷。 外部类型获取嵌入类型的方法的隐式副本。 默认情况下，这些方法委托给嵌入实例的同一方法。 结构还获得与类型同名的字段。 所以，如果嵌入的类型是 public，那么字段是 public。为了保持向后兼容性，外部类型的每个未来版本都必须保留嵌入类型。 很少需要嵌入类型。 这是一种方便，可以帮助您避免编写冗长的委托方法。 即使嵌入兼容的抽象列表 interface，而不是结构体，这将为开发人员提供更大的灵活性来改变未来，但仍然泄露了具体列表使用抽象实现的细节。 BadGood // AbstractList 是各种实体列表的通用实现。 type AbstractList interface { Add(Entity) Remove(Entity) } // ConcreteList 是一个实体列表。 type ConcreteList struct { AbstractList } // ConcreteList 是一个实体列表。 type ConcreteList struct { list *AbstractList } // 添加将实体添加到列表中。 func (l *ConcreteList) Add(e Entity) { return l.list.Add(e) } // 移除从列表中移除实体。 func (l *ConcreteList) Remove(e Entity) { return l.list.Remove(e) } 无论是使用嵌入式结构还是使用嵌入式接口，嵌入式类型都会限制类型的演化. 向嵌入式接口添加方法是一个破坏性的改变。 删除嵌入类型是一个破坏性的改变。 即使使用满足相同接口的替代方法替换嵌入类型，也是一个破坏性的改变。 尽管编写这些委托方法是乏味的，但是额外的工作隐藏了实现细节，留下了更多的更改机会，还消除了在文档中发现完整列表接口的间接性操作。 ","date":"2020-11-25","objectID":"/posts/go-standard/:2:16","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"避免使用内置名称 Go语言规范language specification 概述了几个内置的， 不应在Go项目中使用的名称标识predeclared identifiers。 根据上下文的不同，将这些标识符作为名称重复使用， 将在当前作用域（或任何嵌套作用域）中隐藏原始标识符，或者混淆代码。 在最好的情况下，编译器会报错；在最坏的情况下，这样的代码可能会引入潜在的、难以恢复的错误。 BadGood var error string // `error` 作用域隐式覆盖 // or func handleErrorMessage(error string) { // `error` 作用域隐式覆盖 } var errorMessage string // `error` 指向内置的非覆盖 // or func handleErrorMessage(msg string) { // `error` 指向内置的非覆盖 } type Foo struct { // 虽然这些字段在技术上不构成阴影，但`error`或`string`字符串的重映射现在是不明确的。 error error string string } func (f Foo) Error() error { // `error` 和 `f.error` 在视觉上是相似的 return f.error } func (f Foo) String() string { // `string` and `f.string` 在视觉上是相似的 return f.string } type Foo struct { // `error` and `string` 现在是明确的。 err error str string } func (f Foo) Error() error { return f.err } func (f Foo) String() string { return f.str } 注意，编译器在使用预先分隔的标识符时不会生成错误， 但是诸如go vet之类的工具会正确地指出这些和其他情况下的隐式问题。 ","date":"2020-11-25","objectID":"/posts/go-standard/:2:17","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"避免使用 init() 尽可能避免使用init()。当init()是不可避免或可取的，代码应先尝试： 无论程序环境或调用如何，都要完全确定。 避免依赖于其他init()函数的顺序或副作用。虽然init()顺序是明确的，但代码可以更改， 因此init()函数之间的关系可能会使代码变得脆弱和容易出错。 避免访问或操作全局或环境状态，如机器信息、环境变量、工作目录、程序参数/输入等。 避免I/O，包括文件系统、网络和系统调用。 不能满足这些要求的代码可能属于要作为main()调用的一部分（或程序生命周期中的其他地方）， 或者作为main()本身的一部分写入。特别是，打算由其他程序使用的库应该特别注意完全确定性， 而不是执行“init magic” BadGood type Foo struct { // ... } var _defaultFoo Foo func init() { _defaultFoo = Foo{ // ... } } var _defaultFoo = Foo{ // ... } // or, 为了更好的可测试性: var _defaultFoo = defaultFoo() func defaultFoo() Foo { return Foo{ // ... } } type Config struct { // ... } var _config Config func init() { // Bad: 基于当前目录 cwd, _ := os.Getwd() // Bad: I/O raw, _ := ioutil.ReadFile( path.Join(cwd, \"config\", \"config.yaml\"), ) yaml.Unmarshal(raw, \u0026_config) } type Config struct { // ... } func loadConfig() Config { cwd, err := os.Getwd() // handle err raw, err := ioutil.ReadFile( path.Join(cwd, \"config\", \"config.yaml\"), ) // handle err var config Config yaml.Unmarshal(raw, \u0026config) return config } 考虑到上述情况，在某些情况下，init()可能更可取或是必要的，可能包括： 不能表示为单个赋值的复杂表达式。 可插入的钩子，如database/sql、编码类型注册表等。 对Google Cloud Functions和其他形式的确定性预计算的优化。 ","date":"2020-11-25","objectID":"/posts/go-standard/:2:18","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"追加时优先指定切片容量 追加时优先指定切片容量 在尽可能的情况下，在初始化要追加的切片时为make()提供一个容量值。 BadGood for n := 0; n \u003c b.N; n++ { data := make([]int, 0) for k := 0; k \u003c size; k++{ data = append(data, k) } } for n := 0; n \u003c b.N; n++ { data := make([]int, 0, size) for k := 0; k \u003c size; k++{ data = append(data, k) } } BenchmarkBad-4 100000000 2.48s BenchmarkGood-4 100000000 0.21s ","date":"2020-11-25","objectID":"/posts/go-standard/:2:19","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"性能 性能方面的特定准则只适用于高频场景。 ","date":"2020-11-25","objectID":"/posts/go-standard/:3:0","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"优先使用 strconv 而不是 fmt 将原语转换为字符串或从字符串转换时，strconv速度比fmt快。 BadGood for i := 0; i \u003c b.N; i++ { s := fmt.Sprint(rand.Int()) } for i := 0; i \u003c b.N; i++ { s := strconv.Itoa(rand.Int()) } BenchmarkFmtSprint-4 143 ns/op 2 allocs/op BenchmarkStrconv-4 64.2 ns/op 1 allocs/op ","date":"2020-11-25","objectID":"/posts/go-standard/:3:1","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"避免字符串到字节的转换 不要反复从固定字符串创建字节 slice。相反，请执行一次转换并捕获结果。 BadGood for i := 0; i \u003c b.N; i++ { w.Write([]byte(\"Hello world\")) } data := []byte(\"Hello world\") for i := 0; i \u003c b.N; i++ { w.Write(data) } BenchmarkBad-4 50000000 22.2 ns/op BenchmarkGood-4 500000000 3.25 ns/op ","date":"2020-11-25","objectID":"/posts/go-standard/:3:2","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"指定容器容量 尽可能指定容器容量，以便为容器预先分配内存。这将在添加元素时最小化后续分配（通过复制和调整容器大小）。 指定Map容量提示 在尽可能的情况下，在使用 make() 初始化的时候提供容量信息 make(map[T1]T2, hint) 向make()提供容量提示会在初始化时尝试调整map的大小，这将减少在将元素添加到map时为map重新分配内存。 注意，与slices不同。map capacity提示并不保证完全的抢占式分配，而是用于估计所需的hashmap bucket的数量。 因此，在将元素添加到map时，甚至在指定map容量时，仍可能发生分配。 BadGood m := make(map[string]os.FileInfo) files, _ := ioutil.ReadDir(\"./files\") for _, f := range files { m[f.Name()] = f } files, _ := ioutil.ReadDir(\"./files\") m := make(map[string]os.FileInfo, len(files)) for _, f := range files { m[f.Name()] = f } m 是在没有大小提示的情况下创建的； 在运行时可能会有更多分配。 m 是有大小提示创建的；在运行时可能会有更少的分配。 指定切片容量 在尽可能的情况下，在使用make()初始化切片时提供容量信息，特别是在追加切片时。 make([]T, length, capacity) 与maps不同，slice capacity不是一个提示：编译器将为提供给make()的slice的容量分配足够的内存， 这意味着后续的append()`操作将导致零分配（直到slice的长度与容量匹配，在此之后，任何append都可能调整大小以容纳其他元素）。 BadGood for n := 0; n \u003c b.N; n++ { data := make([]int, 0) for k := 0; k \u003c size; k++{ data = append(data, k) } } for n := 0; n \u003c b.N; n++ { data := make([]int, 0, size) for k := 0; k \u003c size; k++{ data = append(data, k) } } BenchmarkBad-4 100000000 2.48s BenchmarkGood-4 100000000 0.21s ","date":"2020-11-25","objectID":"/posts/go-standard/:3:3","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"规范 ","date":"2020-11-25","objectID":"/posts/go-standard/:4:0","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"一致性 本文中概述的一些标准都是客观性的评估，是根据场景、上下文、或者主观性的判断； 但是最重要的是，保持一致. 一致性的代码更容易维护、是更合理的、需要更少的学习成本、并且随着新的约定出现或者出现错误后更容易迁移、更新、修复 bug 相反，在一个代码库中包含多个完全不同或冲突的代码风格会导致维护成本开销、不确定性和认知偏差。所有这些都会直接导致速度降低、代码审查痛苦、而且增加 bug 数量。 将这些标准应用于代码库时，建议在 package（或更大）级别进行更改，子包级别的应用程序通过将多个样式引入到同一代码中，违反了上述关注点。 ","date":"2020-11-25","objectID":"/posts/go-standard/:4:1","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"相似的声明放在一组 Go 语言支持将相似的声明放在一个组内。 BadGood import \"a\" import \"b\" import ( \"a\" \"b\" ) 这同样适用于常量、变量和类型声明： BadGood const a = 1 const b = 2 var a = 1 var b = 2 type Area float64 type Volume float64 const ( a = 1 b = 2 ) var ( a = 1 b = 2 ) type ( Area float64 Volume float64 ) 仅将相关的声明放在一组。不要将不相关的声明放在一组。 BadGood type Operation int const ( Add Operation = iota + 1 Subtract Multiply ENV_VAR = \"MY_ENV\" ) type Operation int const ( Add Operation = iota + 1 Subtract Multiply ) const ENV_VAR = \"MY_ENV\" 分组使用的位置没有限制，例如：你可以在函数内部使用它们： BadGood func f() string { var red = color.New(0xff0000) var green = color.New(0x00ff00) var blue = color.New(0x0000ff) ... } func f() string { var ( red = color.New(0xff0000) green = color.New(0x00ff00) blue = color.New(0x0000ff) ) ... } ","date":"2020-11-25","objectID":"/posts/go-standard/:4:2","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"import 分组 导入应该分为两组： 标准库 当前项目内的引用 第三方库 默认情况下，这是 goimports 应用的分组。 BadGood import ( \"fmt\" \"os\" \"go.uber.org/atomic\" \"golang.org/x/sync/errgroup\" \"currentProject/model\" \"currentProject/handler\" ) import ( \"fmt\" \"os\" \"currentProject/model\" \"currentProject/handler\" \"go.uber.org/atomic\" \"golang.org/x/sync/errgroup\" ) ","date":"2020-11-25","objectID":"/posts/go-standard/:4:3","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"包名 当命名包时，请按下面规则选择一个名称： 全部小写。没有大写或下划线。 大多数使用命名导入的情况下，不需要重命名。 简短而简洁。请记住，在每个使用的地方都完整标识了该名称。 不用复数。例如net/url，而不是net/urls。 不要用“common”，“util”，“shared”或“lib”。这些是不好的，信息量不足的名称。(应该使用更具体的命名方式 如httputil, mathutil 等。) 另请参阅 Package Names 和 Go 包样式指南. ","date":"2020-11-25","objectID":"/posts/go-standard/:4:4","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"函数名 我们遵循 Go 社区关于使用 MixedCaps 作为函数名 的约定。有一个例外，为了对相关的测试用例进行分组，函数名可能包含下划线，如：TestMyFunction_WhatIsBeingTested. ","date":"2020-11-25","objectID":"/posts/go-standard/:4:5","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"导入别名 如果程序包名称与导入路径的最后一个元素不匹配，则必须使用导入别名。 import ( \"net/http\" client \"example.com/client-go\" trace \"example.com/trace/v2\" ) 在所有其他情况下，除非导入之间有直接冲突，否则应避免导入别名。 BadGood import ( \"fmt\" \"os\" nettrace \"golang.net/x/trace\" ) import ( \"fmt\" \"os\" \"runtime/trace\" nettrace \"golang.net/x/trace\" ) ","date":"2020-11-25","objectID":"/posts/go-standard/:4:6","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"函数分组与顺序 函数应按粗略的调用顺序排序。 同一文件中的函数应按接收者分组。 因此，导出的函数应先出现在文件中，放在struct, const, var定义的后面。 在定义类型之后，但在接收者的其余方法之前，可能会出现一个 newXYZ()/NewXYZ() 由于函数是按接收者分组的，因此普通工具函数应在文件末尾出现。 BadGood func (s *something) Cost() { return calcCost(s.weights) } type something struct{ ... } func calcCost(n []int) int {...} func (s *something) Stop() {...} func newSomething() *something { return \u0026something{} } type something struct{ ... } func newSomething() *something { return \u0026something{} } func (s *something) Cost() { return calcCost(s.weights) } func (s *something) Stop() {...} func calcCost(n []int) int {...} ","date":"2020-11-25","objectID":"/posts/go-standard/:4:7","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"减少嵌套 代码应通过尽可能先处理错误情况/特殊情况并尽早返回或继续循环来减少嵌套。减少嵌套多个级别的代码的代码量。 BadGood for _, v := range data { if v.F1 == 1 { v = process(v) if err := v.Call(); err == nil { v.Send() } else { return err } } else { log.Printf(\"Invalid v: %v\", v) } } for _, v := range data { if v.F1 != 1 { log.Printf(\"Invalid v: %v\", v) continue } v = process(v) if err := v.Call(); err != nil { return err } v.Send() } ","date":"2020-11-25","objectID":"/posts/go-standard/:4:8","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"不必要的 else 如果在 if 的两个分支中都设置了变量，则可以将其替换为单个 if。 BadGood var a int if b { a = 100 } else { a = 10 } a := 10 if b { a = 100 } ","date":"2020-11-25","objectID":"/posts/go-standard/:4:9","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"顶层变量声明 在顶层，使用标准var关键字。请勿指定类型，除非它与表达式的类型不同。 BadGood var _s string = F() func F() string { return \"A\" } var _s = F() // 由于 F 已经明确了返回一个字符串类型，因此我们没有必要显式指定_s 的类型 // 还是那种类型 func F() string { return \"A\" } 如果表达式的类型与所需的类型不完全匹配，请指定类型。 type myError struct{} func (myError) Error() string { return \"error\" } func F() myError { return myError{} } var _e error = F() // F 返回一个 myError 类型的实例，但是我们要 error 类型 ","date":"2020-11-25","objectID":"/posts/go-standard/:4:10","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"对于未导出的顶层常量和变量，使用_作为前缀 在未导出的顶级vars和consts， 前面加上前缀_，以使它们在使用时明确表示它们是全局符号。 例外：未导出的错误值，应以err开头。 基本依据：顶级变量和常量具有包范围作用域。使用通用名称可能很容易在其他文件中意外使用错误的值。 BadGood // foo.go const ( defaultPort = 8080 defaultUser = \"user\" ) // bar.go func Bar() { defaultPort := 9090 ... fmt.Println(\"Default port\", defaultPort) // We will not see a compile error if the first line of // Bar() is deleted. } // foo.go const ( _defaultPort = 8080 _defaultUser = \"user\" ) ","date":"2020-11-25","objectID":"/posts/go-standard/:4:11","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"结构体中的嵌入 嵌入式类型（例如 mutex）应位于结构体内的字段列表的顶部，并且必须有一个空行将嵌入式字段与常规字段分隔开。 BadGood type Client struct { version int http.Client } type Client struct { http.Client version int } 内嵌应该提供切实的好处，比如以语义上合适的方式添加或增强功能。 它应该在对用户不利影响的情况下完成这项工作（另请参见：避免在公共结构中嵌入类型Avoid Embedding Types in Public Structs）。 嵌入 不应该: 纯粹是为了美观或方便。 使外部类型更难构造或使用。 影响外部类型的零值。如果外部类型有一个有用的零值，则在嵌入内部类型之后应该仍然有一个有用的零值。 作为嵌入内部类型的副作用，从外部类型公开不相关的函数或字段。 公开未导出的类型。 影响外部类型的复制形式。 更改外部类型的API或类型语义。 嵌入内部类型的非规范形式。 公开外部类型的实现详细信息。 允许用户观察或控制类型内部。 通过包装的方式改变内部函数的一般行为，这种包装方式会给用户带来一些意料之外情况。 简单地说，有意识地和有目的地嵌入。一种很好的测试体验是， “是否所有这些导出的内部方法/字段都将直接添加到外部类型” 如果答案是some或no，不要嵌入内部类型-而是使用字段。 BadGood type A struct { // Bad: A.Lock() and A.Unlock() 现在可用 // 不提供任何功能性好处，并允许用户控制有关A的内部细节。 sync.Mutex } type countingWriteCloser struct { // Good: Write() 在外层提供用于特定目的， // 并且委托工作到内部类型的Write()中。 io.WriteCloser count int } func (w *countingWriteCloser) Write(bs []byte) (int, error) { w.count += len(bs) return w.WriteCloser.Write(bs) } type Book struct { // Bad: 指针更改零值的有用性 io.ReadWriter // other fields } // later var b Book b.Read(...) // panic: nil pointer b.String() // panic: nil pointer b.Write(...) // panic: nil pointer type Book struct { // Good: 有用的零值 bytes.Buffer // other fields } // later var b Book b.Read(...) // ok b.String() // ok b.Write(...) // ok type Client struct { sync.Mutex sync.WaitGroup bytes.Buffer url.URL } type Client struct { mtx sync.Mutex wg sync.WaitGroup buf bytes.Buffer url url.URL } ","date":"2020-11-25","objectID":"/posts/go-standard/:4:12","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"使用字段名初始化结构体 初始化结构体时，应该指定字段名称。现在由 go vet 强制执行。 BadGood k := User{\"John\", \"Doe\", true} k := User{ FirstName: \"John\", LastName: \"Doe\", Admin: true, } 例外：如果有 3 个或更少的字段，则可以在测试表中省略字段名称。 tests := []struct{ op Operation want string }{ {Add, \"add\"}, {Subtract, \"subtract\"}, } ","date":"2020-11-25","objectID":"/posts/go-standard/:4:13","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"本地变量声明 如果将变量明确设置为某个值，则应使用短变量声明形式 (:=)。 BadGood var s = \"foo\" s := \"foo\" 但是，在某些情况下，var 使用关键字时默认值会更清晰。例如，声明空切片。 BadGood func f(list []int) { filtered := []int{} for _, v := range list { if v \u003e 10 { filtered = append(filtered, v) } } } func f(list []int) { var filtered []int for _, v := range list { if v \u003e 10 { filtered = append(filtered, v) } } } ","date":"2020-11-25","objectID":"/posts/go-standard/:4:14","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"nil 是一个有效的 slice nil 是一个有效的长度为 0 的 slice，这意味着， 您不应明确返回长度为零的切片。应该返回nil 来代替。 BadGood if x == \"\" { return []int{} } if x == \"\" { return nil } 要检查切片是否为空，请始终使用len(s) == 0。而非 nil。 BadGood func isEmpty(s []string) bool { return s == nil } func isEmpty(s []string) bool { return len(s) == 0 } 零值切片（用var声明的切片）可立即使用，无需调用make()创建。 BadGood nums := []int{} // or, nums := make([]int) if add1 { nums = append(nums, 1) } if add2 { nums = append(nums, 2) } var nums []int if add1 { nums = append(nums, 1) } if add2 { nums = append(nums, 2) } 记住，虽然nil切片是有效的切片，但它不等于长度为0的切片（一个为nil，另一个不是），并且在不同的情况下（例如序列化），这两个切片的处理方式可能不同。 ","date":"2020-11-25","objectID":"/posts/go-standard/:4:15","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"缩小变量作用域 如果有可能，尽量缩小变量作用范围。除非它与 减少嵌套的规则冲突。 BadGood err := ioutil.WriteFile(name, data, 0644) if err != nil { return err } if err := ioutil.WriteFile(name, data, 0644); err != nil { return err } 如果需要在 if 之外使用函数调用的结果，则不应尝试缩小范围。 BadGood if data, err := ioutil.ReadFile(name); err == nil { err = cfg.Decode(data) if err != nil { return err } fmt.Println(cfg) return nil } else { return err } data, err := ioutil.ReadFile(name) if err != nil { return err } if err := cfg.Decode(data); err != nil { return err } fmt.Println(cfg) return nil ","date":"2020-11-25","objectID":"/posts/go-standard/:4:16","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"避免参数语义不明确(Avoid Naked Parameters) 函数调用中的意义不明确的参数可能会损害可读性。当参数名称的含义不明显时，请为参数添加 C 样式注释 (/* ... */) BadGood // func printInfo(name string, isLocal, done bool) printInfo(\"foo\", true, true) // func printInfo(name string, isLocal, done bool) printInfo(\"foo\", true /* isLocal */, true /* done */) 对于上面的示例代码，还有一种更好的处理方式是将上面的 bool 类型换成自定义类型。将来，该参数可以支持不仅仅局限于两个状态（true/false）。 type Region int const ( UnknownRegion Region = iota Local ) type Status int const ( StatusReady Status= iota + 1 StatusDone // Maybe we will have a StatusInProgress in the future. ) func printInfo(name string, region Region, status Status) ","date":"2020-11-25","objectID":"/posts/go-standard/:4:17","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"使用原始字符串字面值，避免转义 Go 支持使用 原始字符串字面值，也就是 \" ` \" 来表示原生字符串，在需要转义的场景下，我们应该尽量使用这种方案来替换。 可以跨越多行并包含引号。使用这些字符串可以避免更难阅读的手工转义的字符串。 BadGood wantError := \"unknown name:\\\"test\\\"\" wantError := `unknown error:\"test\"` ","date":"2020-11-25","objectID":"/posts/go-standard/:4:18","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"初始化 Struct 引用 在初始化结构引用时，请使用\u0026T{}代替new(T)，以使其与结构体初始化一致。 BadGood sval := T{Name: \"foo\"} // inconsistent sptr := new(T) sptr.Name = \"bar\" sval := T{Name: \"foo\"} sptr := \u0026T{Name: \"bar\"} ","date":"2020-11-25","objectID":"/posts/go-standard/:4:19","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"初始化 Maps 对于空 map 请使用 make(..) 初始化， 并且 map 是通过编程方式填充的。 这使得 map 初始化在表现上不同于声明，并且它还可以方便地在 make 后添加大小提示。 BadGood var ( // m1 读写安全; // m2 在写入时会 panic m1 = map[T1]T2{} m2 map[T1]T2 ) var ( // m1 读写安全; // m2 在写入时会 panic m1 = make(map[T1]T2) m2 map[T1]T2 ) 声明和初始化看起来非常相似的。 声明和初始化看起来差别非常大。 在尽可能的情况下，请在初始化时提供 map 容量大小，详细请看 指定Map容量提示。 另外，如果 map 包含固定的元素列表，则使用 map literals(map 初始化列表) 初始化映射。 BadGood m := make(map[T1]T2, 3) m[k1] = v1 m[k2] = v2 m[k3] = v3 m := map[T1]T2{ k1: v1, k2: v2, k3: v3, } 基本准则是：在初始化时使用 map 初始化列表 来添加一组固定的元素。否则使用 make (如果可以，请尽量指定 map 容量)。 ","date":"2020-11-25","objectID":"/posts/go-standard/:4:20","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"编程模式 ","date":"2020-11-25","objectID":"/posts/go-standard/:5:0","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"表驱动测试 当测试逻辑是重复的时候，通过 subtests 使用 table 驱动的方式编写 case 代码看上去会更简洁。而且目前编译器可以通过快捷键快速生成单元测试方法，可以帮助养成良好习惯。 BadGood // func TestSplitHostPort(t *testing.T) host, port, err := net.SplitHostPort(\"192.0.2.0:8000\") require.NoError(t, err) assert.Equal(t, \"192.0.2.0\", host) assert.Equal(t, \"8000\", port) host, port, err = net.SplitHostPort(\"192.0.2.0:http\") require.NoError(t, err) assert.Equal(t, \"192.0.2.0\", host) assert.Equal(t, \"http\", port) host, port, err = net.SplitHostPort(\":8000\") require.NoError(t, err) assert.Equal(t, \"\", host) assert.Equal(t, \"8000\", port) host, port, err = net.SplitHostPort(\"1:8\") require.NoError(t, err) assert.Equal(t, \"1\", host) assert.Equal(t, \"8\", port) // func TestSplitHostPort(t *testing.T) tests := []struct{ give string wantHost string wantPort string }{ { give: \"192.0.2.0:8000\", wantHost: \"192.0.2.0\", wantPort: \"8000\", }, { give: \"192.0.2.0:http\", wantHost: \"192.0.2.0\", wantPort: \"http\", }, { give: \":8000\", wantHost: \"\", wantPort: \"8000\", }, { give: \"1:8\", wantHost: \"1\", wantPort: \"8\", }, } for _, tt := range tests { t.Run(tt.give, func(t *testing.T) { host, port, err := net.SplitHostPort(tt.give) require.NoError(t, err) assert.Equal(t, tt.wantHost, host) assert.Equal(t, tt.wantPort, port) }) } 很明显，使用 test table 的方式在代码逻辑扩展的时候，比如新增 test case，都会显得更加的清晰。 我们遵循这样的约定：将结构体切片称为tests。 每个测试用例称为tt。此外，我们鼓励使用give和want前缀说明每个测试用例的输入和输出值。 tests := []struct{ give string wantHost string wantPort string }{ // ... } for _, tt := range tests { // ... } ","date":"2020-11-25","objectID":"/posts/go-standard/:5:1","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"功能选项 功能选项是一种模式，您可以在其中声明一个不透明 Option 类型，该类型在某些内部结构中记录信息。您接受这些选项的可变编号，并根据内部结构上的选项记录的全部信息采取行动。 将此模式用于您需要扩展的构造函数和其他公共 API 中的可选参数，尤其是在这些功能上已经具有三个或更多参数的情况下。 BadGood // package db func Open( addr string, cache bool, logger *zap.Logger ) (*Connection, error) { // ... } // package db type Option interface { // ... } func WithCache(c bool) Option { // ... } func WithLogger(log *zap.Logger) Option { // ... } // Open creates a connection. func Open( addr string, opts ...Option, ) (*Connection, error) { // ... } 必须始终提供缓存和记录器参数，即使用户希望使用默认值。 db.Open(addr, db.DefaultCache, zap.NewNop()) db.Open(addr, db.DefaultCache, log) db.Open(addr, false /* cache */, zap.NewNop()) db.Open(addr, false /* cache */, log) 只有在需要时才提供选项。 db.Open(addr) db.Open(addr, db.WithLogger(log)) db.Open(addr, db.WithCache(false)) db.Open( addr, db.WithCache(false), db.WithLogger(log), ) Our suggested way of implementing this pattern is with an Option interface that holds an unexported method, recording options on an unexported options struct. 我们建议实现此模式的方法是使用一个 Option 接口，该接口保存一个未导出的方法，在一个未导出的 options 结构上记录选项。 type options struct { cache bool logger *zap.Logger } type Option interface { apply(*options) } type cacheOption bool func (c cacheOption) apply(opts *options) { opts.cache = bool(c) } func WithCache(c bool) Option { return cacheOption(c) } type loggerOption struct { Log *zap.Logger } func (l loggerOption) apply(opts *options) { opts.logger = l.Log } func WithLogger(log *zap.Logger) Option { return loggerOption{Log: log} } // Open creates a connection. func Open( addr string, opts ...Option, ) (*Connection, error) { options := options{ cache: defaultCache, logger: zap.NewNop(), } for _, o := range opts { o.apply(\u0026options) } // ... } 注意: 还有一种使用闭包实现这个模式的方法，但是我们相信上面的模式为作者提供了更多的灵活性，并且更容易对用户进行调试和测试。特别是，在不可能进行比较的情况下它允许在测试和模拟中对选项进行比较。此外，它还允许选项实现其他接口，包括 fmt.Stringer，允许用户读取选项的字符串表示形式。 还可以参考下面资料： Self-referential functions and the design of options Functional options for friendly APIs ","date":"2020-11-25","objectID":"/posts/go-standard/:5:2","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["代码规范"],"content":"Linting 比任何 “blessed” linter 集更重要的是，lint在一个代码库中始终保持一致。 建议至少使用以下linters，因为我认为它们有助于发现最常见的问题，并在不需要规定的情况下为代码质量建立一个高标准： errcheck 以确保错误得到处理 goimports 格式化代码和管理 imports golint 指出常见的文体错误 govet 分析代码中的常见错误 staticcheck 各种静态分析检查 ","date":"2020-11-25","objectID":"/posts/go-standard/:6:0","tags":["go"],"title":"Go 语言开发及常用库的使用规范(语言篇)","uri":"/posts/go-standard/"},{"categories":["源码解读"],"content":"Go 的 map 作为该语言最常见的基础数据结构之一。 ","date":"2020-06-14","objectID":"/posts/go-map/:0:0","tags":["go"],"title":"Go Map 源码解读","uri":"/posts/go-map/"},{"categories":["源码解读"],"content":"源码解读 Go 语言实现的 map 并非是完全的哈希 map ，是一种类似两层树状的结构，根据 key 的哈希值的低八位 决定第一层的位置，根据高八位决定第二层，如果第二层所在冲突了则会有一个额外的位置 用于存储哈希碰撞的 kv。看图会帮助理解： ","date":"2020-06-14","objectID":"/posts/go-map/:1:0","tags":["go"],"title":"Go Map 源码解读","uri":"/posts/go-map/"},{"categories":["源码解读"],"content":"图解： ","date":"2020-06-14","objectID":"/posts/go-map/:1:1","tags":["go"],"title":"Go Map 源码解读","uri":"/posts/go-map/"},{"categories":["源码解读"],"content":"数据结构 源码在 go/src/runtime/map.go 文件中： // map 的实现 type hmap struct { count int // 已使用位置数（即 len() 方法会返回该值），之所以说已使用的是因为并非所有的位置都存放位置 flags uint8 // map的状态，通过该字段判断当前是否被某个进程进行写操作 B uint8 // 2^B 为桶的数量， B为 3 时 2^3 一共 8 个桶 noverflow uint16 // 溢出的桶数量 hash0 uint32 // hash seed buckets unsafe.Pointer // 桶的数组 oldbuckets unsafe.Pointer // 旧桶的数组。map 扩容时 原 buckets 变成 oldbuckets 并将数据逐步迁移，并非一次性迁移 nevacuate uintptr // 扩容进度记录 extra *mapextra // 额外信息。存储非指针数据（为了优化空间） } type mapextra struct { // 为了优化空间 将非指针数据存储在 mapextra里 overflow *[]*bmap // 对应 hmap.buckets oldoverflow *[]*bmap // 对应 hmap.oldbuckets // 指向下一个空闲的 bucket nextOverflow *bmap } // bucket 即桶 type bmap struct { // tophash 存储每个 key 的 tophash 即 key 的前八位，用于判断读取的 key 是否在当前桶里。 tophash [bucketCnt]uint8 // 之后是 key-value 的格子，每个桶最多只能存 8 个且 以 key1...key8value1...value8 的形式存储。 // 还有一个 overflow 用于指向下一个桶。 } ","date":"2020-06-14","objectID":"/posts/go-map/:1:2","tags":["go"],"title":"Go Map 源码解读","uri":"/posts/go-map/"},{"categories":["源码解读"],"content":"读取 按 key 读取 遍历 ","date":"2020-06-14","objectID":"/posts/go-map/:1:3","tags":["go"],"title":"Go Map 源码解读","uri":"/posts/go-map/"},{"categories":["源码解读"],"content":"写入 ","date":"2020-06-14","objectID":"/posts/go-map/:1:4","tags":["go"],"title":"Go Map 源码解读","uri":"/posts/go-map/"},{"categories":["源码解读"],"content":"删除 coming soon ","date":"2020-06-14","objectID":"/posts/go-map/:1:5","tags":["go"],"title":"Go Map 源码解读","uri":"/posts/go-map/"},{"categories":null,"content":"技术忠实爱好者，喜欢运动，喜欢与大自然近距离接触。 目前主要经历在微服务、服务架构、服务治理方向 工作内容目前偏向于 Kubernetes 组件开发、网关、弹性伸缩等 有过实际高并发场景的经验 喜欢分享愿意分享 关于我： 邮箱：yusankurban@gmail.com 坐标：北京 微信：yusank77 欢迎各路大神关注公众号，提出宝贵的意见，谢谢~ 公众号二维码 ","date":"2020-03-09","objectID":"/about/:0:0","tags":null,"title":"About me","uri":"/about/"},{"categories":["源码解读"],"content":"Go 的 channel 作为该语言很重要的特性，作为一个 gopher 有必要详细了解其实现原理。 原理解读 Go 语言的 channel 实现源码在go/src/runtime/chan.go 文件里。（go version ：1.13.4） ","date":"2020-03-06","objectID":"/posts/go-channel/:0:0","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"数据结构 首先看一下基础数据结构： // go 语言的 channel 结构以队列的形式实现 type hchan struct { qcount uint // total data in the queue，队列中元素总数 dataqsiz uint // size of the circular queue，循环队列的大小 buf unsafe.Pointer // points to an array of dataqsiz elements， 指向循环队列中元素的指针 elemsize uint16 // 元素 size closed uint32 // channel 是否关闭标志 elemtype *_type // element type // channel 元素类型 sendx uint // send index // 写入 channel 元素的索引 recvx uint // receive index // 从 channel 读取的元素索引 recvq waitq // list of recv waiters // 读取 channel 的等待队列（即阻塞的协程） sendq waitq // list of send waiters // 写入 channel 的等待队列 // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. lock mutex // 互斥锁 } // 双向链表结构，其中每一个元素代表着等待读取或写入 channel 的协程 type waitq struct { first *sudog last *sudog } 通过源码数据结构，对 go 的 channel 实现有了初步的了解，解答了在我们读取或写入 channel 时，其中元素在哪儿，我们的协程在哪儿等待等数据相关问题。 channel 底层实现是以队列作为载体，通过互斥锁保证在同一个时间点，只有一个待读取的协程读元素或待写入的协程写入元素。 如果有多个协程同时读取 channel 时，他们会进入读取等待队列：recvq，反之进入写入等待队列：sendq。 buf 作为指针，指向 channel 中存储元素的数组的地址。 sendx,recvx 作为channel 队列中写入和读取到元素的索引值。 closed 为 channel 当前是否已被关闭标志。 ","date":"2020-03-06","objectID":"/posts/go-channel/:1:0","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"主要方法（func） 以我们常用的 make(chan Type), 写入元素(chan \u003c- element)和读取元素(\u003c-chan)为例 ","date":"2020-03-06","objectID":"/posts/go-channel/:2:0","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"初始化（make） 在实际使用中 我会用下面的代码初始化一个 channel： make(chan Type, size int) 其实现源码入下： // t 为 channel 类型，size 为我们传入 channel 大小 func makechan(t *chantype, size int) *hchan { elem := t.elem // 如果 size 超过声明类型最大值 编译的时候会报错，但是这里多一次判断为了更安全 if elem.size \u003e= 1\u003c\u003c16 { // 抛出异常 throw(\"makechan: invalid channel element type\") } // align 为类型的对齐系数，不同平台上对其系数不完全一样，但是都最大值 maxAlign=8 // 不同类型的对齐系数不一样 但是均以 2^N 形式 if hchanSize%maxAlign != 0 || elem.align \u003e maxAlign { throw(\"makechan: bad alignment\") } // 检查是否channel 大小值是否溢出 mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem \u003e maxAlloc-hchanSize || size \u003c 0 { panic(plainError(\"makechan: size out of range\")) } // 根据 size 和原始是否为指针情况，分配内存初始化 channel var c *hchan switch { // channel size 为 0 case mem == 0: c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr() case elem.ptrdata == 0: // 元素不包含指针，则将为元素分配内存，并将 buf 指向该地址 c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: // 元素包含指针，buf 指向该指针指向地址 c = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) return c } 可以看出，channel 中的元素最终都是以指针的方式存储，即便初始化时 用非指针类型（如 string），在初始化话的时候 会先分配内存 并将 channel 的元素指针字段指向该地址。 ","date":"2020-03-06","objectID":"/posts/go-channel/:2:1","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"写入 先给出源码： // entry point for c \u003c- x from compiled code // 代码重 `c \u003c- x` 编译时，会编译成该方法从而被调用 func chansend1(c *hchan, elem unsafe.Pointer) { chansend(c, elem, true, getcallerpc()) } /* * generic single channel send/recv * If block is not nil, * then the protocol will not * sleep but return if it could * not complete. * * sleep can wake up with g.param == nil * when a channel involved in the sleep has * been closed. it is easiest to loop and re-run * the operation; we'll see that it's now closed. */ // 向 channel 写入 // c: channel // ep: 写入元素地址 // block: 表示该 channel 是否被阻塞 // callerpc: func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { if c == nil { // return or panic } if raceenabled { // 不同协程之前竞争写入 racereadpc(c.raceaddr(), callerpc, funcPC(chansend)) } // 没有阻塞 \u0026\u0026 未关闭 \u0026\u0026 （channel 为空且没有协程读取 或 channel 已满，直接返回 false） if !block \u0026\u0026 c.closed == 0 \u0026\u0026 ((c.dataqsiz == 0 \u0026\u0026 c.recvq.first == nil) || (c.dataqsiz \u003e 0 \u0026\u0026 c.qcount == c.dataqsiz)) { return false } var t0 int64 if blockprofilerate \u003e 0 { t0 = cputicks() } // 上锁 准备写 lock(\u0026c.lock) // 已关闭 解锁并 panic if c.closed != 0 { unlock(\u0026c.lock) panic(plainError(\"send on closed channel\")) } // 从等待读取的队列中 拿出第一个协程，写入并发送到该协程 if sg := c.recvq.dequeue(); sg != nil { // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). send(c, sg, ep, func() { unlock(\u0026c.lock) }, 3) return true } // 如果 channel 缓存有空间，则向缓存中写入 // 此时是 channel 是有 buffer channel if c.qcount \u003c c.dataqsiz { // Space is available in the channel buffer. Enqueue the element to send. qp := chanbuf(c, c.sendx) // 应该是协程之间竞争，暂时没有完全搞懂 if raceenabled { raceacquire(qp) racerelease(qp) } // 写入缓存 typedmemmove(c.elemtype, qp, ep) // 写入位置加一 c.sendx++ // 如果写完 buffer 满了，将位置置位 0 if c.sendx == c.dataqsiz { c.sendx = 0 } // channel 数据总数加一 c.qcount++ // 解锁 unlock(\u0026c.lock) return true } // 如果是非阻塞类型 channel，则只返回 if !block { unlock(\u0026c.lock) return false } // 如果是阻塞类型，则一直阻塞一直到被读取，保证数据在被读取之前不被内存回收 // Block on the channel. Some receiver will complete our operation for us. gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } KeepAlive(ep) // someone woke us up. if mysg != gp.waiting { throw(\"G waiting list is corrupted\") } gp.waiting = nil if gp.param == nil { if c.closed == 0 { throw(\"chansend: spurious wakeup\") } panic(plainError(\"send on closed channel\")) } gp.param = nil if mysg.releasetime \u003e 0 { blockevent(mysg.releasetime-t0, 2) } mysg.c = nil releaseSudog(mysg) return true } ","date":"2020-03-06","objectID":"/posts/go-channel/:2:2","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"读取 近期补充。。。 使用 Channel是Go中的一个核心类型，你可以把它看成一个管道，通过它并发核心单元就可以发送或者接收数据进行通讯(communication)。 它的操作符是箭头 \u003c- 。 ch \u003c- v v := \u003c-ch (箭头的指向就是数据的流向) 就像 map 和 slice 数据类型一样, channel必须先创建再使用: ch := make(chan int) ","date":"2020-03-06","objectID":"/posts/go-channel/:2:3","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"Channel 类型 Channel类型的定义格式如下： ChannelType = ( \"chan\" | \"chan\" \"\u003c-\" | \"\u003c-\" \"chan\" ) ElementType . 它包括三种类型的定义。可选的\u003c-代表channel的方向。如果没有指定方向，那么Channel就是双向的，既可以接收数据，也可以发送数据。 chan T // 可以接收和发送类型为 T 的数据 chan\u003c- float64 // 只可以用来发送 float64 类型的数据 \u003c-chan int // 只可以用来接收 int 类型的数据 \u003c-总是优先和最左边的类型结合。(The \u003c- operator associates with the leftmost chan possible) chan\u003c- chan int // 等价 chan\u003c- (chan int) chan\u003c- \u003c-chan int // 等价 chan\u003c- (\u003c-chan int) \u003c-chan \u003c-chan int // 等价 \u003c-chan (\u003c-chan int) chan (\u003c-chan int) 使用make初始化Channel,并且可以设置容量: make(chan int, 100) 容量(capacity)代表Channel容纳的最多的元素的数量，代表Channel的缓存的大小。 如果没有设置容量，或者容量设置为0, 说明Channel没有缓存，只有sender和receiver都准备好了后它们的通讯(communication)才会发生(Blocking)。如果设置了缓存，就有可能不发生阻塞， 只有buffer满了后 send才会阻塞， 而只有缓存空了后receive才会阻塞。一个nil channel不会通信。 可以通过内建的close方法可以关闭Channel。 你可以在多个goroutine从/往 一个channel 中 receive/send 数据, 不必考虑额外的同步措施。 Channel可以作为一个先入先出(FIFO)的队列，接收的数据和发送的数据的顺序是一致的。 channel的 receive支持 multi-valued assignment，如 v, ok := \u003c-ch 它可以用来检查Channel是否已经被关闭了。 send语句 send语句用来往Channel中发送数据， 如ch \u003c- 3。 它的定义如下: SendStmt = Channel \"\u003c-\" Expression . Channel = Expression . 在通讯(communication)开始前channel和expression必选先求值出来(evaluated)，比如下面的(3+4)先计算出7然后再发送给channel。 c := make(chan int) defer close(c) go func() { c \u003c- 3 + 4 }() i := \u003c-c fmt.Println(i) send被执行前(proceed)通讯(communication)一直被阻塞着。如前所言，无缓存的channel只有在receiver准备好后send才被执行。如果有缓存，并且缓存未满，则send会被执行。 往一个已经被close的channel中继续发送数据会导致run-time panic。 往nil channel中发送数据会一致被阻塞着。 receive 操作符 \u003c-ch用来从channel ch中接收数据，这个表达式会一直被block,直到有数据可以接收。 从一个nil channel中接收数据会一直被block。 从一个被close的channel中接收数据不会被阻塞，而是立即返回，接收完已发送的数据后会返回元素类型的零值(zero value)。 如前所述，你可以使用一个额外的返回参数来检查channel是否关闭。 x, ok := \u003c-ch x, ok = \u003c-ch var x, ok = \u003c-ch ","date":"2020-03-06","objectID":"/posts/go-channel/:3:0","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"blocking 缺省情况下，发送和接收会一直阻塞着，知道另一方准备好。这种方式可以用来在gororutine中进行同步，而不必使用显示的锁或者条件变量。 如官方的例子中x, y := \u003c-c, \u003c-c这句会一直等待计算结果发送到channel中。 import \"fmt\" func sum(s []int, c chan int) { sum := 0 for _, v := range s { sum += v } c \u003c- sum } func main() { s := []int{7, 2, 8, -9, 4, 0} c := make(chan int) go sum(s[:len(s)/2], c) go sum(s[len(s)/2:], c) x, y := \u003c-c, \u003c-c // receive from c fmt.Println(x, y, x+y) } ","date":"2020-03-06","objectID":"/posts/go-channel/:4:0","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"Buffered Channels make的第二个参数指定缓存的大小：ch := make(chan int, 100)。 通过缓存的使用，可以尽量避免阻塞，提供应用的性能。 ","date":"2020-03-06","objectID":"/posts/go-channel/:5:0","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"Range for …… range语句可以处理Channel。 func main() { go func() { time.Sleep(1 * time.Hour) }() c := make(chan int) go func() { for i := 0; i \u003c 10; i = i + 1 { c \u003c- i } close(c) }() for i := range c { fmt.Println(i) } fmt.Println(\"Finished\") } range c产生的迭代值为Channel中发送的值，它会一直迭代知道channel被关闭。上面的例子中如果把close(c)注释掉，程序会一直阻塞在for …… range那一行。 ","date":"2020-03-06","objectID":"/posts/go-channel/:6:0","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"select select语句选择一组可能的send操作和receive操作去处理。它类似switch,但是只是用来处理通讯(communication)操作。 它的case可以是send语句，也可以是receive语句，亦或者default。 receive语句可以将值赋值给一个或者两个变量。它必须是一个receive操作。 最多允许有一个default case,它可以放在case列表的任何位置，尽管我们大部分会将它放在最后。 import \"fmt\" func fibonacci(c, quit chan int) { x, y := 0, 1 for { select { case c \u003c- x: x, y = y, x+y case \u003c-quit: fmt.Println(\"quit\") return } } } func main() { c := make(chan int) quit := make(chan int) go func() { for i := 0; i \u003c 10; i++ { fmt.Println(\u003c-c) } quit \u003c- 0 }() fibonacci(c, quit) } 如果有同时多个case去处理,比如同时有多个channel可以接收数据，那么Go会伪随机的选择一个case处理(pseudo-random)。如果没有case需要处理，则会选择default去处理，如果default case存在的情况下。如果没有default case，则select语句会阻塞，直到某个case需要处理。 需要注意的是，nil channel上的操作会一直被阻塞，如果没有default case,只有nil channel的select会一直被阻塞。 select语句和switch语句一样，它不是循环，它只会选择一个case来处理，如果想一直处理channel，你可以在外面加一个无限的for循环： for { select { case c \u003c- x: x, y = y, x+y case \u003c-quit: fmt.Println(\"quit\") return } } ","date":"2020-03-06","objectID":"/posts/go-channel/:7:0","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"timeout select有很重要的一个应用就是超时处理。 因为上面我们提到，如果没有case需要处理，select语句就会一直阻塞着。这时候我们可能就需要一个超时操作，用来处理超时的情况。 下面这个例子我们会在2秒后往channel c1中发送一个数据，但是select设置为1秒超时,因此我们会打印出timeout 1,而不是result 1。 import \"time\" import \"fmt\" func main() { c1 := make(chan string, 1) go func() { time.Sleep(time.Second * 2) c1 \u003c- \"result 1\" }() select { case res := \u003c-c1: fmt.Println(res) case \u003c-time.After(time.Second * 1): fmt.Println(\"timeout 1\") } } 其实它利用的是time.After方法，它返回一个类型为\u003c-chan Time的单向的channel，在指定的时间发送一个当前时间给返回的channel中。 ","date":"2020-03-06","objectID":"/posts/go-channel/:7:1","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"Timer 和 Ticker 我们看一下关于时间的两个Channel。 timer是一个定时器，代表未来的一个单一事件，你可以告诉timer你要等待多长时间，它提供一个Channel，在将来的那个时间那个Channel提供了一个时间值。下面的例子中第二行会阻塞2秒钟左右的时间，直到时间到了才会继续执行。 timer1 := time.NewTimer(time.Second * 2) \u003c-timer1.C fmt.Println(\"Timer 1 expired\") 当然如果你只是想单纯的等待的话，可以使用time.Sleep来实现。 你还可以使用timer.Stop来停止计时器。 timer2 := time.NewTimer(time.Second) go func() { \u003c-timer2.C fmt.Println(\"Timer 2 expired\") }() stop2 := timer2.Stop() if stop2 { fmt.Println(\"Timer 2 stopped\") } ticker是一个定时触发的计时器，它会以一个间隔(interval)往Channel发送一个事件(当前时间)，而Channel的接收者可以以固定的时间间隔从Channel中读取事件。下面的例子中ticker每500毫秒触发一次，你可以观察输出的时间。 ticker := time.NewTicker(time.Millisecond * 500) go func() { for t := range ticker.C { fmt.Println(\"Tick at\", t) } }() 类似timer, ticker也可以通过Stop方法来停止。一旦它停止，接收者不再会从channel中接收数据了。 ","date":"2020-03-06","objectID":"/posts/go-channel/:8:0","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"close 内建的close方法可以用来关闭channel。 总结一下channel关闭后sender的receiver操作。 如果channel c已经被关闭,继续往它发送数据会导致panic: send on closed channel: import \"time\" func main() { go func() { time.Sleep(time.Hour) }() c := make(chan int, 10) c \u003c- 1 c \u003c- 2 close(c) c \u003c- 3 } 但是从这个关闭的channel中不但可以读取出已发送的数据，还可以不断的读取零值: c := make(chan int, 10) c \u003c- 1 c \u003c- 2 close(c) fmt.Println(\u003c-c) //1 fmt.Println(\u003c-c) //2 fmt.Println(\u003c-c) //0 fmt.Println(\u003c-c) //0 但是如果通过range读取，channel关闭后for循环会跳出： c := make(chan int, 10) c \u003c- 1 c \u003c- 2 close(c) for i := range c { fmt.Println(i) } 通过i, ok := \u003c-c可以查看Channel的状态，判断值是零值还是正常读取的值。 c := make(chan int, 10) close(c) i, ok := \u003c-c fmt.Printf(\"%d, %t\", i, ok) //0, false ","date":"2020-03-06","objectID":"/posts/go-channel/:9:0","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["源码解读"],"content":"同步 channel可以用在goroutine之间的同步。 下面的例子中main goroutine通过done channel等待worker完成任务。 worker做完任务后只需往channel发送一个数据就可以通知main goroutine任务完成。 import ( \"fmt\" \"time\" ) func worker(done chan bool) { time.Sleep(time.Second) // 通知任务已完成 done \u003c- true } func main() { done := make(chan bool, 1) go worker(done) // 等待任务完成 \u003c-done } [参考资料]： https://gobyexample.com/channels https://tour.golang.org/concurrency/2 https://golang.org/ref/spec#Select_statements https://github.com/a8m/go-lang-cheat-sheet http://devs.cloudimmunity.com/gotchas-and-common-mistakes-in-go-golang/ ","date":"2020-03-06","objectID":"/posts/go-channel/:10:0","tags":["go"],"title":"Go Channel 源码解读","uri":"/posts/go-channel/"},{"categories":["个人"],"content":"由于上一个博客项目的原文件丢失，无法继续更新，只会重新开启新的博客项目，重新做起~ 技术原因原博客已下线，原博客技术文档会逐步同步到新博客上。 ","date":"2020-03-06","objectID":"/posts/my-first-post/:0:0","tags":null,"title":"新的篇章","uri":"/posts/my-first-post/"},{"categories":["基础知识"],"content":"用 GO 实现图片处理和文字合成 Go 的图片处理 最近需要一个合成明信片的工具，即往背景图的固定位置上添加一个图片和一段文字， 最后合成一张图片。由于是 go 程序的一个子功能，所以我想我只加拿 go 写好了，正好有 go 的 image 库，拿来练练。 ","date":"2017-08-22","objectID":"/posts/go-image/:0:0","tags":["go","图片处理"],"title":"Go Image","uri":"/posts/go-image/"},{"categories":["基础知识"],"content":"图片合成 图片合成我用到了这个库 github.com/disintegration/imaging 代码： package main import ( \"fmt\" \"image\" \"github.com/disintegration/imaging\" ) func HandleUserImage(fileName string) (string, error) { m, err := imaging.Open(\"target.jpg\") if err != nil { fmt.Printf(\"open file failed\") } bm, err := imaging.Open(\"bg.jpg\") if err != nil { fmt.Printf(\"open file failed\") } // 图片按比例缩放 dst := imaging.Resize(m, 200, 200, imaging.Lanczos) // 将图片粘贴到背景图的固定位置 result := imaging.Overlay(bm, dst, image.Pt(120, 140), 1) fileName := fmt.Sprintf(\"%d.jpg\", fileName) err = imaging.Save(result, fileName) if err != nil { return \"\", err } return fileName, nil } 以上是将 target.jpg 文件先进行缩放，再贴到 bg.jpg 文件的 （120，140）位置，最后保存成文件。 ","date":"2017-08-22","objectID":"/posts/go-image/:1:0","tags":["go","图片处理"],"title":"Go Image","uri":"/posts/go-image/"},{"categories":["基础知识"],"content":"图片上写文字 以下是写文字和贴图的一块用的实例： package main import ( \"fmt\" \"image\" \"image/color\" \"io/ioutil\" \"github.com/disintegration/imaging\" \"github.com/golang/freetype\" \"github.com/golang/freetype/truetype\" \"golang.org/x/image/font\" ) func main() { HandleUserImage() } // HandleUserImage paste user image onto background func HandleUserImage() (string, error) { m, err := imaging.Open(\"target.png\") if err != nil { fmt.Printf(\"open file failed\") } bm, err := imaging.Open(\"bg.jpg\") if err != nil { fmt.Printf(\"open file failed\") } // 图片按比例缩放 dst := imaging.Resize(m, 200, 200, imaging.Lanczos) // 将图片粘贴到背景图的固定位置 result := imaging.Overlay(bm, dst, image.Pt(120, 140), 1) writeOnImage(result) fileName := fmt.Sprintf(\"%d.jpg\", 1234) err = imaging.Save(result, fileName) if err != nil { return \"\", err } return fileName, nil } var dpi = flag.Float64(\"dpi\", 256, \"screen resolution\") func writeOnImage(target *image.NRGBA) { c := freetype.NewContext() c.SetDPI(*dpi) c.SetClip(target.Bounds()) c.SetDst(target) c.SetHinting(font.HintingFull) // 设置文字颜色、字体、字大小 c.SetSrc(image.NewUniform(color.RGBA{R: 240, G: 240, B: 245, A: 180})) c.SetFontSize(16) fontFam, err := getFontFamily() if err != nil { fmt.Println(\"get font family error\") } c.SetFont(fontFam) pt := freetype.Pt(500, 400) _, err = c.DrawString(\"我是水印\", pt) if err != nil { fmt.Printf(\"draw error: %v \\n\", err) } } func getFontFamily() (*truetype.Font, error) { // 这里需要读取中文字体，否则中文文字会变成方格 fontBytes, err := ioutil.ReadFile(\"Hei.ttc\") if err != nil { fmt.Println(\"read file error:\", err) return \u0026truetype.Font{}, err } f, err := freetype.ParseFont(fontBytes) if err != nil { fmt.Println(\"parse font error:\", err) return \u0026truetype.Font{}, err } return f, err 最后来一张效果图 ","date":"2017-08-22","objectID":"/posts/go-image/:2:0","tags":["go","图片处理"],"title":"Go Image","uri":"/posts/go-image/"},{"categories":["基础知识"],"content":"总结 做的过程中，合作这一块比较好做，但是图片上写文字，相对比较麻烦，而且 freetype 库并没有默认的中英文字体，如果不指定字体会报错，而且字体格式只限制于 ttf 和 ttc 两种。 ","date":"2017-08-22","objectID":"/posts/go-image/:3:0","tags":["go","图片处理"],"title":"Go Image","uri":"/posts/go-image/"},{"categories":["网络编程"],"content":"udp 和 tcp 的简单比较和用 go 实现最简单的 udp 客户端和服务端 …… 用 go 实现简单的 udp 用户数据包协议（英语：User Datagram Protocol，缩写为UDP），又称用户数据报文协议，是一个简单的面向数据报的传输层协议，正式规范为RFC 768。 在TCP/IP模型中，UDP为网络层以上和应用层以下提供了一个简单的接口。UDP只提供数据的不可靠传递，它一旦把应用程序发给网络层的数据发送出去，就不保留数据备份（所以UDP有时候也被认为是不可靠的数据报协议）。UDP在IP数据报的头部仅仅加入了复用和数据校验（字段）。 ","date":"2017-08-02","objectID":"/posts/go-udp/:0:0","tags":["go","udp"],"title":"Go UDP Socket","uri":"/posts/go-udp/"},{"categories":["网络编程"],"content":"UDP 与 TCP 的比较 UDP – 用户数据协议包，是一个简单的面向数据报的运输层协议。UDP 不提供可靠性，它只是把应用程序给 IP 层的数据报发送出去，但是并不能保证他们能达到目的地。由于 UDP 在传输数据报之前不用在客户端和服务端之间建立连接，且没有超时机制，故而传输速度很快。 TCP – 传输控制协议，提供的是面向连接，可靠的字节流服务。当客户端和服务端彼此交换数据前，必须先在双方之间建立 TCP 连接，之后才能传输数据。TCP 提供超时重发，丢弃重复数据，检验数据，流量控制等功能，保证数据能从一段传到另一端。 - TCP UDP 是否连接 面向连接 面向非连接 传输可靠性 可靠 会丢包，不可靠 应用场景 传输数据量大 传输数据量小 速度 慢 快 ","date":"2017-08-02","objectID":"/posts/go-udp/:1:0","tags":["go","udp"],"title":"Go UDP Socket","uri":"/posts/go-udp/"},{"categories":["网络编程"],"content":"TCP 与 UDP 的选择 当数据传输的性能必须让位于数据传输的完整性、可控制性和可靠性时，TCP协议是当然的选择。当强调传输性能而不是传输的完整性时，如：音频和多媒体应用，UDP是最好的选择。在数据传输时间很短，以至于此前的连接过程成为整个流量主体的情况下，UDP也是一个好的选择，如：DNS交换。把SNMP建立在UDP上的部分原因是设计者认为当发生网络阻塞时，UDP较低的开销使其有更好的机会去传送管理数据。TCP丰富的功能有时会导致不可预料的性能低下，但是我们相信在不远的将来，TCP可靠的点对点连接将会用于绝大多数的网络应用。 ","date":"2017-08-02","objectID":"/posts/go-udp/:2:0","tags":["go","udp"],"title":"Go UDP Socket","uri":"/posts/go-udp/"},{"categories":["网络编程"],"content":"UDP 使用场景 在选择使用协议的时候，选择UDP必须要谨慎。在网络质量令人十分不满意的环境下，UDP协议数据包丢失会比较严重。但是由于UDP的特性：它不属于连接型协议，因而具有资源消耗小，处理速度快的优点，所以通常音频、视频和普通数据在传送时使用UDP较多，因为它们即使偶尔丢失一两个数据包，也不会对接收结果产生太大影响。而且如果在内网的情况下，丢包率也很低，所以内网的数据传输也可以用 UDP 协议。我们常用的 QQ，一部分数据传输功能也是用 UDP协议来实现的。 ","date":"2017-08-02","objectID":"/posts/go-udp/:3:0","tags":["go","udp"],"title":"Go UDP Socket","uri":"/posts/go-udp/"},{"categories":["网络编程"],"content":"实现 下面分别是服务端和客户端实现代码： 服务端代码 server.go: package main import ( \"fmt\" \"net\" ) func main() { // 解析地址 addr, err := net.ResolveUDPAddr(\"udp\", \":3017\") if err != nil { fmt.Println(\"Can't resolve addr:\", err.Error()) panic(err) } // 监听端口 conn, err := net.ListenUDP(\"udp\", addr) if err != nil { fmt.Println(\"listen error:\", err.Error()) panic(err) } defer conn.Close() for { handlerClient(conn) } } func handlerClient(conn *net.UDPConn) { data := make([]byte, 1024) // 从 UDP 中读取内容并写到 data _, remoteAddr, err := conn.ReadFromUDP(data) if err != nil { fmt.Println(\"read udp msg failed with:\", err.Error()) return } // 给收到消息的 client 写回信息 conn.WriteToUDP([]byte(\"a\"), remoteAddr) } 客户端代码client.go： package client import ( \"fmt\" \"net\" ) var ( // Connection *net.UDPConn Connection []*net.UDPConn ) // Client 创建一个 UDP 连接 func Client() { addr, err := net.ResolveUDPAddr(\"udp\", \"127.0.0.1:3017\") if err != nil { fmt.Println(\"Can't resolve address: \", err) panic(err) } conn, err := net.DialUDP(\"udp\", nil, addr) if err != nil { fmt.Println(\"Can't dial: \", err) panic(err) } Connection = append(Connection, conn) } // WriteTo 像传入参数 conn 写数据 func WriteTo(conn *net.UDPConn) { _, err := conn.Write([]byte(\"hello from the other site\")) if err != nil { fmt.Println(\"failed:\", err) } data := make([]byte, 1024) _, err = conn.Read(data) if err != nil { fmt.Println(\"failed to read UDP msg because of \", err) } } ","date":"2017-08-02","objectID":"/posts/go-udp/:4:0","tags":["go","udp"],"title":"Go UDP Socket","uri":"/posts/go-udp/"},{"categories":["网络编程"],"content":"总结 以上是一个最简单的 UDP 客户端服务器的代码，只有启动服务和收发消息的功能，但实际应用 UDP 协议到具体需求的时候，需要考虑的问题很多，比如包的设计，包头的设计，错误处理，丢包处理，包顺序调换处理等。所以需要用到传输数据协议的时候，请考虑好需求和可能遇到的问题，以及对问题的处理方案。 ","date":"2017-08-02","objectID":"/posts/go-udp/:5:0","tags":["go","udp"],"title":"Go UDP Socket","uri":"/posts/go-udp/"},{"categories":["网络编程"],"content":"转载文章 Go语言TCP Socket编程 文章原始地址: http://tonybai.com/2015/11/17/tcp-programming-in-golang/ Golang的主要 设计目标之一就是面向大规模后端服务程序，网络通信这块是服务端 程序必不可少也是至关重要的一部分。在日常应用中，我们也可以看到Go中的net以及其subdirectories下的包均是“高频+刚需”，而TCP socket则是网络编程的主流，即便您没有直接使用到net中有关TCP Socket方面的接口，但net/http总是用到了吧，http底层依旧是用tcp socket实现的。 网络编程方面，我们最常用的就是tcp socket编程了，在posix标准出来后，socket在各大主流OS平台上都得到了很好的支持。关于tcp programming，最好的资料莫过于W. Richard Stevens 的网络编程圣经《UNIX网络 编程 卷1：套接字联网API》 了，书中关于tcp socket接口的各种使用、行为模式、异常处理讲解的十分细致。Go是自带runtime的跨平台编程语言，Go中暴露给语言使用者的tcp socket api是建立OS原生tcp socket接口之上的。由于Go runtime调度的需要，golang tcp socket接口在行为特点与异常处理方面与OS原生接口有着一些差别。这篇博文的目标就是整理出关于Go tcp socket在各个场景下的使用方法、行为特点以及注意事项。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:0:0","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"一、模型 从tcp socket诞生后，网络编程架构模型也几经演化，大致是：“每进程一个连接” –\u003e “每线程一个连接” –\u003e “Non-Block + I/O多路复用(linux epoll/windows iocp/freebsd darwin kqueue/solaris Event Port)”。伴随着模型的演化，服务程序愈加强大，可以支持更多的连接，获得更好的处理性能。 目前主流web server一般均采用的都是”Non-Block + I/O多路复用”（有的也结合了多线程、多进程）。不过I/O多路复用也给使用者带来了不小的复杂度，以至于后续出现了许多高性能的I/O多路复用框架， 比如libevent、libev、libuv等，以帮助开发者简化开发复杂性，降低心智负担。不过Go的设计者似乎认为I/O多路复用的这种通过回调机制割裂控制流 的方式依旧复杂，且有悖于“一般逻辑”设计，为此Go语言将该“复杂性”隐藏在Runtime中了：Go开发者无需关注socket是否是 non-block的，也无需亲自注册文件描述符的回调，只需在每个连接对应的goroutine中以**“block I/O”**的方式对待socket处理即可，这可以说大大降低了开发人员的心智负担。一个典型的Go server端程序大致如下： //go-tcpsock/server.go func handleConn(c net.Conn) { defer c.Close() for { // read from the connection // ... ... // write to the connection //... ... } } func main() { l, err := net.Listen(\"tcp\", \":8888\") if err != nil { fmt.Println(\"listen error:\", err) return } for { c, err := l.Accept() if err != nil { fmt.Println(\"accept error:\", err) break } // start a new goroutine to handle // the new connection. go handleConn(c) } } 用户层眼中看到的goroutine中的“block socket”，实际上是通过Go runtime中的netpoller通过Non-block socket + I/O多路复用机制“模拟”出来的，真实的underlying socket实际上是non-block的，只是runtime拦截了底层socket系统调用的错误码，并通过netpoller和goroutine 调度让goroutine“阻塞”在用户层得到的Socket fd上。比如：当用户层针对某个socket fd发起read操作时，如果该socket fd中尚无数据，那么runtime会将该socket fd加入到netpoller中监听，同时对应的goroutine被挂起，直到runtime收到socket fd 数据ready的通知，runtime才会重新唤醒等待在该socket fd上准备read的那个Goroutine。而这个过程从Goroutine的视角来看，就像是read操作一直block在那个socket fd上似的。具体实现细节在后续场景中会有补充描述。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:1:0","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"二、TCP连接的建立 众所周知，TCP Socket的连接的建立需要经历客户端和服务端的三次握手的过程。连接建立过程中，服务端是一个标准的Listen + Accept的结构(可参考上面的代码)，而在客户端Go语言使用net.Dial或DialTimeout进行连接建立： 阻塞Dial： conn, err := net.Dial(\"tcp\", \"google.com:80\") if err != nil { //handle error } // read or write on conn 或是带上超时机制的Dial： conn, err := net.DialTimeout(\"tcp\", \":8080\", 2 * time.Second) if err != nil { //handle error } // read or write on conn 对于客户端而言，连接的建立会遇到如下几种情形： ","date":"2017-07-31","objectID":"/posts/go-tcp/:2:0","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"1、网络不可达或对方服务未启动 如果传给Dial的Addr是可以立即判断出网络不可达，或者Addr中端口对应的服务没有启动，端口未被监听，Dial会几乎立即返回错误，比如： //go-tcpsock/conn_establish/client1.go ... ... func main() { log.Println(\"begin dial...\") conn, err := net.Dial(\"tcp\", \":8888\") if err != nil { log.Println(\"dial error:\", err) return } defer conn.Close() log.Println(\"dial ok\") } 如果本机8888端口未有服务程序监听，那么执行上面程序，Dial会很快返回错误： $go run client1.go 2015/11/16 14:37:41 begin dial... 2015/11/16 14:37:41 dial error: dial tcp :8888: getsockopt: connection refused ","date":"2017-07-31","objectID":"/posts/go-tcp/:2:1","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"2、对方服务的listen backlog满 还有一种场景就是对方服务器很忙，瞬间有大量client端连接尝试向server建立，server端的listen backlog队列满，server accept不及时((即便不accept，那么在backlog数量范畴里面，connect都会是成功的，因为new conn已经加入到server side的listen queue中了，accept只是从queue中取出一个conn而已)，这将导致client端Dial阻塞。我们还是通过例子感受Dial的行为特点： 服务端代码： //go-tcpsock/conn_establish/server2.go ... ... func main() { l, err := net.Listen(\"tcp\", \":8888\") if err != nil { log.Println(\"error listen:\", err) return } defer l.Close() log.Println(\"listen ok\") var i int for { time.Sleep(time.Second * 10) if _, err := l.Accept(); err != nil { log.Println(\"accept error:\", err) break } i++ log.Printf(\"%d: accept a new connection\\n\", i) } } 客户端代码： //go-tcpsock/conn_establish/client2.go ... ... func establishConn(i int) net.Conn { conn, err := net.Dial(\"tcp\", \":8888\") if err != nil { log.Printf(\"%d: dial error: %s\", i, err) return nil } log.Println(i, \":connect to server ok\") return conn } func main() { var sl []net.Conn for i := 1; i \u003c 1000; i++ { conn := establishConn(i) if conn != nil { sl = append(sl, conn) } } time.Sleep(time.Second * 10000) } 从程序可以看出，服务端在listen成功后，每隔10s钟accept一次。客户端则是串行的尝试建立连接。这两个程序在Darwin下的执行 结果： $go run server2.go 2015/11/16 21:55:41 listen ok 2015/11/16 21:55:51 1: accept a new connection 2015/11/16 21:56:01 2: accept a new connection ... ... $go run client2.go 2015/11/16 21:55:44 1 :connect to server ok 2015/11/16 21:55:44 2 :connect to server ok 2015/11/16 21:55:44 3 :connect to server ok ... ... 2015/11/16 21:55:44 126 :connect to server ok 2015/11/16 21:55:44 127 :connect to server ok 2015/11/16 21:55:44 128 :connect to server ok 2015/11/16 21:55:52 129 :connect to server ok 2015/11/16 21:56:03 130 :connect to server ok 2015/11/16 21:56:14 131 :connect to server ok ... ... 可以看出Client初始时成功地一次性建立了128个连接，然后后续每阻塞近10s才能成功建立一条连接。也就是说在server端 backlog满时(未及时accept)，客户端将阻塞在Dial上，直到server端进行一次accept。至于为什么是128，这与darwin 下的默认设置有关： $sysctl -a|grep kern.ipc.somaxconn kern.ipc.somaxconn: 128 如果我在ubuntu 14.04上运行上述server程序，我们的client端初始可以成功建立499条连接。 如果server一直不accept，client端会一直阻塞么？我们去掉accept后的结果是：在Darwin下，client端会阻塞大 约1分多钟才会返回timeout： 2015/11/16 22:03:31 128 :connect to server ok 2015/11/16 22:04:48 129: dial error: dial tcp :8888: getsockopt: operation timed out 而如果server运行在ubuntu 14.04上，client似乎一直阻塞，我等了10多分钟依旧没有返回。 阻塞与否看来与server端的网络实现和设置有关。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:2:2","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"3、网络延迟较大，Dial阻塞并超时 如果网络延迟较大，TCP握手过程将更加艰难坎坷（各种丢包），时间消耗的自然也会更长。Dial这时会阻塞，如果长时间依旧无法建立连接，则Dial也会返回“ getsockopt: operation timed out”错误。 在连接建立阶段，多数情况下，Dial是可以满足需求的，即便阻塞一小会儿。但对于某些程序而言，需要有严格的连接时间限定，如果一定时间内没能成功建立连接，程序可能会需要执行一段“异常”处理逻辑，为此我们就需要DialTimeout了。下面的例子将Dial的最长阻塞时间限制在2s内，超出这个时长，Dial将返回timeout error： //go-tcpsock/conn_establish/client3.go ... ... func main() { log.Println(\"begin dial...\") conn, err := net.DialTimeout(\"tcp\", \"104.236.176.96:80\", 2*time.Second) if err != nil { log.Println(\"dial error:\", err) return } defer conn.Close() log.Println(\"dial ok\") } 执行结果如下（需要模拟一个延迟较大的网络环境）： $go run client3.go 2015/11/17 09:28:34 begin dial... 2015/11/17 09:28:36 dial error: dial tcp 104.236.176.96:80: i/o timeout ","date":"2017-07-31","objectID":"/posts/go-tcp/:2:3","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"三、Socket读写 连接建立起来后，我们就要在conn上进行读写，以完成业务逻辑。前面说过Go runtime隐藏了I/O多路复用的复杂性。语言使用者只需采用goroutine+Block I/O的模式即可满足大部分场景需求。Dial成功后，方法返回一个net.Conn接口类型变量值，这个接口变量的动态类型为一个*TCPConn： //$GOROOT/src/net/tcpsock_posix.go type TCPConn struct { conn } TCPConn内嵌了一个unexported类型：conn，因此TCPConn”继承”了conn的Read和Write方法，后续通过Dial返回值调用的Write和Read方法均是net.conn的方法： //$GOROOT/src/net/net.go type conn struct { fd *netFD } func (c *conn) ok() bool { return c != nil \u0026\u0026 c.fd != nil } // Implementation of the Conn interface. // Read implements the Conn Read method. func (c *conn) Read(b []byte) (int, error) { if !c.ok() { return 0, syscall.EINVAL } n, err := c.fd.Read(b) if err != nil \u0026\u0026 err != io.EOF { err = \u0026OpError{Op: \"read\", Net: c.fd.net, Source: c.fd.laddr, Addr: c.fd.raddr, Err: err} } return n, err } // Write implements the Conn Write method. func (c *conn) Write(b []byte) (int, error) { if !c.ok() { return 0, syscall.EINVAL } n, err := c.fd.Write(b) if err != nil { err = \u0026OpError{Op: \"write\", Net: c.fd.net, Source: c.fd.laddr, Addr: c.fd.raddr, Err: err} } return n, err } 下面我们先来通过几个场景来总结一下conn.Read的行为特点。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:3:0","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"1、Socket中无数据 连接建立后，如果对方未发送数据到socket，接收方(Server)会阻塞在Read操作上，这和前面提到的“模型”原理是一致的。执行该Read操作的goroutine也会被挂起。runtime会监视该socket，直到其有数据才会重新 调度该socket对应的Goroutine完成read。由于篇幅原因，这里就不列代码了，例子对应的代码文件：go-tcpsock/read_write下的client1.go和server1.go。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:3:1","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"2、Socket中有部分数据 如果socket中有部分数据，且长度小于一次Read操作所期望读出的数据长度，那么Read将会成功读出这部分数据并返回，而不是等待所有期望数据全部读取后再返回。 Client端： //go-tcpsock/read_write/client2.go ... ... func main() { if len(os.Args) \u003c= 1 { fmt.Println(\"usage: go run client2.go YOUR_CONTENT\") return } log.Println(\"begin dial...\") conn, err := net.Dial(\"tcp\", \":8888\") if err != nil { log.Println(\"dial error:\", err) return } defer conn.Close() log.Println(\"dial ok\") time.Sleep(time.Second * 2) data := os.Args[1] conn.Write([]byte(data)) time.Sleep(time.Second * 10000) } Server端： //go-tcpsock/read_write/server2.go ... ... func handleConn(c net.Conn) { defer c.Close() for { // read from the connection var buf = make([]byte, 10) log.Println(\"start to read from conn\") n, err := c.Read(buf) if err != nil { log.Println(\"conn read error:\", err) return } log.Printf(\"read %d bytes, content is %s\\n\", n, string(buf[:n])) } } ... ... 我们通过client2.go发送”hi”到Server端： 运行结果: $go run client2.go hi 2015/11/17 13:30:53 begin dial... 2015/11/17 13:30:53 dial ok $go run server2.go 2015/11/17 13:33:45 accept a new connection 2015/11/17 13:33:45 start to read from conn 2015/11/17 13:33:47 read 2 bytes, content is hi ... Client向socket中写入两个字节数据(“hi”)，Server端创建一个len = 10的slice，等待Read将读取的数据放入slice；Server随后读取到那两个字节：”hi”。Read成功返回，n =2 ，err = nil。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:3:2","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"3、Socket中有足够数据 如果socket中有数据，且长度大于等于一次Read操作所期望读出的数据长度，那么Read将会成功读出这部分数据并返回。这个情景是最符合我们对Read的期待的了：Read将用Socket中的数据将我们传入的slice填满后返回：n = 10, err = nil。 我们通过client2.go向Server2发送如下内容：abcdefghij12345，执行结果如下： $go run client2.go abcdefghij12345 2015/11/17 13:38:00 begin dial... 2015/11/17 13:38:00 dial ok $go run server2.go 2015/11/17 13:38:00 accept a new connection 2015/11/17 13:38:00 start to read from conn 2015/11/17 13:38:02 read 10 bytes, content is abcdefghij 2015/11/17 13:38:02 start to read from conn 2015/11/17 13:38:02 read 5 bytes, content is 12345 client端发送的内容长度为15个字节，Server端Read buffer的长度为10，因此Server Read第一次返回时只会读取10个字节；Socket中还剩余5个字节数据，Server再次Read时会把剩余数据读出（如：情形2）。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:3:3","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"4、Socket关闭 如果client端主动关闭了socket，那么Server的Read将会读到什么呢？这里分为“有数据关闭”和“无数据关闭”。 “有数据关闭”是指在client关闭时，socket中还有server端未读取的数据，我们在go-tcpsock/read_write/client3.go和server3.go中模拟这种情况： $go run client3.go hello 2015/11/17 13:50:57 begin dial... 2015/11/17 13:50:57 dial ok $go run server3.go 2015/11/17 13:50:57 accept a new connection 2015/11/17 13:51:07 start to read from conn 2015/11/17 13:51:07 read 5 bytes, content is hello 2015/11/17 13:51:17 start to read from conn 2015/11/17 13:51:17 conn read error: EOF 从输出结果来看，当client端close socket退出后，server3依旧没有开始Read，10s后第一次Read成功读出了5个字节的数据，当第二次Read时，由于client端 socket关闭，Read返回EOF error。 通过上面这个例子，我们也可以猜测出“无数据关闭”情形下的结果，那就是Read直接返回EOF error。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:3:4","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"5、读取操作超时 有些场合对Read的阻塞时间有严格限制，在这种情况下，Read的行为到底是什么样的呢？在返回超时错误时，是否也同时Read了一部分数据了呢？这个实验比较难于模拟，下面的测试结果也未必能反映出所有可能结果。我们编写了client4.go和server4.go来模拟这一情形。 //go-tcpsock/read_write/client4.go ... ... func main() { log.Println(\"begin dial...\") conn, err := net.Dial(\"tcp\", \":8888\") if err != nil { log.Println(\"dial error:\", err) return } defer conn.Close() log.Println(\"dial ok\") data := make([]byte, 65536) conn.Write(data) time.Sleep(time.Second * 10000) } //go-tcpsock/read_write/server4.go ... ... func handleConn(c net.Conn) { defer c.Close() for { // read from the connection time.Sleep(10 * time.Second) var buf = make([]byte, 65536) log.Println(\"start to read from conn\") c.SetReadDeadline(time.Now().Add(time.Microsecond * 10)) n, err := c.Read(buf) if err != nil { log.Printf(\"conn read %d bytes, error: %s\", n, err) if nerr, ok := err.(net.Error); ok \u0026\u0026 nerr.Timeout() { continue } return } log.Printf(\"read %d bytes, content is %s\\n\", n, string(buf[:n])) } } 在Server端我们通过Conn的SetReadDeadline方法设置了10微秒的读超时时间，Server的执行结果如下： $go run server4.go 2015/11/17 14:21:17 accept a new connection 2015/11/17 14:21:27 start to read from conn 2015/11/17 14:21:27 conn read 0 bytes, error: read tcp 127.0.0.1:8888-\u003e127.0.0.1:60970: i/o timeout 2015/11/17 14:21:37 start to read from conn 2015/11/17 14:21:37 read 65536 bytes, content is 虽然每次都是10微秒超时，但结果不同，第一次Read超时，读出数据长度为0；第二次读取所有数据成功，没有超时。反复执行了多次，没能出现“读出部分数据且返回超时错误”的情况。 和读相比，Write遇到的情形一样不少，我们也逐一看一下。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:3:5","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"1、成功写 前面例子着重于Read，client端在Write时并未判断Write的返回值。所谓“成功写”指的就是Write调用返回的n与预期要写入的数据长度相等，且error = nil。这是我们在调用Write时遇到的最常见的情形，这里不再举例了。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:3:6","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"2、写阻塞 TCP连接通信两端的OS都会为该连接保留数据缓冲，一端调用Write后，实际上数据是写入到OS的协议栈的数据缓冲的。TCP是全双工通信，因此每个方向都有独立的数据缓冲。当发送方将对方的接收缓冲区以及自身的发送缓冲区写满后，Write就会阻塞。我们来看一个例子：client5.go和server.go。 //go-tcpsock/read_write/client5.go ... ... func main() { log.Println(\"begin dial...\") conn, err := net.Dial(\"tcp\", \":8888\") if err != nil { log.Println(\"dial error:\", err) return } defer conn.Close() log.Println(\"dial ok\") data := make([]byte, 65536) var total int for { n, err := conn.Write(data) if err != nil { total += n log.Printf(\"write %d bytes, error:%s\\n\", n, err) break } total += n log.Printf(\"write %d bytes this time, %d bytes in total\\n\", n, total) } log.Printf(\"write %d bytes in total\\n\", total) time.Sleep(time.Second * 10000) } //go-tcpsock/read_write/server5.go ... ... func handleConn(c net.Conn) { defer c.Close() time.Sleep(time.Second * 10) for { // read from the connection time.Sleep(5 * time.Second) var buf = make([]byte, 60000) log.Println(\"start to read from conn\") n, err := c.Read(buf) if err != nil { log.Printf(\"conn read %d bytes, error: %s\", n, err) if nerr, ok := err.(net.Error); ok \u0026\u0026 nerr.Timeout() { continue } } log.Printf(\"read %d bytes, content is %s\\n\", n, string(buf[:n])) } } ... ... Server5在前10s中并不Read数据，因此当client5一直尝试写入时，写到一定量后就会发生阻塞： $go run client5.go 2015/11/17 14:57:33 begin dial... 2015/11/17 14:57:33 dial ok 2015/11/17 14:57:33 write 65536 bytes this time, 65536 bytes in total 2015/11/17 14:57:33 write 65536 bytes this time, 131072 bytes in total 2015/11/17 14:57:33 write 65536 bytes this time, 196608 bytes in total 2015/11/17 14:57:33 write 65536 bytes this time, 262144 bytes in total 2015/11/17 14:57:33 write 65536 bytes this time, 327680 bytes in total 2015/11/17 14:57:33 write 65536 bytes this time, 393216 bytes in total 2015/11/17 14:57:33 write 65536 bytes this time, 458752 bytes in total 2015/11/17 14:57:33 write 65536 bytes this time, 524288 bytes in total 2015/11/17 14:57:33 write 65536 bytes this time, 589824 bytes in total 2015/11/17 14:57:33 write 65536 bytes this time, 655360 bytes in total 在Darwin上，这个size大约在679468bytes。后续当server5每隔5s进行Read时，OS socket缓冲区腾出了空间，client5就又可以写入了： $go run server5.go 2015/11/17 15:07:01 accept a new connection 2015/11/17 15:07:16 start to read from conn 2015/11/17 15:07:16 read 60000 bytes, content is 2015/11/17 15:07:21 start to read from conn 2015/11/17 15:07:21 read 60000 bytes, content is 2015/11/17 15:07:26 start to read from conn 2015/11/17 15:07:26 read 60000 bytes, content is .... client端： 2015/11/17 15:07:01 write 65536 bytes this time, 720896 bytes in total 2015/11/17 15:07:06 write 65536 bytes this time, 786432 bytes in total 2015/11/17 15:07:16 write 65536 bytes this time, 851968 bytes in total 2015/11/17 15:07:16 write 65536 bytes this time, 917504 bytes in total 2015/11/17 15:07:27 write 65536 bytes this time, 983040 bytes in total 2015/11/17 15:07:27 write 65536 bytes this time, 1048576 bytes in total .... ... ","date":"2017-07-31","objectID":"/posts/go-tcp/:3:7","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"3、写入部分数据 Write操作存在写入部分数据的情况，比如上面例子中，当client端输出日志停留在“write 65536 bytes this time, 655360 bytes in total”时，我们杀掉server5，这时我们会看到client5输出以下日志： ... 2015/11/17 15:19:14 write 65536 bytes this time, 655360 bytes in total 2015/11/17 15:19:16 write 24108 bytes, error:write tcp 127.0.0.1:62245-\u003e127.0.0.1:8888: write: broken pipe 2015/11/17 15:19:16 write 679468 bytes in total 显然Write并非在655360这个地方阻塞的，而是后续又写入24108后发生了阻塞，server端socket关闭后，我们看到Wrote返回er != nil且n = 24108，程序需要对这部分写入的24108字节做特定处理。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:3:8","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"4、写入超时 如果非要给Write增加一个期限，那我们可以调用SetWriteDeadline方法。我们copy一份client5.go，形成client6.go，在client6.go的Write之前增加一行timeout设置代码： conn.SetWriteDeadline(time.Now().Add(time.Microsecond * 10)) 启动server6.go，启动client6.go，我们可以看到写入超时的情况下，Write的返回结果： $go run client6.go 2015/11/17 15:26:34 begin dial... 2015/11/17 15:26:34 dial ok 2015/11/17 15:26:34 write 65536 bytes this time, 65536 bytes in total ... ... 2015/11/17 15:26:34 write 65536 bytes this time, 655360 bytes in total 2015/11/17 15:26:34 write 24108 bytes, error:write tcp 127.0.0.1:62325-\u003e127.0.0.1:8888: i/o timeout 2015/11/17 15:26:34 write 679468 bytes in total 可以看到在写入超时时，依旧存在部分数据写入的情况。 综上例子，虽然Go给我们提供了阻塞I/O的便利，但在调用Read和Write时依旧要综合需要方法返回的n和err的结果，以做出正确处理。net.conn实现了io.Reader和io.Writer接口，因此可以试用一些wrapper包进行socket读写，比如bufio包下面的Writer和Reader、io/ioutil下的函数等。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:3:9","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"Goroutine safe 基于goroutine的网络架构模型，存在在不同goroutine间共享conn的情况，那么conn的读写是否是goroutine safe的呢？在深入这个问题之前，我们先从应用意义上来看read操作和write操作的goroutine-safe必要性。 对于read操作而言，由于TCP是面向字节流，conn.Read无法正确区分数据的业务边界，因此多个goroutine对同一个conn进行read的意义不大，goroutine读到不完整的业务包反倒是增加了业务处理的难度。对与Write操作而言，倒是有多个goroutine并发写的情况。不过conn读写是否goroutine-safe的测试不是很好做，我们先深入一下runtime代码，先从理论上给这个问题定个性： net.conn只是*netFD的wrapper结构，最终Write和Read都会落在其中的fd上： type conn struct { fd *netFD } netFD在不同平台上有着不同的实现，我们以net/fd_unix.go中的netFD为例： // Network file descriptor. type netFD struct { // locking/lifetime of sysfd + serialize access to Read and Write methods fdmu fdMutex // immutable until Close sysfd int family int sotype int isConnected bool net string laddr Addr raddr Addr // wait server pd pollDesc } 我们看到netFD中包含了一个runtime实现的fdMutex类型字段，从注释上来看，该fdMutex用来串行化对该netFD对应的sysfd的Write和Read操作。从这个注释上来看，所有对conn的Read和Write操作都是有fdMutex互斥的，从netFD的Read和Write方法的实现也证实了这一点： func (fd *netFD) Read(p []byte) (n int, err error) { if err := fd.readLock(); err != nil { return 0, err } defer fd.readUnlock() if err := fd.pd.PrepareRead(); err != nil { return 0, err } for { n, err = syscall.Read(fd.sysfd, p) if err != nil { n = 0 if err == syscall.EAGAIN { if err = fd.pd.WaitRead(); err == nil { continue } } } err = fd.eofError(n, err) break } if _, ok := err.(syscall.Errno); ok { err = os.NewSyscallError(\"read\", err) } return } func (fd *netFD) Write(p []byte) (nn int, err error) { if err := fd.writeLock(); err != nil { return 0, err } defer fd.writeUnlock() if err := fd.pd.PrepareWrite(); err != nil { return 0, err } for { var n int n, err = syscall.Write(fd.sysfd, p[nn:]) if n \u003e 0 { nn += n } if nn == len(p) { break } if err == syscall.EAGAIN { if err = fd.pd.WaitWrite(); err == nil { continue } } if err != nil { break } if n == 0 { err = io.ErrUnexpectedEOF break } } if _, ok := err.(syscall.Errno); ok { err = os.NewSyscallError(\"write\", err) } return nn, err } 每次Write操作都是受lock保护，直到此次数据全部write完。因此在应用层面，要想保证多个goroutine在一个conn上write操作的Safe，需要一次write完整写入一个“业务包”；一旦将业务包的写入拆分为多次write，那就无法保证某个Goroutine的某“业务包”数据在conn发送的连续性。 同时也可以看出即便是Read操作，也是lock保护的。多个Goroutine对同一conn的并发读不会出现读出内容重叠的情况，但内容断点是依 runtime调度来随机确定的。存在一个业务包数据，1/3内容被goroutine-1读走，另外2/3被另外一个goroutine-2读 走的情况。比如一个完整包：world，当goroutine的read slice size \u003c 5时，存在可能：一个goroutine读到 “worl”,另外一个goroutine读出”d”。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:3:10","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"四、Socket属性 原生Socket API提供了丰富的sockopt设置接口，但Golang有自己的网络架构模型，golang提供的socket options接口也是基于上述模型的必要的属性设置。包括 SetKeepAlive SetKeepAlivePeriod SetLinger SetNoDelay （默认no delay） SetWriteBuffer SetReadBuffer 不过上面的Method是TCPConn的，而不是Conn的，要使用上面的Method的，需要type assertion： tcpConn, ok := c.(*TCPConn) if !ok { //error handle } tcpConn.SetNoDelay(true) 对于listener socket, golang默认采用了 SO_REUSEADDR，这样当你重启 listener程序时，不会因为address in use的错误而启动失败。而listen backlog的默认值是通过获取系统的设置值得到的。不同系统不同：mac 128, linux 512等。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:4:0","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"五、关闭连接 和前面的方法相比，关闭连接算是最简单的操作了。由于socket是全双工的，client和server端在己方已关闭的socket和对方关闭的socket上操作的结果有不同。看下面例子： //go-tcpsock/conn_close/client1.go ... ... func main() { log.Println(\"begin dial...\") conn, err := net.Dial(\"tcp\", \":8888\") if err != nil { log.Println(\"dial error:\", err) return } conn.Close() log.Println(\"close ok\") var buf = make([]byte, 32) n, err := conn.Read(buf) if err != nil { log.Println(\"read error:\", err) } else { log.Printf(\"read % bytes, content is %s\\n\", n, string(buf[:n])) } n, err = conn.Write(buf) if err != nil { log.Println(\"write error:\", err) } else { log.Printf(\"write % bytes, content is %s\\n\", n, string(buf[:n])) } time.Sleep(time.Second * 1000) } //go-tcpsock/conn_close/server1.go ... ... func handleConn(c net.Conn) { defer c.Close() // read from the connection var buf = make([]byte, 10) log.Println(\"start to read from conn\") n, err := c.Read(buf) if err != nil { log.Println(\"conn read error:\", err) } else { log.Printf(\"read %d bytes, content is %s\\n\", n, string(buf[:n])) } n, err = c.Write(buf) if err != nil { log.Println(\"conn write error:\", err) } else { log.Printf(\"write %d bytes, content is %s\\n\", n, string(buf[:n])) } } ... ... 上述例子的执行结果如下： $go run server1.go 2015/11/17 17:00:51 accept a new connection 2015/11/17 17:00:51 start to read from conn 2015/11/17 17:00:51 conn read error: EOF 2015/11/17 17:00:51 write 10 bytes, content is $go run client1.go 2015/11/17 17:00:51 begin dial... 2015/11/17 17:00:51 close ok 2015/11/17 17:00:51 read error: read tcp 127.0.0.1:64195-\u003e127.0.0.1:8888: use of closed network connection 2015/11/17 17:00:51 write error: write tcp 127.0.0.1:64195-\u003e127.0.0.1:8888: use of closed network connection 从client1的结果来看，在己方已经关闭的socket上再进行read和write操作，会得到”use of closed network connection” error； 从server1的执行结果来看，在对方关闭的socket上执行read操作会得到EOF error，但write操作会成功，因为数据会成功写入己方的内核socket缓冲区中，即便最终发不到对方socket缓冲区了，因为己方socket并未关闭。因此当发现对方socket关闭后，己方应该正确合理处理自己的socket，再继续write已经无任何意义了。 ","date":"2017-07-31","objectID":"/posts/go-tcp/:5:0","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["网络编程"],"content":"六、小结 本文比较基础，但却很重要，毕竟golang是面向大规模服务后端的，对通信环节的细节的深入理解会大有裨益。另外Go的goroutine+阻塞通信的网络通信模型降低了开发者心智负担，简化了通信的复杂性，这点尤为重要。 本文代码实验环境：go 1.5.1 on Darwin amd64以及部分在ubuntu 14.04 amd64。 本文demo代码在这里可以找到。 © 2015, bigwhite. 版权所有. ","date":"2017-07-31","objectID":"/posts/go-tcp/:6:0","tags":["go","tcp"],"title":"Go TCP Socket","uri":"/posts/go-tcp/"},{"categories":["Docker"],"content":"关于 容器、Docker 的基础知识、基础操作和常用的命令。 Docker 基础知识和使用 ","date":"2017-07-17","objectID":"/posts/docker/:0:0","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"关于Docker ","date":"2017-07-17","objectID":"/posts/docker/:1:0","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"容器技术 对于容器，目前并没有一个严格的定义，但是普遍被认可的说法是，它首先必须是一个相对独立的环境，在这一点上有点类似虚拟机，但是没有虚拟机那么彻底。另外，在一个容器环境中，应该最小化其对外界的影响，比如不能在容器中吧host上的资源耗尽，这就是资源的控制。 容器技术之所以受欢迎，一个重要的原因是它已经集成到了 Linux 内核中，已经被当作 Linux 内核原生提供的特征。当然其他平台也有相应的容器技术，但是我们讨论的以及Docker涉及的都是指 Linux 平台上的容器技术。 一般来说，容器技术主要包括Namespace和Cgroup两个内核特征。 Namespace 命名空间，它主要做的是访问隔离。其原理是对一类资源进行抽象，并将其封装在一起提供给容器使用，对于这类资源，因为每个容器都有自己的抽象，而他们彼此之间是不可见的，所以就做到访问隔离。 Cgroup是 control group 的简称，又称为控制组，它主要是控制资源控制。其原理是将一组进程放在一个控制组里，通过给这个控制组分配指定的可用资源，达到控制这一组进程可用资源的目的。 容器最核心技术是 Namespace+Cgroup，但是光有这两个抽象的技术概念是无法组成一个完整的容器的。 对于 linux 容器的最小组成，是由一下四个部分构成： Cgroup： 资源控制。 Namespace： 访问隔离。 rootfs： 系统文件隔离。 容器引擎： 生命周期控制。 ","date":"2017-07-17","objectID":"/posts/docker/:1:1","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"容器的创建原理 代码一 pid = clone(fun, stack, flags, clone_arg); (flags: CLONE_NEWPID | CLONE_NEWNS | CLONE_NEWUSER | CLONE_NEWNET | CLONE_NEWIPC | CLONE_NEWUTS | ...) 对于以上代码，通过clone系统调用，并传入各个Namespace对应的clone flag，创建了一个新的子进程，该进程拥有自己的Namespace。从上面的代码可以看出，该进程拥有自己的pid,mount,user,net,ipc,uts namespace 。 代码二： echo $pid \u003e /sys/fs/cgroup/cpu/tasks echo $pid \u003e /sys/fs/cgroup/cpuset/tasks echo $pid \u003e /sys/fs/cgroup/blkio/tasks echo $pid \u003e /sys/fs/cgroup/memory/tasks echo $pid \u003e /sys/fs/cgroup/devices/tasks echo $pid \u003e /sys/fs/cgroup/freezer/tasks 对于代码二，将代码一中的pid写入各个Cgroup子系统中，这样该进程就可以受到相应Cgroup子系统的控制。 代码三： fun () { ... pivot_root(\"path_of_rootfs/\", path); ... exec(\"/bin/bash\"); ... } 对于代码三，该fun函数由上面生成的新进程执行，在fun函数中，通过pivot_root系统调用，使进程进入新的rootfs，之后通过exec系统调用，在新的Namespace,Cgroup,rootfs中执行\"/bin/bash\"程序。 通过以上操作，成功在一个“容器”中运行了一个bash程序。对于Cgroup和Namespace的技术细节，我们下一节详细描述 ","date":"2017-07-17","objectID":"/posts/docker/:1:2","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"Cgroup Cgroup 是什么 Cgroup是control group 的简写，属于 Linux 内核提供的一个特性，用于限制和隔离一组进程对系统资源的使用。这些资源主要包括 CPU， 内存， block I/O（数据块 I/O） 和网络宽带。 Cgroup 从 2.6.24版本进入内核主线，目前各大发行版linux都默认打开了 Cgroup 特性 从实现的角度来看，Cgroup 实现了一个通用的进程分组的框架，而不同资源的具体管理则是由各个 Cgroup 子系统实现的。截止内核4.1版本，Cgroup 中实现的子系统的及其作用如下： devices： 设备权限控制 cpuset： 分配指定的CPU和内存节点 cpu： 控制 CPU 占用率 cpuacct： 统计 CPU 使用情况 memory： 限制内存的使用上限 freezer： 冻结（暂停）Cgroup 中的进程 net_cls： 配合tc（traffic controller）限制网络宽带 net_prio： 设置进程的网络流量优先级 huge_tlb： 限制HugeTLB（块表缓冲区）的使用 perf_event： 允许 Perf 工具基于Cgroup分组做性能测试 ","date":"2017-07-17","objectID":"/posts/docker/:1:3","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"Namespace Namespace 是什么 Namespace 是将内核的全局资源做封装，使得每个Namespace都有有一份独立的资源，因此不同的进程各自的 Namespace 内对同一个资源的使用不会互相干扰。 举个例子，执行 sethostname 这个系统调用时，可以改变系统的主机名，这个主机名就是一个内核的全局资源。内核通过实现 UTS Namespace，可以将不同的进程分隔在不同的 UTS Namespace 中，在某个 Namespace 修改主机名时，另一个 Namespace 的主机名还是保持不变。 目前 Linux 内核总共实现了6种 Namespace： IPC： 隔离 System V IPC 和 POSIX 消息队列 Network： 隔离网络资源 Mount： 隔离文件系统挂载点 PID： 隔离进程 ID UTS： 隔离主机名和域名 User： 隔离用户 ID 和 组 ID Namespace 和 Cgroup 的使用是灵活的，同时也有不少需要注意的地方，因此直接操作 Namespace 和 Cgroup 并不是很容易。正是因为这些原因，Docker 通过 Libcontainer 来处理这些底层的事情。这样一来，Docker 只需简单地调用 Libcontainer 的 API ，就能将完整的容器搭建起来。而作为 Docker 的用户，就更不用操心这些事情了。 ","date":"2017-07-17","objectID":"/posts/docker/:1:4","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"容器造就 Docker 关于容器是否是 Docker 的技术核心技术，业界一直存在着争议。 在理解了容器，理解了容器的核心技术 Cgroup 和 Namespace，理解了容器技术如何巧妙且轻量地实现“容器”本身的资源控制和访问隔离之后，可以看到 Docker 和容器是一种完美的融合和辅助相成的关系，它们不是唯一的搭配，但一定是最完美的结合（目前来说）。与其说是容器造就了 Docker ， 不如说是它们造就了彼此，容器技术让 Docker 得到更多的应用和推广，Docker 也使得容器技术被更多人熟知。 ","date":"2017-07-17","objectID":"/posts/docker/:1:5","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"基本操作 ","date":"2017-07-17","objectID":"/posts/docker/:2:0","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"启动容器 新建并启动 所需的命令是 docker run 例如： $ docker run ubuntu:14.04 /bin/echo 'hello, worl' 容器执行后面的命令直接就会终止 . 下面的命令会启动容器并起一个 bash 终端,允许用户进行交互 $ docker run -t -i ubuntu:14.04 /bin/bash 其中 -t 让 Docker 分配一个伪终端 (pseudo-tty) 并绑定到容器的标准输入上, -i 则让容器的标准输入保持打开 . 利用 docker run 来创建容器是, Docker 在后台运行的标准操作包括: 检查本地是否存在指定的镜像,不存在就从共有仓库下载 利用镜像创建并启动一个容器 分配一个文件系统并在只读的镜像层外面挂载一层可读写层 在宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去 从地址池配置一个 ip 地址给容器 执行用户指定的应用程序 执行完毕后容器终止 启动已终止容器 可以利用 docker start 命令,直接将一个已经终止的容器启动运行 . 可以通过 docker ps -a 查看所有的容器和其状态 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES aada74689bf7 cockroachdb/cockroach \"/cockroach/cockro...\" 3 weeks ago Exited (137) 3 weeks ago roach_master 2e9eb6cf3f66 owncloud \"/entrypoint.sh ap...\" 3 weeks ago Up 3 weeks 0.0.0.0:80-\u003e80/tcp owncloud 91290c737c73 postgres \"docker-entrypoint...\" 3 weeks ago Up 3 weeks 5432/tcp owncloud-postgres 8f546ec65e61 mysql \"docker-entrypoint...\" 3 weeks ago Up 3 weeks 0.0.0.0:3306-\u003e3306/tcp mysql 不难发现 name 为 roch_master 的容器已经终止了,想重新启动它,可以执行下面的命令 $ docker start aada74689bf7 参数为容器的 id . ","date":"2017-07-17","objectID":"/posts/docker/:2:1","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"后台( background )运行 在很多时候,我们需要让 docker 在后台运行而并不是把执行结果直接输出出来. 这个时候我们可以添加 -d 参数来实现 如果使用 -d 参数运行容器 $ docker run -d mysql:5.7.17 77b2dc01fe0f3f1265df143181e7b9af5e05279a884f4776ee75350ea9d8017a 只会输出运行的容器 id, 而输出结果可以用 docker logs 查看 . $ docker logs [container ID or NAMES] ","date":"2017-07-17","objectID":"/posts/docker/:2:2","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"终止容器 可以使用 docker stop 来终止正在运行的容器 . 此外,当 Docker 容器中指定的应用终结时, 容器也自动终止 . 例如运行一个容器时,指定了一个终端后,当退出终端的时候,所创建的容器也会立刻终止 . 终止状态的容器, 可以通过 docker start 来重新启动 . 此外,docker restart 命令会将一个运行态的容器终止,然后重新启动它 . ","date":"2017-07-17","objectID":"/posts/docker/:2:3","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"进入容器 在使用 -d 参数时, docker 容器会在后台运行. 有些时候需要进入容器,如运行数据库时,需要进入增删改查库里的内容. 进入容器有很多种办法. attach 命令 docker attach 是 Docker 自带的命令,用法\u0008 但是使用 attach 命令有个缺陷,即多个窗口同时用 attach 命令到同一个容器的时候,所有的窗口都是同步显示的,如果其中一个窗口阻塞的时候,其他窗口也无法使用 . nsenter 命令 这个工具需要用如下命令安装 $ docker run --rm -v /usr/local/bin:/target jpetazzo/nsenter 使用方法也比较简单,首先是你要进入的容器的 ID $ PID=$(docker inspect --format {{.State.Pid}} \u003ccontainer ID or NAMES\u003e) 然后通过这个 PID 进入容器 $ nsenter --target $PID --mount --uts --ipc --net --pid 如果无法通过上述的命令连接到容器,有可能是因为宿主的默认 shell 在容器中并不存在,比如 zsh, 可以使用如下命令显示地使用 bash . exec 命令 $docker exec -it [container ID or NAMES] -i -t 前面说过为了标准输入输出保持打开 . ","date":"2017-07-17","objectID":"/posts/docker/:2:4","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"导出和导入容器 导出容器 如果要导出本地某个容器,可以使用 docker export 命令 . $ docker export [container ID or NAMES] \u003e target.tar 这样将导出容器快照到本地文件 . 导入容器快照 可以使用 docker import 从容器快照文件导入镜像, $ cat target.tar | docker import - test/mysql:v1.0 $ sudo docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE test/ubuntu v1.0 9d37a6082e97 About a minute ago 171.3 MB 此外,还可以通过指定 URL 或者某个目录来导入 $ docker import http://example.com/exampleimage.tgz example/imagerepo *注：用户既可以使用 docker load 来导入镜像存储文件到本地镜像库,也可以使用 docker import 来导入一个容器快照到本地镜像库 .这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态）,而镜像存储文件将保存完整记录,体积也要大 .此外,从容器快照文件导入时可以重新指定标签等元数据信息 . ","date":"2017-07-17","objectID":"/posts/docker/:2:5","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"删除容器 单独删除 可以使用 docker rm 来删除一个处于终止状态的容器 . $ docker rm [container ID or NAMES] 如果要删除一个运行中的容器,可以添加 -f 参数 .Docker 会发送 SIGKILL 信号给容器 . 清理所有处于终止状态的容器 用 docker ps -a 命令可以查看所有已创建的包括终止状态的容器,如果想批量删除多个容器的话(当然是终止状态的容器) ,可以用这个命令 $ docker rm $(docker ps -a -q) *注意：这个命令其实会试图删除所有的包括还在运行中的容器,不过就像上面提过的 docker rm 默认并不会删除运行中的容器 . ","date":"2017-07-17","objectID":"/posts/docker/:2:6","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"访问仓库 仓库（Repository）是集中存放镜像的地方 . 一个容易混淆的概念是注册服务器（Registry） .实际上注册服务器是管理仓库的具体服务器,每个服务器上可以有多个仓库,而每个仓库下面有多个镜像 .从这方面来说,仓库可以被认为是一个具体的项目或目录 .例如对于仓库地址dl.dockerpool.com/ubuntu 来说, dl.dockerpool.com 是注册服务器地址, ubuntu 是仓库名 . 大部分时候,并不需要严格区分这两者的概念 . ","date":"2017-07-17","objectID":"/posts/docker/:3:0","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"Docker Hub 目前 Docker 官方维护了一个公共仓库 Docker Hub, 但是开始把阵地移到 Docker Store 这个平台上,其上能找到几乎所有的能想得到的容器, 不可小觑 . 登录 可以通过执行 docker login 命令来输入用户名、密码和邮箱来完成注册和登录 . 注册成功后,本地用户目录的.dockercfg 中将保存用户的认证信息 . 基本操作 用户无需登录即可通过 docker search 命令来查找官方仓库中的镜像, 并利用 docker pull 命令来将它下载到本地 . 以搜索 mongo 为关键字搜索: $ docker search mongo NAME DESCRIPTION STARS OFFICIAL AUTOMATED mongo MongoDB document databases provide high av... 3427 [OK] mongo-express Web-based MongoDB admin interface, written... 168 [OK] mvertes/alpine-mongo light MongoDB container 51 [OK] mongoclient/mongoclient Official docker image for Mongoclient, fea... 29 [OK] torusware/speedus-mongo Always lastmod official MongoDB docker ima... 9 [OK] mongooseim/mongooseim-docker MongooseIM server the latest stable version 9 [OK] ​搜索结果可以看到很多包含关键字的镜像,其中包括镜像名字、描述、星数（表示该镜像的受欢迎程度）、是否官方创建、是否自动创建 . 官方的镜像说明是官方项目组创建和维护的,automated 资源允许用户验证镜像的来源和内容 . ​根据是否为官方提供, 镜像资源可分为两类 . 一类是累类似 mongo这样的基础镜像 . 这些镜像由 Docker 的用户创建、验证、支持、提供 . 这样的镜像往往是使用单个单词作为名字 . 另一种类型,比如mvertes/alpine-mongo 镜像,它是由 Docker 的用户创建并维护的,往往带有用户名称前缀 . 可以通过前缀 user_name/ 来指定使用某个用户提供的镜像 . 另外,在查找的时候通过 -s N 参数可以指定仅显示星数为 N 以上的镜像 （新版本的 Docker 推荐使用 --flter=stars=N 参数） . 下载镜像到本地 $ sudo docker pull centos Pulling repository centos 0b443ba03958: Download complete 539c0211cd76: Download complete 511136ea3c5a: Download complete 7064731afe90: Download complete 用户也可以登录之后通过 docker push 命令来讲镜像推送到 Docker Hub . 自动创建 ​自动创建（automated builds）功能对于需要经常升级镜像内程序来说,十分方便 .有时候,用户创建了镜像安装了某个软件,如果软件发布新版本则需要手动更新镜像 . .而自动创建允许用户通过 Docker Hub 指定跟踪一个目标网站（目前支持 GitHub或 BitBucket）上的项目,一旦项目发生新的提交,则自动执行创建 . 要配置自动创建,包括如下的步骤： 创建并登录 Docker Hub,以及目标网站； 在目标网站中连接帐户到 Docker Hub； 在 Docker Hub 中 配置一个自动创建； 选取一个目标网站中的项目（需要含 Dockerfile）和分支； 指定 Dockerfile 的位置,并提交创建 . 之后,可以 在Docker Hub 的 自动创建页面 中跟踪每次创建的状态 . ","date":"2017-07-17","objectID":"/posts/docker/:3:1","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"私有仓库 有时候使用 Docker Hub 这样的公共仓库由于网络等原因可能不方便,用户可以创建一个本地仓库供私人使用 . 需要用到 docker-registry 工具 . docker-registry 是官方提供的工具,可以用于构建私有的镜像仓库 . 安装运行 docker-registry 容器运行 在安装了 Docker 后,可以通过获取官方 registry 镜像来运行 . $ docker run -d -p 5000:5000 registry 这将使用官方的 registry 镜像来启动本地的私有仓库 .用户可以通过制定参数来配置私有仓库位置,例如配置镜像存储到 Amazon S3 服务 . $ sudo docker run \\ -e SETTINGS_FLAVOR=s3 \\ -e AWS_BUCKET=acme-docker \\ -e STORAGE_PATH=/registry \\ -e AWS_KEY=AKIAHSHB43HS3J92MXZ \\ -e AWS_SECRET=xdDowwlK7TJajV1Y7EoOZrmuPEJlHYcNP2k4j49T \\ -e SEARCH_BACKEND=sqlalchemy \\ -p 5000:5000 \\ registry 此外,还可以指定本地路径（如/home/user/registry-conf ）下的配置文件 . $ sudo docker run -d -p 5000:5000 -v /home/user/registry-conf:/r egistry-conf -e DOCKER_REGISTRY_CONFIG=/registry-conf/config.yml registry 默认情况下,仓库会被创建在容器的 /var/lib/registry 下 .可以通过 -v 参数来将镜像文件存放在本地的指定路径 . 例如下面的例子将上传的镜像放到 /opt/data/registy 目录 . $ sudo docker run -d -p 5000:5000 -v /opt/data/registry:/var/lib /registry registry 本地安装 对于 Ubuntu 或 CentOS 等发行版,可以直接安装 . Ubuntu $ sudo apt-get install -y build-essential python-dev libevent-dev python-pip liblzma-dev $ sudo pip install docker-registry CentOS $ sudo yum install -y python-devel libevent-devel python-pip gcc xz-devel $ sudo python-pip install docker-registry 也可以从 docker-registry 项目下载源码进行安装 . $ sudo apt-get install build-essential python-dev libevent-dev python-pip libssl-dev liblzma-dev libffi-dev $ git clone https://github.com/docker/docker-registry.git $ cd docker-registry $ sudo python setup.py install 然后修改配置文件,主要修改 dev 模板段的 storage_path 到本地的存储仓库的路径 . $ cp config/config_sample.yml config/config.yml 之后启动 web 服务 . $ sudo gunicorn -c contrib/gunicorn.py docker_registry.wsgi:application 或者 $ sudo gunicorn --access-logfile - --error-logfile - -k gevent -b 0.0.0.0:5000 -w 4 --max-requests 100 docker_registry.wsgi:application 此时使用 crul 访问本地的 5000 端口,看到输出 docker-registry 的版本信息说明运行成功 . *注 ： config/config_sample.yml 文件时示例配置文件 在私有仓库上传、下载、搜索镜像 创建好私有仓库之后,就可以使用 docker tag 来标记一个镜像,然后推送它到仓库,别的机器上就可以下载了 .如 私有仓库地址为 1192.168.7.26:5000 先在本机上查看已有的镜像 . $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE node latest f93ba6280cbd 3 weeks ago 667MB cockroachdb/cockroach latest 404f7ee26d38 4 weeks ago 163MB postgres latest ca3a55649cfc 7 weeks ago 269MB tomcat latest 0785a1d16826 7 weeks ago 367MB owncloud latest 2327c8d59618 8 weeks ago 572MB mysql latest e799c7f9ae9c 2 months ago 407MB 使用 docker tag 将 tomcat 这个镜像标记为 192.168.7.26：5000/test [root@vultr ~]# docker tag tomcat 192.168.7.26:5000/test [root@vultr ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE node latest f93ba6280cbd 3 weeks ago 667MB cockroachdb/cockroach latest 404f7ee26d38 4 weeks ago 163MB postgres latest ca3a55649cfc 7 weeks ago 269MB 192.168.7.26:5000/test latest 0785a1d16826 7 weeks ago 367MB tomcat latest 0785a1d16826 7 weeks ago 367MB owncloud latest 2327c8d59618 8 weeks ago 572MB mysql latest e799c7f9ae9c 2 months ago 407MB 用 docker push 上传标记的镜像 . $ docker push 192.168.7.26:5000/test The push refers to a repository [192.168.7.26:5000/test] (len: 1) Sending image list Pushing repository 192.168.7.26:5000/test (1 tags) Image 511136ea3c5a already pushed, skipping Image 9bad880da3d2 already pushed, skipping Image 25f11f5fb0cb already pushed, skipping Image ebc34468f71d already pushed, skipping Image 2318d26665ef already pushed, skipping Image ba5877dc9bec already pushed, skipping Pushing tag for rev [ba5877dc9bec] on {http://192.168.7.26:5000/ v1/repositories/test/tags/latest} 用 curl 查看仓库中的镜像 curl http://192.168.7.26:5000/v1/search {\"num_results\": 7, \"query\": \"\", \"results\": [{\"description\": \"\",\"name\": \"library/miaxis_j2ee\"}, {\"description\": \"\", \"name\": \"library/tomcat\"}, {\"description\": \"\", \"name\": \"library/ubuntu\"}, {\"description\": \"\", \"name\": \"library/ubuntu_office\"}, {\"description\": \"\", \"name\": \"library/desktop_ubu\"}, {\"description\": \"\", \"name\": \"dockerfile/ubuntu\"}, {\"description\": \"\", \"name\": \"library/test\"}]} 这里可以看到 {\"description\": \"\", \"name\": \"library/test\"} ,表面镜像已经上传成功了 . 下载可以用另一台机器去下载这个镜像 . $ docker pull 192.168.7.26","date":"2017-07-17","objectID":"/posts/docker/:3:2","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"仓库配置文件 Docker 的 registry 利用配置文件提供 了一些仓库的模板（flavor）,用户可以直接使用它们来进行开发或身产环境 . 模板 在 config_sample.yml 文件中,可以看到一些现成的模板段： common ：基础配置 local ：存储数据到本地文件系统 s3 ：存储数据到 AWS S3 中 dev ：使用 local 模板的基本配置 test ：单元测试使用 prod ：生产环境配置（基本上跟s3配置类似） gcs ：存储数据到 Google 的云存储 swift ：存储数据到 OpenStack Swift 服务 glance ：存储数据到 OpenStack Glance 服务,本地文件系统为后备 glance-swift ：存储数据到 OpenStack Glance 服务,Swift 为后备 elliptics ：存储数据到 Elliptics key/value 存储 用户可以添加自定义的模板段 . 默认情况下使用的模板是 dev ,要是使用某个模板作为默认值,可以添加 SETTING-FLAVOR 到环境变量中去, export SETTING_FLAVOR=dev 另外,配置文件中支持从环境变量中加载,语法格式为 _env:VARIABLENAME[:DEFAULT] 示例配置 common: loglevel: info search_backend: \"_env:SEARCH_BACKEND:\" sqlalchemy_index_database: \"_env:SQLALCHEMY_INDEX_DATABASE:sqlite:////tmp/docker-re gistry.db\" prod: loglevel: warn storage: s3 s3_access_key: _env:AWS_S3_ACCESS_KEY s3_secret_key: _env:AWS_S3_SECRET_KEY s3_bucket: _env:AWS_S3_BUCKET boto_bucket: _env:AWS_S3_BUCKET storage_path: /srv/docker smtp_host: localhost from_addr: docker@myself.com to_addr: my@myself.com dev: loglevel: debug storage: local storage_path: /home/myself/docker test: storage: local storage_path: /tmp/tmpdockertmp ","date":"2017-07-17","objectID":"/posts/docker/:3:3","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"Docker 数据管理 在容器管理中数据主要有两种方式： 数据卷 （Data volumes） 数据卷容器 （Data volume containers） ","date":"2017-07-17","objectID":"/posts/docker/:4:0","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"数据卷 数据卷是一个可提供一个或多个容器使用的特殊目录,它绕过 UFS, 可以提供很多有用的特征： 数据卷可以再荣期间共享和重用 对数据卷的修改立马生效 对数据及的更新,不会影响镜像 数据卷默认会一直存在,即使容器被删除 注：数据卷的使用,类似于Linux 下对目录或文件进行 mount, 镜像中的被指定为挂载点的目录中的文件会隐藏掉,能显示看的是挂载的数据卷 创建一个数据卷 ​在使用 docker run 命令的时候,使用 -v 参数来创建一个数据卷并挂载到容器里 .在一次 run 中可以挂载多个数据卷 . 下面创建一个名为 web 的容器,并加载一个数据卷到容器的 /webapp 目录 . $ docker run -d -p --name web -v /webapp training/webapp python app.py 注：也可以在 Docker 中使用 volume 来添加一个或多个新的卷到有该镜像创建的任意容器 . 删除数据卷 数据卷是被设计用来持久化数据的,它的生命周期独立于容器,Docker 不会在容器被删除后自动删除数据卷,并且也不存在垃圾回收这样的机制来处理没有任何容器引用的数据卷 .日光需要在删除容器的同时移除数据卷,可以再删除容器的时候使用 docker rm -v 这个命令 . 挂载一个主句目录作为数据卷 使用 -v 参数也可以指定挂载一个本地主机的目录到容器中去 . $ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py ​ 上面的命令加载主机的 /src/webapp 目录到容器的 /opt/webapp 目录 .这个功能在进行测试的时候十分方便,比如用户可以放置一些程序到本地目录中,来查看容器是否正常工作 .本地目录的路径必须是绝对路径,如果目录不存在 Docker会自动为你创建它 . 注：Dockerfile 中不支持这种用法,因为 Dockerfile 是为了移植和分享用的 . 然而,不同的操作系统的路径格式不一样,所以目前还不支持 Docker 挂载数据卷的默认权限是读写, 用户也可以通过 :ro 指定为只读 $ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp:ro training/webapp python app.py 加了 :ro 之后,就挂载为只读了 . 查看数据卷的具体信息 在主机里使用以下命令可以查看指定容器的信息 $ docker inspect web ... 在输出的内容中找到其中和数据卷相关的部分,可以看到所有的数据卷都是创建在主句的 /var/lib/docker/volumes/ 下面的 \"Volumes\": { \"/webapp\": \"/var/lib/docker/volumes/fac362...80535\" }, \"VolumesRW\": { \"/webapp\": true } ... 注：从 Docker 1.8.0 起,数据卷配置在 “Mounts” Key 下面, 可以看到所有的数据卷都是创建在主机的 /mnt/sda1/var/lib/docker/volumes/... 下面了 . \"Mounts\": [ { \"Name\": \"b53ebd40054dae599faf7c9666acfe205c3e922 fc3e8bc3f2fd178ed788f1c29\", \"Source\": \"/mnt/sda1/var/lib/docker/volumes/b53e bd40054dae599faf7c9666acfe205c3e922fc3e8bc3f2fd178ed788f1c29/_data\", \"Destination\": \"/webapp\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" } ] ... 挂载一个本地主机文件作为数据卷 -v 参数也可以从主机挂载单个文件到文件到容器中 $ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash 这样就可以记录在容器输入过得命令了 . ","date":"2017-07-17","objectID":"/posts/docker/:4:1","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"数据卷容器 如果你有一些持续更新的数据需要在容器之间共享,最好创建数据卷容器 . 数据卷容器,其实就是一个正常的容器,专门用来提供数据卷供其他容器挂载的 . 首先,创建一个名为 dbdata 的数据卷容器： $ sudo docker run -d -v /dbdata --name dbdata training/postgres echo Data-only container for postgres 然后,在其他容器中使用 --volumes-from 来挂载 dbdata 容器中的数据卷 . $ sudo docker run -d --volumes-form dbdata --name db1 training/postgres $ sudo docker run -d --volumes-form dbdata --name db2 training/postgres 可以使用超过一个的--volumes-from 参数来指定从多个容器挂载不同的数据卷 . 也可以从其他已经挂载了数据卷的容器来级联挂载数据卷 . $ docker run -d --name db3 --volumes-from db1 training/postgres 注：使用 --volumes-from 参数所挂载数据卷的容器自己并不需要保持运行状态 如果删除了挂载的容器（包括 dbdata、db1 和 db2 ）,数据卷并不会被自动删除 .如果删除一个数据卷,必须在删除最后一个还挂着它的容器时使用 docker rm -v 命令来指定同时删除关联的容器 .这可以让用户在容器之间升级和移到数据卷 . 利用数据卷容器来备份、恢复、迁移数据卷 可以利用数据卷对其中的数据进行备份、恢复和迁移 . 备份 首先使用 --volumes-from 标记来创建一个加载 dbdata 数据卷的容器,并从主机挂载当前目录到容器的 /backup 目录 .命令如下： $ sudo docker run --volumes-from dbdata -v$(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata 容器启动后,使用了 tar 命令来将 dbdata 卷备份为容器中 /backup/backup.tar 文件,也就是主机当前目录下的名为 backup.tar 的文件 . 恢复 如果要恢复数据到一个容器,首先创建一个带有空数据卷的容器 dbdata2 . $ docker run -v /dbdata --name dbdata2 ubuntu /bin/bash 然后创建另一个容器,挂载 dbdata2 容器卷中的数据卷,并使用 untar 解压备份文件到挂载的容器卷中 . $ sudo docker run --volumes-form dbdata2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar 为了查看/验证恢复的数据,可以再启动一个容器挂载同样的容器卷来查看 $ docker run --volumes-from dbdata2 busybox /bin/ls dbdata 迁移数据卷 代写 . . . Docker 中的网络 Docker 允许通过外部访问容器或容器互联的方式来提供网络服务 . ","date":"2017-07-17","objectID":"/posts/docker/:4:2","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"外部访问容器 容器中可以与运行一些网络应用,要让外部也可以访问这些应用,可以通过 -P 或 -p 参数来指定端口映射 . 当使用 -P 参数时,Docker 会随机映射一个 49000~49900 的端口到内部容器开放的网络端口 . 使用 docker ps 可以看到,本地主机的49155 被映射到了容器的5000 端口 . 此时访问本机的49155 端口即可访问容器内 web 应用提供的界面 . $ sudo docker run -d -P training/webapp python app.py $ sudo docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES bc533791f3f5 training/webapp:latest python app.py 5 seconds ag o Up 2 seconds 0.0.0.0:49155-\u003e5000/tcp nostalgic_morse -P （小写）则可以指定要映射的端口,并且在一个指定端口上只可以绑定一个容器 .支持的格式有 ip:HostPort:containerPort ip::containerPort hostPort:containerPort ","date":"2017-07-17","objectID":"/posts/docker/:5:0","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"映射所有接口地址 使用 hostPort ：containerPort 格式本地的5000端口映射到容器的5000端口,可以执行 $ docker run -d -p 5000:5000 training/webapp python app.py 此时默认会绑定本地所有接口上的所有接口 . ","date":"2017-07-17","objectID":"/posts/docker/:5:1","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"映射到指定地址的指定端口 可以使用 ip:hostPort:containerPort 格式指定映射使用一个特定地址,比如 localhost 地址 127.0.0.1 $ sudo docker run -d -p 127.0.0.1:5000:5000 training/webapp python app.py ","date":"2017-07-17","objectID":"/posts/docker/:5:2","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"查看映射端口配置 使用 docker port 来查看当前映射的端口配置,也可以查看到绑定的地址 $ docker port gogs 22/tcp -\u003e 0.0.0.0:10022 3000/tcp -\u003e 0.0.0.0:10080 可以看到 gogs 有两个容器内的端口 22, 3000 分别映射主机的10022,10080 端口 . 注： -p 可以多次使用来绑定多个端口,也就是说一条命令可以有多个 -p ,如：上面👆的 gogs 容器就绑定了俩端口 ","date":"2017-07-17","objectID":"/posts/docker/:5:3","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"容器互联 容器的连接（linking）系统是除了端口映射外,另一种跟容器中应用交互的方式 .该系统会在源和接受容器之间创建一个通道,接受容器可以看到源容器指定的信息 . ","date":"2017-07-17","objectID":"/posts/docker/:6:0","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"自定义容器命名 连接系统依据容器的名称来执行 .因此,首先需要自定义一个好记的容器命名 . 虽然创建容器的时候,系统默认会分配给一个名字 .但是自定义命名容器的话,第一,好记,第二,可以作为有用的参考的 . 使用 --name 参数可以为容器自定义命名 . $ docker run -d -p 8181:4040 --name own-cloud owncloud 使用 docker ps 来查看正运行的容器 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2c2e766e86fd owncloud \"/entrypoint.sh ap...\" 23 hours ago Up 23 hours 80/tcp, 0.0.0.0:8181-\u003e4040/tcp own-cloud 使用 docker inspect 命令来查看容器名字 $ docker inspect -f \"{{.Name}}\" 2c2e766e86fd /own-cloud 注：容器的名称是唯一的 .如果已经命名了一个叫 own-cloud 的容器,当你再次使用这个名词的时候,需要先把之前的的同名容器删除 tips：在执行 docker run 的时候可以添加 —rm 参数,这样容器在终止后立刻删除 .注意,—rm 和 -d 参数不能同时使用 . ","date":"2017-07-17","objectID":"/posts/docker/:6:1","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"容器互联 使用 --link 参数可以让容器之间安全的进行交互 . 下面是,运行 Nginx 容器的时候把 gogs 这个容器连接上 docker run -d --name my_nginx --link gogs:app --link own-cloud:app2 -p 80:80 -v /root/nginx/config:/etc/nginx/conf.d nginx 此时,gogs 容器和 my_nginx 容器建立互联关系 --link 参数的格式为 --link name:alias ,其中 name 是要连接的容器名称, alias 是这个连接的别名 . 可以通过 docker inspect 命令查看 my_nginx 容器信息,就会发现有这么一段信息 \"Links\": [ \"/gogs:/trusting_brown/app\", \"/own-cloud:/trusting_brown/app2\" ], 表面此容器已经连上两个容器, gogs 和 own-cloud,trusting_brown 是系统分配给 Nginx 的名称,连接名称分别是 app 和 app2 . Docker 在两个互联的容器之间创建了一个安全的隧道,而且不用映射到它们的端口到主机上 .在启动被连接的容器的时候不用添加 -p 或 -P 参数,从而避免暴露端口到外部网络上 . 连接之后,在 Nginx 容器里,就会发生两个变化 . 一是环境变量 .在 Nginx 容器中会出现6个新增的环境变量,这些环境变量的名称分贝时由被连接的服务别名、端口等拼接而成的 . 由于起得 gogs 容器有两个端口,所以其中 APP_PORT、APP_NAME、APP_ENV_GOGS_CUSTOM 是公用的,其它8个变量每四个的分别对应22, 3000 端口 # env | grep APP APP_PORT_3000_TCP=tcp://172.17.0.2:3000 APP_PORT_22_TCP_PROTO=tcp APP_ENV_GOGS_CUSTOM=/data/gogs APP_PORT_3000_TCP_ADDR=172.17.0.2 APP_PORT_3000_TCP_PROTO=tcp APP_PORT_22_TCP_PORT=22 APP_PORT_3000_TCP_PORT=3000 APP_PORT=tcp://172.17.0.2:22 APP_NAME=/my_nginx/app APP_PORT_22_TCP=tcp://172.17.0.2:22 APP_PORT_22_TCP_ADDR=172.17.0.2 二是 hosts 文件 .在 Nginx 容器的 hosts 文件看到下面的记录 .这就是说,一切访问 连接别名（app）、容器 ID（ac4c0cf35adf）和容器名（gogs）的请求都会被重新导向到实时实际的 app 的 ip 地址上 . # cat /etc/hosts | grep app 172.17.0.2 app ac4c0cf35adf gogs ","date":"2017-07-17","objectID":"/posts/docker/:6:2","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"高级网络配置 当 Docker 启动时,会自动的主机上创建一个 docker0 虚拟网桥,实际上是 Linux 的一个 bridge,可以理解为一个软件交换机 .它会挂载到它的网口之间进行转发 . $ ip addr | grep docker0 docker0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc noqueue state UP link/ether 02:42:23:c6:3f:1c brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:23ff:fec6:3f1c/64 scope link valid_lft forever preferred_lft forever 同时,Docker 随机分一个本地未占用的私有网段（在 RFC1919 中定义）中的一个地址给 docker0 接口 .比如我的主机上的 docker0 ip 为 172.17.0.1 ,掩码为 255.255.0.0 .此后启动的容器内的网口也会自动分配有个一个同一网段（172.17.0.0/16）的地址 . 当创建一个 Docker 容器的时候,同时会创建一对 vath pair 接口（当数据包发送到一个接口,另一个接口也可以收到相同的数据包） .这对接口一段在容器内,即 eth0 ；另一端在本地并挂载到 docker0 网桥,名称以 veth 开头 .通过这种方式,主机可以跟容器通信,容器之间也可以相互通信 . Docker 就创建了在主机和所有容器之间一个虚拟共享网络 . ​ 图 i.i docker 网络 接下来部分将介绍在一些场景中,Docker 所有的网络定制配置 .以及通过 Linux 命令来调整、补充、甚至替换 Docker 默认的网络配置 . ","date":"2017-07-17","objectID":"/posts/docker/:7:0","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["Docker"],"content":"快速配置 下面是一个跟 Docker 网络相关的命令列表 . 其中有些命令选项只有在 Docker 服务启动的时候才能配置,而且不能马上生效 . -b BRIDGE or --bridge==BRIDGE –指定容器挂载的网桥 --bip=CIDR — 定制 docker0 的掩码 -H SOCKET... or --host=SOCKET… —Docker 服务端接受命令的通道 --icc=true|false –是否支持容器之间进行通信 --ip-forward=true|false —容器是否能访问外网（详细解析请看下文的容器通信） --iptables=true|false –是否允许 Docker 添加 iptables 规则 --mtu=BYTES —容器网络中的 MTU 下面的两个命令既可以在服务启动时指定,也可以 Docker 容器启动（docker run ）时候指定 . 在 Docker 服务启动的时候指定则会成为默认值,后面执行docker run 时可以覆盖设置的默认值 . --dns=IP_ADDRESS… —使用指定的 DNS 服务器 --dns-search=DOMAIN... 指定 DNS 搜索域 最后这些选项只有在 docker run 执行时使用,因为它是针对容器的特性内容 . -h HOSTNAME or --hostname=HOSTNAME –配置容器主机名 --link=CONRATAINER_NAME:ALIAS —添加到另一个容器的连接 ","date":"2017-07-17","objectID":"/posts/docker/:7:1","tags":["go","docker"],"title":"Docker 基础知识和基本操作","uri":"/posts/docker/"},{"categories":["源码解读"],"content":"go 的 interface 的实现和原理。 Go interface ","date":"2017-06-08","objectID":"/posts/go-interface/:0:0","tags":["go"],"title":"GO interface","uri":"/posts/go-interface/"},{"categories":["源码解读"],"content":"interface 在 Golang 中 interface 是一个很重要的概念和特性。 ","date":"2017-06-08","objectID":"/posts/go-interface/:1:0","tags":["go"],"title":"GO interface","uri":"/posts/go-interface/"},{"categories":["源码解读"],"content":"什么是 interface？ In object-oriented programming, a protocol or interface is a common means for unrelated objects to communicate with each other. These are definitions of methods and values which the objects agree upon in order to co-operate. — wikipedia 这是 wikipedia 关于 protocal 的定义，将 interface 类比如 protocal 是一种非常助于理解的方式。protocol，中文一般叫做协议，比如网络传输中的 TCP 协议。protocol 可以认为是一种双方为了交流而做出的约定，interface 可以类比如此。 在 Golang 中，interface 是一种抽象类型，相对于抽象类型的是具体类型（concrete type）：int，string。如下是 io 包里面的例子。 // Writer is the interface that wraps the basic Write method. // // Write writes len(p) bytes from p to the underlying data stream. // It returns the number of bytes written from p (0 \u003c= n \u003c= len(p)) // and any error encountered that caused the write to stop early. // Write must return a non-nil error if it returns n \u003c len(p). // Write must not modify the slice data, even temporarily. // // Implementations must not retain p. type Writer interface { Write(p []byte) (n int, err error) } // Closer is the interface that wraps the basic Close method. // // The behavior of Close after the first call is undefined. // Specific implementations may document their own behavior. type Closer interface { Close() error } 在 Golang 中，interface 是一组 method 的集合，是 duck-type programming (鸭子类型)的一种体现。不关心属性（数据），只关心行为（方法）。具体使用中你可以自定义自己的 struct，并提供特定的 interface 里面的 method 就可以把它当成 interface 来使用。下面是一种 interface 的典型用法，定义函数的时候参数定义成 interface，调用函数的时候就可以做到非常的灵活。 type MyInterface interface{ Print() } func TestFunc(x MyInterface) {} type MyStruct struct {} func (me MyStruct) Print() {} func main() { var me MyStruct TestFunc(me) } ","date":"2017-06-08","objectID":"/posts/go-interface/:1:1","tags":["go"],"title":"GO interface","uri":"/posts/go-interface/"},{"categories":["源码解读"],"content":"为什么 interface Gopher China 上给出了下面的三个理由： writing generic algorithm （泛型编程） hiding implementation detail （隐藏具体实现） providing interception points （提供监听点/拦截点？） write generic algorithm 严格来说，在 Golang 中并不支持泛型编程。在 C++ 等高级语言中使用泛型编程非常的简单，所以泛型编程一直是 Golang 诟病最多的地方。但是使用 interface 我们可以实现泛型编程，我这里简单说一下，具体可以参考我前面给出来的那篇文章。比如我们现在要写一个泛型算法，形参定义采用 interface 就可以了，以标准库的 sort 为例。 package sort // A type, typically a collection, that satisfies sort.Interface can be // sorted by the routines in this package. The methods require that the // elements of the collection be enumerated by an integer index. type Interface interface { // Len is the number of elements in the collection. Len() int // Less reports whether the element with // index i should sort before the element with index j. Less(i, j int) bool // Swap swaps the elements with indexes i and j. Swap(i, j int) } ... // Sort sorts data. // It makes one call to data.Len to determine n, and O(n*log(n)) calls to // data.Less and data.Swap. The sort is not guaranteed to be stable. func Sort(data Interface) { // Switch to heapsort if depth of 2*ceil(lg(n+1)) is reached. n := data.Len() maxDepth := 0 for i := n; i \u003e 0; i \u003e\u003e= 1 { maxDepth++ } maxDepth *= 2 quickSort(data, 0, n, maxDepth) } Sort 函数的形参是一个 interface，包含了三个方法：Len()，Less(i,j int)，Swap(i, j int)。使用的时候不管数组的元素类型是什么类型（int, float, string…），只要我们实现了这三个方法就可以使用 Sort 函数，这样就实现了“泛型编程”。有一点比较麻烦的是，我们需要将数组自定义一下。下面是一个例子。 type Person struct { Name string Age int } func (p Person) String() string { return fmt.Sprintf(\"%s: %d\", p.Name, p.Age) } // ByAge implements sort.Interface for []Person based on // the Age field. type ByAge []Person //自定义 func (a ByAge) Len() int { return len(a) } func (a ByAge) Swap(i, j int) { a[i], a[j] = a[j], a[i] } func (a ByAge) Less(i, j int) bool { return a[i].Age \u003c a[j].Age } func main() { people := []Person{ {\"Bob\", 31}, {\"John\", 42}, {\"Michael\", 17}, {\"Jenny\", 26}, } fmt.Println(people) sort.Sort(ByAge(people)) fmt.Println(people) } 另外 Gopher China 上还提到了一个比较有趣的东西和大家分享一下。在我们设计函数的时候，下面是一个比较好的准则。 Be conservative in what you send, be liberal in what you accept. — Robustness Principle 对应到 Golang 就是： Return concrete types, receive interfaces as parameter. — Robustness Principle applied to Go 话说这么说，但是当我们翻阅 Golang 源码的时候，有些函数的返回值也是 interface。 hiding implement detail 隐藏具体实现，这个很好理解。比如我设计一个函数给你返回一个 interface，那么你只能通过 interface 里面的方法来做一些操作，但是内部的具体实现是完全不知道的。Francesc 举了个 context 的例子。 context 最先由 google 提供，现在已经纳入了标准库，而且在原有 context 的基础上增加了：cancelCtx，timerCtx，valueCtx。语言的表达有时候略显苍白无力，看一下 context 包的代码吧。 func WithCancel(parent Context) (ctx Context, cancel CancelFunc) { c := newCancelCtx(parent) propagateCancel(parent, \u0026c) return \u0026c, func() { c.cancel(true, Canceled) } } 表明上 WithCancel 函数返回的还是一个 Context interface，但是这个 interface 的具体实现是 cancelCtx struct。 // newCancelCtx returns an initialized cancelCtx. func newCancelCtx(parent Context) cancelCtx { return cancelCtx{ Context: parent, done: make(chan struct{}), } } // A cancelCtx can be canceled. When canceled, it also cancels any children // that implement canceler. type cancelCtx struct { Context //注意一下这个地方 done chan struct{} // closed by the first cancel call. mu sync.Mutex children map[canceler]struct{} // set to nil by the first cancel call err error // set to non-nil by the first cancel call } func (c *cancelCtx) Done() \u003c-chan struct{} { return c.done } func (c *cancelCtx) Err() error { c.mu.Lock() defer c.mu.Unlock() return c.err } func (c *cancelCtx) String() string { return fmt.Sprintf(\"%v.WithCancel\", c.Context) } 尽管内部实现上下面三个函数返回的具体 struct （都实现了 Context interface）不同，但是对于使用者来说是完全无感知的。 func WithCancel(parent Context) (ctx Context, cancel CancelFunc) //返回 cancelCtx func WithDeadline(parent Context, deadline time.Time) (Context, CancelFunc) //返回 timerCtx func WithValue(parent Context, key, val interface{}) Context //返回 valueCtx providing interception points 这里的 interception 想表达的意思应该是 wrapper 或者装饰器，他给出了一个例子如下： type header struct { rt http.RoundTripper v map[string]string } func (h header) RoundTrip(r *htt","date":"2017-06-08","objectID":"/posts/go-interface/:1:2","tags":["go"],"title":"GO interface","uri":"/posts/go-interface/"},{"categories":["源码解读"],"content":"非侵入式 什么是侵入式呢？比如 Java 的 interface 实现需要显示的声明。 public class MyWriter implements io.Writer {} 这样就意味着如果要实现多个 interface 需要显示地写很多遍，同时 package 的依赖还需要进行管理。Dependency is evil。比如我要实现 io 包里面的 Reader，Writer，ReadWriter 接口，代码可以像下面这样写。 type MyIO struct {} func (io *MyIO) Read(p []byte) (n int, err error) {...} func (io *MyIO) Write(p []byte) (n int, err error) {...} // io package type Reader interface { Read(p []byte) (n int, err error) } type Writer interface { Write(p []byte) (n int, err error) } type ReadWriter interface { Reader Writer } 这种写法真的很方便，而且不用去显示的 import io package，interface 底层实现的时候会动态的检测。这样也会引入一些问题： 性能下降。使用 interface 作为函数参数，runtime 的时候会动态的确定行为。而使用 struct 作为参数，编译期间就可以确定了。 不知道 struct 实现哪些 interface。这个问题可以使用 guru 工具来解决。 综上，Golang interface 的这种非侵入实现真的很难说它是好，还是坏。但是可以肯定的一点是，对开发人员来说代码写起来更简单了。 ","date":"2017-06-08","objectID":"/posts/go-interface/:1:3","tags":["go"],"title":"GO interface","uri":"/posts/go-interface/"},{"categories":["源码解读"],"content":"interface type assertion interface 像其他类型转换的时候一般我们称作断言，举个例子。 func do(v interface{}) { n := v.(int) // might panic } 这样写的坏处在于：一旦断言失败，程序将会 panic。一种避免 panic 的写法是使用 type assertion。 func do(v interface{}) { n, ok := v.(int) if !ok { // 断言失败处理 } } 对于 interface 的操作可以使用 reflect 包来处理，关于 reflect 包的原理和使用可以参考我的文章。 ","date":"2017-06-08","objectID":"/posts/go-interface/:1:4","tags":["go"],"title":"GO interface","uri":"/posts/go-interface/"},{"categories":["源码解读"],"content":"总结 interface 是 Golang 的一种重要的特性，但是这是以 runtime 为代价的，也就意味着性能的损失（关于 interface 的底层实现之后有时间再写）。抛开性能不谈，interface 对于如何设计我们的代码确实给了一个很好的思考。 ","date":"2017-06-08","objectID":"/posts/go-interface/:1:5","tags":["go"],"title":"GO interface","uri":"/posts/go-interface/"},{"categories":["源码解读"],"content":"参考 Golang “泛型编程” 谈一谈 Golang 的 interface 和 reflect understanding golang interface(Gopher China) — youtube understanding golang interface(Gopher China) — slide ","date":"2017-06-08","objectID":"/posts/go-interface/:2:0","tags":["go"],"title":"GO interface","uri":"/posts/go-interface/"},{"categories":["代码规范"],"content":"go 代码的一些规范和命名规则…… Golang 代码规范 ","date":"2017-06-04","objectID":"/posts/go-format/:0:0","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"项目目录结构 PROJECT_NAME ├── README.md 介绍软件及文档入口 ├── bin 编译好的二进制文件,执行./build.sh自动生成，该目录也用于程序打包 ├── build.sh 自动编译的脚本 ├── doc 该项目的文档 ├── pack 打包后的程序放在此处 ├── pack.sh 自动打包的脚本，生成类似xxxx.20170713_14:45:35.tar.gz的文件，放在pack文件下 └── src 该项目的源代码 ├── main 项目主函数 ├── model 项目代码 ├── research 在实现该项目中探究的一些程序 └── vendor 存放go的库 ├── github.com/xxx 第三方库 └── xxx.com/obc 公司内部的公共库 项目的目录结构尽量做到简明、层次明确。 ","date":"2017-06-04","objectID":"/posts/go-format/:1:0","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"命名规范 ","date":"2017-06-04","objectID":"/posts/go-format/:2:0","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"文件名命名规范 用小写，尽量见名思义，看见文件名就可以知道这个文件下的大概内容，对于源代码里的文件，文件名要很好的代表了一个模块实现的功能。 ","date":"2017-06-04","objectID":"/posts/go-format/:2:1","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"包名 包名用小写，使用短命名，尽量不要和标准库冲突。 ","date":"2017-06-04","objectID":"/posts/go-format/:2:2","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"接口名 单个函数的接口以 er 作为后缀，如 Reader， Writer 接口的实现则去掉后缀 type Reader interface { Read(p []byte) (int, error) } 两个函数的接口名综合两个函数名 type WriteFlusher interface { Write([]byte) (int, error) Flush() error } 三个以上函数的接口名，类似于结构体名 type Car interface { Start([]byte) Stop() error Recover() } ","date":"2017-06-04","objectID":"/posts/go-format/:2:3","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"变量 全局变量：采用驼峰命名法，仅限在包内的全局变量，包外引用需要写接口，提供调用； 局部变量：驼峰式，第一个单词的首字母小写，如有两个以上单词组成的变量名，第二个单词开始首字母大写。 ","date":"2017-06-04","objectID":"/posts/go-format/:2:4","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"常量 全局：驼峰命名，每个单词的首字母大写 局部：与变量的风格一样 ","date":"2017-06-04","objectID":"/posts/go-format/:2:5","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"函数名 函数名采用驼峰命名法，不要使用下划线。 ","date":"2017-06-04","objectID":"/posts/go-format/:3:0","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"import 规范 import在多行的情况下，goimports 会自动帮你格式化，在一个文件里面引入了一个package，建议采用如下格式： import ( \"fmt\" ) 如果你的包引入了三种类型的包，标准库包，程序内部包，第三方包，建议采用如下方式进行组织你的包： import { \"net\" \"strings\" \"github.com/astaxie/beego\" \"gopkg.in/mgo.v2\" \"myproject/models\" \"myproject/utils\" } 项目中最好不要使用相对路径导入包： // 这是不好的导入 import “../net” // 这是正确的做法 import “xxxx.com/proj/net” ","date":"2017-06-04","objectID":"/posts/go-format/:4:0","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"错误处理 error作为函数的值返回,必须尽快对error进行处理 采用独立的错误流进行处理 不要采用这种方式 if err != nil { // error handling } else { // normal code } 而采用以下方式 if err != nil { // error handling return // or continue, etc. } // normal code 如果返回值需要初始化，则采用以下方式 x, err := f() if err != nil { // error handling return } // use x ","date":"2017-06-04","objectID":"/posts/go-format/:5:0","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"panic 在逻辑处理中禁用panic 在 main 包中只有当实在不可运行的情况采用 panic，例如文件无法打开，数据库无法连接导致程序无法 正常运行，但是对于其他的 package 对外的接口不能有 panic，只能在包内采用。 建议在 main 包中使用 log.Fatal 来记录错误，这样就可以由 log 来结束程序。 ","date":"2017-06-04","objectID":"/posts/go-format/:5:1","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"Recover recover 用于捕获 runtime 的异常，禁止滥用 recover，在开发测试阶段尽量不要用 recover，recover 一般放在你认为会有不可预期的异常的地方。 func server(workChan \u003c-chan *Work) { for work := range workChan { go safelyDo(work) } } func safelyDo(work *Work) { defer func() { if err := recover(); err != nil { log.Println(\"work failed:\", err) } }() // do 函数可能会有不可预期的异常 do(work) } ","date":"2017-06-04","objectID":"/posts/go-format/:6:0","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"Defer defer 在函数 return 之前执行，对于一些资源的回收用 defer 是好的，但也禁止滥用 defer，defer 是需要消耗性能的,所以频繁调用的函数尽量不要使用 defer。 // Contents returns the file's contents as a string. func Contents(filename string) (string, error) { f, err := os.Open(filename) if err != nil { return \"\", err } defer f.Close() // f.Close will run when we're finished. var result []byte buf := make([]byte, 100) for { n, err := f.Read(buf[0:]) result = append(result, buf[0:n]...) // append is discussed later. if err != nil { if err == io.EOF { break } return \"\", err // f will be closed if we return here. } } return string(result), nil // f will be closed if we return here. } ","date":"2017-06-04","objectID":"/posts/go-format/:7:0","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"控制结构 ","date":"2017-06-04","objectID":"/posts/go-format/:8:0","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"if if接受初始化语句，约定如下方式建立局部变量 if err := file.Chmod(0664); err != nil { return err } ","date":"2017-06-04","objectID":"/posts/go-format/:8:1","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"for 采用短声明建立局部变量 sum := 0 for i := 0; i \u003c 10; i++ { sum += i } ","date":"2017-06-04","objectID":"/posts/go-format/:8:2","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"range 如果只需要第一项（key），就丢弃第二个： for key := range m { if key.expired() { delete(m, key) } } 如果只需要第二项，则把第一项置为下划线 sum := 0 for _, value := range array { sum += value } ","date":"2017-06-04","objectID":"/posts/go-format/:8:3","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"return 尽早return：一旦有错误发生，马上返回 f, err := os.Open(name) if err != nil { return err } d, err := f.Stat() if err != nil { f.Close() return err } codeUsing(f, d) ","date":"2017-06-04","objectID":"/posts/go-format/:8:4","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"方法接收器 名称一般采用 struct 的第一个字母且为小写， 而不是 this，me 或 self type Transfer struct{} func(t *Transfer) Get() {} 如果接收者是 map， slice 或者 chan，不要用指针传递 //Map package main import ( \"fmt\" ) type mp map[string]string func (m mp) Set(k, v string) { m[k] = v } func main() { m := make(mp) m.Set(\"k\", \"v\") fmt.Println(m) } //Channel package main import ( \"fmt\" ) type ch chan interface{} func (c ch) Push(i interface{}) { c \u003c- i } func (c ch) Pop() interface{} { return \u003c-c } func main() { c := make(ch, 1) c.Push(\"i\") fmt.Println(c.Pop()) } 如果需要对 slice 进行修改，通过返回值的方式重新复制 //Slice package main import ( \"fmt\" ) type slice []byte func main() { s := make(slice, 0) s = s.addOne(42) fmt.Println(s) } func (s slice) addOne(b byte) []byte { return append(s, b) } 如果接收者是含有 sync.Mutex 或者类似同步字段的结构体，必须使用指针传递避免复制 package main import ( \"sync\" ) type T struct { m sync.Mutex } func (t *T) lock() { t.m.Lock() } /* Wrong !!! func (t T) lock() { t.m.Lock() } */ func main() { t := new(T) t.lock() } 如果接收者是大的结构体或者数组，使用指针传递会更有效率。 package main import ( \"fmt\" ) type T struct { data [1024]byte } func (t *T) Get() byte { return t.data[0] } func main() { t := new(T) fmt.Println(t.Get()) } ","date":"2017-06-04","objectID":"/posts/go-format/:9:0","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"一键代码规范 使用 JetBrain 系列 IDE 的同学，可以按快捷键或者鼠标右键来一键使用 go 提供的 format 命令; 快捷键： cmd + option + shift + f 对当前文件进行 format cmd + option + shift + p 对当前项目所有 go 文件进行 format 鼠标右键： 在 IDE 内点击鼠标右键，选择 Go Tools,然后可以选择对单个文件或项目进行 format。 用 vscode 的同学，在设置里面加上以下语句即可以保存文件后自动进行 format \"go.formatOnSave\": true ","date":"2017-06-04","objectID":"/posts/go-format/:10:0","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":["代码规范"],"content":"总结 代码风格和代码规范是体现一个程序员的基本素质的一项指标，也是对自己的代码和他人的一个最基本的尊重。 ","date":"2017-06-04","objectID":"/posts/go-format/:11:0","tags":["go"],"title":"Go Format","uri":"/posts/go-format/"},{"categories":null,"content":"Go 测试用例 开发程序其中很重要的一点是测试，我们如何保证代码的质量，如何保证每个函数是可运行，运行结果是正确的，又如何保证写出来的代码性能是好的，我们知道单元测试的重点在于发现程序设计或实现的逻辑错误，使问题及早暴露，便于问题的定位解决，而性能测试的重点在于发现程序设计上的一些问题，让线上的程序能够在高并发的情况下还能保持稳定。本小节将带着这一连串的问题来讲解Go语言中如何来实现单元测试和性能测试。 Go语言中自带有一个轻量级的测试框架testing和自带的go test命令来实现单元测试和性能测试，testing框架和其他语言中的测试框架类似，你可以基于这个框架写针对相应函数的测试用例，也可以基于该框架写相应的压力测试用例，那么接下来让我们一一来看一下怎么写。 另外建议安装gotests插件自动生成测试代码: go get -u -v github.com/cweill/gotests/... ","date":"2017-06-01","objectID":"/posts/go-test/:0:0","tags":["go"],"title":"GO test","uri":"/posts/go-test/"},{"categories":null,"content":"如何编写测试用例 由于go test命令只能在一个相应的目录下执行所有文件，所以我们接下来新建一个项目目录gotest,这样我们所有的代码和测试代码都在这个目录下。 接下来我们在该目录下面创建两个文件：gotest.go和gotest_test.go gotest.go:这个文件里面我们是创建了一个包，里面有一个函数实现了除法运算: package gotest import ( \"errors\" ) func Division(a, b float64) (float64, error) { if b == 0 { return 0, errors.New(\"除数不能为0\") } return a / b, nil } gotest_test.go:这是我们的单元测试文件，但是记住下面的这些原则： 文件名必须是_test.go结尾的，这样在执行go test的时候才会执行到相应的代码 你必须import testing这个包 所有的测试用例函数必须是Test开头 测试用例会按照源代码中写的顺序依次执行 测试函数TestXxx()的参数是testing.T，我们可以使用该类型来记录错误或者是测试状态 测试格式：func TestXxx (t *testing.T),Xxx部分可以为任意的字母数字的组合，但是首字母不能是小写字母[a-z]，例如Testintdiv是错误的函数名。 函数中通过调用testing.T的Error, Errorf, FailNow, Fatal, FatalIf方法，说明测试不通过，调用Log方法用来记录测试的信息。 下面是我们的测试用例的代码： package gotest import ( \"testing\" ) func Test_Division_1(t *testing.T) { if i, e := Division(6, 2); i != 3 || e != nil { //try a unit test on function t.Error(\"除法函数测试没通过\") // 如果不是如预期的那么就报错 } else { t.Log(\"第一个测试通过了\") //记录一些你期望记录的信息 } } func Test_Division_2(t *testing.T) { t.Error(\"就是不通过\") } 我们在项目目录下面执行go test,就会显示如下信息： --- FAIL: Test_Division_2 (0.00 seconds) gotest_test.go:16: 就是不通过 FAIL exit status 1 FAIL gotest 0.013s 从这个结果显示测试没有通过，因为在第二个测试函数中我们写死了测试不通过的代码t.Error，那么我们的第一个函数执行的情况怎么样呢？默认情况下执行go test是不会显示测试通过的信息的，我们需要带上参数go test -v，这样就会显示如下信息： === RUN Test_Division_1 --- PASS: Test_Division_1 (0.00 seconds) gotest_test.go:11: 第一个测试通过了 === RUN Test_Division_2 --- FAIL: Test_Division_2 (0.00 seconds) gotest_test.go:16: 就是不通过 FAIL exit status 1 FAIL gotest 0.012s 上面的输出详细的展示了这个测试的过程，我们看到测试函数1Test_Division_1测试通过，而测试函数2Test_Division_2测试失败了，最后得出结论测试不通过。接下来我们把测试函数2修改成如下代码： func Test_Division_2(t *testing.T) { if _, e := Division(6, 0); e == nil { //try a unit test on function t.Error(\"Division did not work as expected.\") // 如果不是如预期的那么就报错 } else { t.Log(\"one test passed.\", e) //记录一些你期望记录的信息 } } 然后我们执行go test -v，就显示如下信息，测试通过了： === RUN Test_Division_1 --- PASS: Test_Division_1 (0.00 seconds) gotest_test.go:11: 第一个测试通过了 === RUN Test_Division_2 --- PASS: Test_Division_2 (0.00 seconds) gotest_test.go:20: one test passed. 除数不能为0 PASS ok gotest 0.013s ","date":"2017-06-01","objectID":"/posts/go-test/:1:0","tags":["go"],"title":"GO test","uri":"/posts/go-test/"},{"categories":null,"content":"如何编写压力测试 压力测试用来检测函数(方法）的性能，和编写单元功能测试的方法类似,此处不再赘述，但需要注意以下几点： 压力测试用例必须遵循如下格式，其中XXX可以是任意字母数字的组合，但是首字母不能是小写字母 func BenchmarkXXX(b *testing.B) { ... } go test不会默认执行压力测试的函数，如果要执行压力测试需要带上参数-test.bench，语法:-test.bench=\"test_name_regex\",例如go test -test.bench=\".*\"表示测试全部的压力测试函数 在压力测试用例中,请记得在循环体内使用testing.B.N,以使测试可以正常的运行 文件名也必须以_test.go结尾 下面我们新建一个压力测试文件webbench_test.go，代码如下所示： package gotest import ( \"testing\" ) func Benchmark_Division(b *testing.B) { for i := 0; i \u003c b.N; i++ { //use b.N for looping Division(4, 5) } } func Benchmark_TimeConsumingFunction(b *testing.B) { b.StopTimer() //调用该函数停止压力测试的时间计数 //做一些初始化的工作,例如读取文件数据,数据库连接之类的, //这样这些时间不影响我们测试函数本身的性能 b.StartTimer() //重新开始时间 for i := 0; i \u003c b.N; i++ { Division(4, 5) } } 我们执行命令go test -file webbench_test.go -test.bench=\".*\"，可以看到如下结果： PASS Benchmark_Division 500000000 7.76 ns/op Benchmark_TimeConsumingFunction 500000000 7.80 ns/op ok gotest 9.364s 上面的结果显示我们没有执行任何TestXXX的单元测试函数，显示的结果只执行了压力测试函数，第一条显示了Benchmark_Division执行了500000000次，每次的执行平均时间是7.76纳秒，第二条显示了Benchmark_TimeConsumingFunction执行了500000000，每次的平均执行时间是7.80纳秒。最后一条显示总共的执行时间。 ","date":"2017-06-01","objectID":"/posts/go-test/:2:0","tags":["go"],"title":"GO test","uri":"/posts/go-test/"},{"categories":null,"content":"小结 通过上面对单元测试和压力测试的学习，我们可以看到testing包很轻量，编写单元测试和压力测试用例非常简单，配合内置的go test命令就可以非常方便的进行测试，这样在我们每次修改完代码,执行一下go test就可以简单的完成回归测试了。 ","date":"2017-06-01","objectID":"/posts/go-test/:3:0","tags":["go"],"title":"GO test","uri":"/posts/go-test/"},{"categories":["基础知识"],"content":"GO 文件操作 在任何计算机设备中，文件是都是必须的对象，而在Web编程中,文件的操作一直是Web程序员经常遇到的问题,文件操作在Web应用中是必须的,非常有用的,我们经常遇到生成文件目录,文件(夹)编辑等操作,现在我们来看看 go 对文件是怎么操作的。 ","date":"2017-05-22","objectID":"/posts/go-file/:0:0","tags":["go"],"title":"Go 文件操作","uri":"/posts/go-file/"},{"categories":["基础知识"],"content":"目录操作 文件操作的大多数函数都是在os包里面，下面列举了几个目录操作的： func Mkdir(name string, perm FileMode) error 创建名称为name的目录，权限设置是perm，例如0777 func MkdirAll(path string, perm FileMode) error 根据path创建多级子目录，例如 test/test1/test2。 func Remove(name string) error 删除名称为name的目录，当目录下有文件或者其他目录时会出错 func RemoveAll(path string) error 根据path删除多级子目录，如果path是单个名称，那么该目录下的子目录全部删除。 以下是简单的使用： package main import ( \"fmt\" \"os\" ) func main() { os.Mkdir(\"test\", 0777) os.MkdirAll(\"test/test1/test2\", 0777) err := os.Remove(\"test\") if err != nil { fmt.Printf(\"crash with error %v \\n\", err) } os.RemoveAll(\"test\") } ","date":"2017-05-22","objectID":"/posts/go-file/:1:0","tags":["go"],"title":"Go 文件操作","uri":"/posts/go-file/"},{"categories":["基础知识"],"content":"文件操作 ","date":"2017-05-22","objectID":"/posts/go-file/:2:0","tags":["go"],"title":"Go 文件操作","uri":"/posts/go-file/"},{"categories":["基础知识"],"content":"建立与打开文件 新建文件可以通过如下两个方法 func Create(name string) (file *File, err Error) 根据提供的文件名创建新的文件，返回一个文件对象，默认权限是0666的文件，返回的文件对象是可读写的。 func NewFile(fd uintptr, name string) *File 根据文件描述符创建相应的文件，返回一个文件对象 通过如下两个方法来打开文件： func Open(name string) (file *File, err Error) 该方法打开一个名称为name的文件，但是是只读方式，内部实现其实调用了OpenFile。 func OpenFile(name string, flag int, perm uint32) (file *File, err Error) 打开名称为name的文件，flag是打开的方式，只读、读写等，perm是权限 ","date":"2017-05-22","objectID":"/posts/go-file/:2:1","tags":["go"],"title":"Go 文件操作","uri":"/posts/go-file/"},{"categories":["基础知识"],"content":"写文件 写文件函数： func (file *File) Write(b []byte) (n int, err Error) 写入byte类型的信息到文件 func (file *File) WriteAt(b []byte, off int64) (n int, err Error) 在指定位置开始写入byte类型的信息 func (file *File) WriteString(s string) (ret int, err Error) 写入string信息到文件 写文件的示例代码 package main import ( \"fmt\" \"os\" ) func main() { userFile := \"yusank.txt\" fout, err := os.Create(userFile) if err != nil { fmt.Println(userFile, err) return } defer fout.Close() for i := 0; i \u003c 10; i++ { fout.WriteString(\"Just a test!\\r\\n\") fout.Write([]byte(\"Just a test!\\r\\n\")) } } ","date":"2017-05-22","objectID":"/posts/go-file/:2:2","tags":["go"],"title":"Go 文件操作","uri":"/posts/go-file/"},{"categories":["基础知识"],"content":"读文件 读文件函数： func (file *File) Read(b []byte) (n int, err Error) 读取数据到b中 func (file *File) ReadAt(b []byte, off int64) (n int, err Error) 从 off 开始读取数据到 b 中 读文件的示例代码: package main import ( \"fmt\" \"os\" ) func main() { userFile := \"yusank.txt\" fl, err := os.Open(userFile) if err != nil { fmt.Println(userFile, err) return } defer fl.Close() buf := make([]byte, 1024) for { n, _ := fl.Read(buf) if 0 == n { break } os.Stdout.Write(buf[:n]) } } ","date":"2017-05-22","objectID":"/posts/go-file/:2:3","tags":["go"],"title":"Go 文件操作","uri":"/posts/go-file/"},{"categories":["基础知识"],"content":"删除文件 Go语言里面删除文件和删除文件夹是同一个函数 func Remove(name string) Error 调用该函数就可以删除文件名为name的文件 ","date":"2017-05-22","objectID":"/posts/go-file/:2:4","tags":["go"],"title":"Go 文件操作","uri":"/posts/go-file/"},{"categories":["基础知识"],"content":"计算文件哈希值 在网络上传输文件完成后，往往都会有一步文件的校验。需要确认传过来的文件是否是损坏的。 小文件 代码： package main import ( \"crypto/md5\" \"fmt\" \"io\" \"os\" ) func main() { testFile := \"/path/to/file\" file, err := os.Open(testFile) if err != nil { fmt.Println(err) return } // 以上是为了获的 os.File 对象 md5h := md5.New() io.Copy(md5h, file) fmt.Printf(\"%x\", md5h.Sum([]byte(\"\"))) // 打印出来的是 MD5 算法下的哈希结果 } 大文件 代码： package main import ( \"crypto/md5\" \"fmt\" \"io\" \"math\" \"os\" ) const filechunk = 8192 // 假定 8KB 以上为大文件 func main() { file, err := os.Open(\"utf8.txt\") if err != nil { panic(err.Error()) } defer file.Close() // 计算大小 info, _ := file.Stat() filesize := info.Size() blocks := uint64(math.Ceil(float64(filesize) / float64(filechunk))) hash := md5.New() for i := uint64(0); i \u003c blocks; i++ { blocksize := int(math.Min(filechunk, float64(filesize-int64(i*filechunk)))) buf := make([]byte, blocksize) file.Read(buf) io.WriteString(hash, string(buf)) // append into the hash } fmt.Printf(\"%s checksum is %x\\n\", file.Name(), hash.Sum(nil)) } 代码内容是打开本地文件分块读取进行哈希计算，在网络传输中，可以每次传入一包的文件，先用 io.WriteString() 方法添加到哈希并在最后进行 hash.Sum() 操作 ","date":"2017-05-22","objectID":"/posts/go-file/:2:5","tags":["go"],"title":"Go 文件操作","uri":"/posts/go-file/"},{"categories":["网络编程"],"content":"Unix 网络编程 ​ 卷II - 进程间通信 IPC是进程间通信（interprocess communication）的简称。传统上该术语描述的是运行在某个操作系统之上的不同进程间各种消息传递（message passing）的方式。 进程间的通信一般是一下四种形式： 消息传递（管道、FIFO和消息队列）； 同步（互斥量、条件变量、读写锁、文件和记录锁、信号量）； 共享内存（匿名的和具名的）； 远程过程调用（Solaris 门和 Sun RPC）。 消息队列 消息传递： 管道和FIFO； Posix 消息队列； System V消息队列。 ","date":"2017-04-22","objectID":"/posts/unix-network/:0:0","tags":["unix"],"title":"Unix 网络编程","uri":"/posts/unix-network/"},{"categories":["网络编程"],"content":"管道和FIFO 管道是最初的Unix IPC 形式。由于管道没有名字，所以它只能用于有亲缘关系的进程间的通信。 实现机制： 管道是由内核管理的一个缓冲区，相当于我们放入内存中的一个纸条。管道的一端连接一个进程的输出。这个进程会向管道中放入信息。管道的另一端连接一个进程的输入，这个进程取出被放入管道的信息。一个缓冲区不需要很大，它被设计成为环形的数据结构，以便管道可以被循环利用。当管道中没有信息的话，从管道中读取的进程会等待，直到另一端的进程放入信息。当管道被放满信息的时候，尝试放入信息的进程会等待，直到另一端的进程取出信息。当两个进程都终结的时候，管道也自动消失。 #include \u003cunistd.h\u003e int pipe (int fd[2]) //返回：若成功返回0，若出错返回-1 该函数返回两个文件描述符：fd[0] 和 fd[1]。前者打开来读，后者打开来写。 管道尽管是单个进程创建，但是管道的典型用途是为两个不同的进程（一个父进程，一个子进程）提供进程间的通信手段。 ​ 数据流 »»» 首先是，由一个进程（它将成为父进程）创建一个 pipe 后调用 fork 派生一个自身的副本，接着关闭着个 pipe 的读成端，子进程关闭同一个 pipe 的写入端。这就是进程间提供了一个单向数据流，如下图。 int main(void) { int n; int fd[2]; pid_t pid; char line[MAXLINE]; if(pipe(fd) === 0){ // 先建立管道得到一对文件描述符 exit(0); } if((pid = fork()) == 0) // 父进程把文件描述符复制给子进程 exit(1); else if(pid \u003e 0){ // 父进程写 close(fd[0]); // 关闭读描述符 write(fd[1], \"\\nhello world\\n\", 14); } else{ // 子进程读 close(fd[1]); // 关闭写端 n = read(fd[0], line, MAXLINE); write(STDOUT_FILENO, line, n); } exit(0); } technically，自从可以在进程间传递描述符后，管道也能用于无亲缘关系的进程间，而现实中管道通常用于具有共同祖先的进程间。 FIFO：命名管道(named PIPE) 管道尽管对很多操作来说是很有用的，但是它的根本局限性在于没有名字，从而只能由亲缘关系的进程（父子进程）使用。为了解决这一问题，Linux提供了FIFO方式连接进程。有了FIFO之后这一缺点得以改正。FIFO有时也称之为有名管道（named pipe）。FIFO除了有管道的功能外，它还允许无亲缘关系的进程的通信。pipe 和 FIFO 都是使用通常的 read 和 write 函数访问的。 FIFO (First in, First out)为一种特殊的文件类型，它在文件系统中有对应的路径。当一个进程以读(r)的方式打开该文件，而另一个进程以写(w)的方式打开该文件，那么内核就会在这两个进程之间建立管道，所以FIFO实际上也由内核管理，不与硬盘打交道。之所以叫FIFO，是因为管道本质上是一个先进先出的队列数据结构，最早放入的数据被最先读出来，从而保证信息交流的顺序。FIFO只是借用了文件系统(file system,命名管道是一种特殊类型的文件，因为Linux中所有事物都是文件，它在文件系统中以文件名的形式存在。)来为管道命名。写模式的进程向FIFO文件中写入，而读模式的进程从FIFO文件中读出。当删除FIFO文件时，管道连接也随之消失。FIFO的好处在于我们可以通过文件的路径来识别管道，从而让没有亲缘关系的进程之间建立连接 #include \u003csys/types.h\u003e #include \u003csys/stat.h\u003e int mkfifo (const char *pathname, mode_t mode); // 返回： 成功返回0，出错返回 -1 其中 pathname 是一个普通的 Unix 路径名，它是该 FIFO 的名字。 mkfifo 函数中参数 mode 指定 FIFO 的读写权限。 mkfifo 函数是要么创建一个新的 FIFO ，要么返回一个 EEXIST 错误（如果该 FIFO 已存在），如果不希望创建一个新的 FIFO 那就用 open 函数就可以。 FIFO 不能打开既写又读。 如果一个 FIFO 只读不写，只写不读都会形成阻塞。 下边是一个简单地例子： #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003csys/types.h\u003e #include \u003csys/stat.h\u003e # define FIFO1 \"/tmp/my_fifo\" int main() { int res = mkfifo(\"/tmp/my_fifo\", 0777); if (res == 0) { printf(\"FIFO created/n\"); } // 打开FIFO //writefd = Open(FIFO1, O_WRONLY | O_NONBLOCK, 0) //readfd = Open(FIFO1, O_RDONLY, 0) exit(EXIT_SUCCESS); } open 第二个参数中的选项O_NONBLOCK，选项O_NONBLOCK表示非阻塞，加上这个选项后，表示open调用是非阻塞的，如果没有这个选项，则表示open调用是阻塞的。 对于以只读方式（O_RDONLY）打开的FIFO文件，如果open调用是阻塞的（即第二个参数为O_RDONLY），除非有一个进程以写方式打开同一个FIFO，否则它不会返回；如果open调用是非阻塞的的（即第二个参数为O_RDONLY|O_NONBLOCK），则即使没有其他进程以写方式打开同一个FIFO文件，open调用将成功并立即返回。 对于以只写方式（O_WRONLY）打开的FIFO文件，如果open调用是阻塞的（即第二个参数为O_WRONLY），open调用将被阻塞，直到有一个进程以只读方式打开同一个FIFO文件为止；如果open调用是非阻塞的（即第二个参数为O_WRONLY|O_NONBLOCK），open总会立即返回，但如果没有其他进程以只读方式打开同一个FIFO文件，open调用将返回-1，并且FIFO也不会被打开。 关于管道或 FIFO 的读写的若干规则： 如果请求读出的数据量多于管道或 FIFO 中当前的可用数据量，那么只会返回这些可用的数据。 如果请求你写入的数据的字节数小于或等于 PIPE_BUF (可原子地写入往一个管道或 FIFO 的最大数据量， Posix 要求至少为512)，那么 write 操作保证是原子的。这意味着，如果两个进程差不多同时往同一个管道或 FIFO 写，那么不管是先写入来自第一个进程的所有数据再写第二个，还是顺序颠倒过来。系统都不会相互混杂来自两个进程的数据。然而如果数据的字节数大于 PIPE_BUF ，那么 write 操作不能保证是原子的。 不止以上这些。。。 小结： FIFO 与管道类似，但是它用 mkfifo 创建，之后需要open 打开。打开管道必须小心，因为许多规则（read 只写管道、write 只读管道、从空的管道或FIFO read 等的情况的返回结果。）制约着 open 的阻塞与否。 ","date":"2017-04-22","objectID":"/posts/unix-network/:1:0","tags":["unix"],"title":"Unix 网络编程","uri":"/posts/unix-network/"},{"categories":["网络编程"],"content":"Posix IPC Posix–可移植性操作系统接口（Protable operating system interface） 有关Unix标准化的大多数活动是由 Posix 和 Open Group 做的。 Posix 不是单一的标准，是一系列的标准。 以下三种类型的IPC合成为“Posix IPC” Posix 消息队列 Posix 信号量 Posix 共享内存区 ","date":"2017-04-22","objectID":"/posts/unix-network/:2:0","tags":["unix"],"title":"Unix 网络编程","uri":"/posts/unix-network/"},{"categories":["网络编程"],"content":"Posix 消息队列 消息队列可认为是个消息链表。有足够写权限的进程可往队列放置信息，有足够读权限的进程可从队列读取信息。每一个信息都是一条记录，它是由发送者赋予一个优先级。在某个进程往一个队列写入消息之前，并不需要另一个进程在该队列上等待消息的到达。这根管道和 FIFO 是相反的。 一个进程可以往某些队列写入一些信息，然后终止，再让另外一个进程在以后的某个时刻读取这些信息。 Posix 消息队列和下面讲的System V 消息队列有许多的相似性。以下是主要的差别： 对 Posix 消息队列的读总是返回最高优先级的最早消息，对 System V 消息队列的读则可以返回任意指定优先级的消息； 当往一个空队列放置一个信息时，Posix 消息队列允许产生一个信号或启动一个线程，System V消息队列则是不提供类似的机制。 队列中的每一个消息都有如下属性： 一个无符号整数优先级（Posix）或 一个长整数类型（system V）； 消息的数据部分长度（可以为0）； 数据本身（如果长度大于0）。 一个消息队列的可能布局。 我们所设想的是一个链表，该链表的有中含有当前队列的两个属性：队列中允许的最大开销数以及每一个消息的最大大小。 **mq_open ,mq_close 和 mq_unlink 函数 **： mq_open 函数创建一个新的消息队列或打开一个已存在的消息队列。 # include \u003cmqueue.h\u003e mqd_t mq_open (const char *name, int oflag, ... /* mode_t mode, struct mq_attr *attr */); //返回： 成功返回消息对列描述符，出错返回-1 其中 name 有自己的一套命名规则，因为 Posix IPC 使用“Posix IPC 名字”进行标识。为方便于移植起见，Posix IPC 名字必须以斜杠符开头并且不能再包含任何斜杠符。 oflag 是O_RDONLY、O_WRONLY 或 O_RDWR 之一， 可能按位或上O_CREATE(若不存在则创建)、O_EXCL(与O_CREATE一起，若已存在返回EEXIST 错误)或 O_NONBLOCK（非阻塞标识符）。 当实际操作创建一个新的消息队列时（指定O_CREATE标志，且请求的队列不存在），mode 和 attr 参数是需要的。mode上面介绍过。attr参数用于给新队列指定某些属性。 mq_open 返回值称为消息队列描述符（message queue descriptor），这个值用作其他消息队列函数的第一参数。 已打开的消息队列是由 mq_close 关闭的。 #include \u003cmqueue.h\u003e int mq_close(mqd_t mqdes) //返回： 成功返回0，出错返回-1 关闭之后调用进程不再使用该描述符，但其消息队列并不从系统中删除。一个进程终止时，它打开着的消息队列都关闭，就像调用mq_close 一样。 要从系统中删除消息队列则用mq_unlink 函数，其第一参数为 mq_open 的第一参数 name。 # include \u003cmqueue.h\u003e int mq_unlink(const char *name) //返回： 成功返回0，出错返回-1 mq_getattr 和 mq_setattr 函数 消息队列有四个属性，这两个函数是获取和修改这些属性。 mq_flags //队列阻塞标志位 mq_maxmsg //队列最大允许消息数 mq_msgsize //队列消息最大字节数 mq_curmsgs //队列当前消息条数 #include \u003cmqueue.h\u003e int mq_getattr(mqd_t mqdes,struct mq_attr *attr); int mq_setattr(mqd_t mqdes,const struct mq_attr *attr, struct mq_attr *oattr); //返回：均成功返回0，出错返回-1 mq_send 和 mq_receive 函数 ​ 这两个函数分别往一个队列放置一个信息和从一个队列取走一个消息。每一个消息都有优先级，它是一个小于MQ_PRIO_MAX 的无符号整数。Posix要求这个上限至少为32. ​ mq_receive 总是返回所指定队列中优先级最高的的最早消息，而且该优先级能随该消息的内容及其长度一同返回。 #include \u003cmqueue.h\u003e int mq_send(mqd_t mqdes, const char *ptr, size_t len, unsigned int prio); //返回： 成功返回0，出错返回-1 ssize_t mq_reccevie(mqd_t mqdes, char *ptr, size_t len, unsigned int *priop); //返回： 成功返回消息中的字节数，出错返回-1 mq_receive 的 len 参数的值不能小于能加到所指定队列中的最大大小（该队列 mq_attr 结构的 mq_msgsize ）。要是 len 小于该值， mq_receive立即返回 EMSGSIZE 错误。 mq_send 的 prio 参数是待发信息的优先级，其值必须小于 MQ_PRIO_MAX 。如果 mq_receive 的 priop 参数是一个非空指针，所返回消息的优先级就通过该指针存放。如果应用不必使用优先级不同的消息，那就给mq_send 指针值为0的优先级，给 mq_receive 指定一个空指针作为其最后一个参数。 往某个队列中增加一个消息 #include \u003cmqueue.h\u003e int main(int argc, char **argv) { mqd_t mqd; //描述符 void *ptr; //指向缓冲区的指针 size_t len; //长度 uint_t prio; //优先度 if (argc != 4) err_quit(\"usage: mqsend \u003cname\u003e \u003c#bytes\u003e \u003cpriority\u003e\"); len = atoi(argv[2]); prio = atoi(argv[3]); mqd = Mq_open(argv[1], O_WRONLY); // 创建一个消息队列 ptr = Calloc(len, sizeof(char));// 所用的缓冲区用colloc分配，该函数会把该缓冲区初始化为0 Mq_send(mqd, ptr, len, prio); exit(0); } 待发消息的大小和优先级必须作为命令行参数指定。 从某队列读出下一个信息 #include \"unpipc.h\" int main(int argc, char **argv) { int c,flags; mqd_t maq; ssize_t n; uint_t prio; void *buff; struct mq_attr attr; flags = O_RDONLY; while ( (c = Getopt(argc, argv, \"n\")) != -1) { switch (c) { case 'n': flags |= O_NONBLOCK; break; } } if (optind != argc - 1) err_quit(\"usage: mqreceive [-n] \u003cname\u003e\"); mqd =Mq_open(argv[optind], flags); Mq_getattr(mqd, \u0026attr); buff = Malloc(attr.mq_msgsize); n = Mq_receive(mqd, buff, attr.mq_msgsize, \u0026prio); printf(\"read %ld bytes, priority = %u\\n\", (long) n, prio); exit(0); } 命令行选项 -n 指定非阻塞属性，这样如果所指定的队列中没有消息， 则返回一个错误。 调用 mq_getattr 打开队列并取得属性。需要确定最大消息大小，因为必须为调用的 mq_receive 分配一个这样大小的缓冲区。最后输出所读出消息的大小及其属性。 solaris %mqcreate /test1 创建并获取属性 solaris %mqgetattr /test1 max solaris % mqsend /test1 100 9999 以无效的优先级发送 mq_send error: Invalid argument solaris % mqsend /test1 100 6 100字节，优先级6 solaris % mqsend /test1 50 18 50字节，优先级18 solaris % mqsend /test1 33 18 33字节，优先级18 solaris % mqreceive /test1 read 50 bytes, priority = 18 返回优先级最高的最早消息 solaris % mqreceive /test1 read 33 bytes, priority = 18 solaris % mqreceive /test1 read 100 bytes, priority = 6 solaris % mqreceive /test1 指定非阻塞属性，队列为空 mq_recevie error: Resource temporarily unavalibale 消息队列限制","date":"2017-04-22","objectID":"/posts/unix-network/:3:0","tags":["unix"],"title":"Unix 网络编程","uri":"/posts/unix-network/"},{"categories":["网络编程"],"content":"System V 消息队列 以下三种类型的IPC称为 System V IPC： System V 消息队列； System V 信号量； System V 共享内存区。 这个称为作为这三个IPC机制的通称是因为它们源自 System V Unix 。这三种IPC最先出现在AT\u0026T System v UNIX上面，并遵循XSI标准，有时候也被称为XSI IPC。 System V 消息队列使用消息队列标识符（message queue identifier） 标识。有足够权限的任何进程可往队列放置信息，有足够权限的任何进程可从队列读取信息。跟 Posix 一样，在某个进程往一个队列写入消息之前，不求另外某个进程正在等待该队列上一个消息的到达。 对于系统的每个消息队列，内核维护一个定义在 \u003csys/msg.h\u003e 头文件中的信息结构. struct msqid_ds { struct ipc_perm msg_perm //operation permission structure struct msg *msg_frist //ptr to frist message on queue struct msg *msg_last //ptr to last message on queue msglen_t msg_cbytes //current #bytes on queue msgqnum_t msg_qnum //number of messages currently on queue msglen_t msg_qbytes //maximum number of bytes allowed on queue pid_t msg_lspid //process ID of last msgsnd() pid_t msg_lrpid //process ID of last msgrcv() time_t msg_stime //time of last msgsnd() time_t msg_rtime //time of last msgrcv() time_t msg_ctime //time of last change } Unix 98 不要求有 msg_frist、msg_last 和 msg_cbytes 成员。然而普通的源自 System V 的实现中可以找到这三个成员。就算提供了这两个指针，那么它们指向的是内核内存空间，对于应用来说基本没有作用的。 我们可以将内核中某个特定的消息队列画为一个消息链表，如图。 msgget 函数 msgget 函数用于创建一个新的消息队列或访问一个已存在的消息队列。 #include \u003csys/msg.h\u003e int msgget (key_t key, int oflag) //返回： 成功返回非负标识符，出错返回-1 返回值是一个整数标识符，其他三个msg函数就用它来指代该队列。 oflag是读写权限的组合。（稍微复杂。。。） 当创建一个新的消息队列的时，msqid_ds 结构的如下成员被初始化。 msg_perm 结构的 uid 和 cuid 成员被设置成当前进程的有效用户ID，gid 和 cgid 成员被设置成当前的进程的有效组ID。 oflag 中的读写权限位存放在msg_perm.mode 中。 msg_qnum、msg_lspid，msg_lrpid、msg_stime 和 msg_rtime 被设置为0. msg_ctime 被设置为当前时间。 msg_qbytes 被设置成系统限制值。 struct ipc_perm { key_t key; /*调用shmget()时给出的关键字*/ uid_t uid; /*共享内存所有者的有效用户ID */ gid_t gid; /* 共享内存所有者所属组的有效组ID*/ uid_t cuid; /* 共享内存创建 者的有效用户ID*/ gid_t cgid; /* 共享内存创建者所属组的有效组ID*/ mode_t mode; /* Permissions + SHM_DEST和SHM_LOCKED标志*/ ulong_t seq; /* 序列号*/ }; ​ msgsnd 函数 使用 msgget 函数打开一个消息队列后，使用 msgsnd 函数往其上放置一个消息。 # include \u003csys/msg.h\u003e int msgsnd(int msqid, const void *ptr,size_t length, int flag); 其中msqid 是由msgget 函数返回的标识符。ptr 是一个结构指针，该结构具有如下的模板： struct msgbuf { long mtype; // message type ,must be \u003e 0 char mtext[1] // message data }; 消息类型必须大于0，因为对于 msgrcv 函数来说，非正的消息类型用作特殊的指示器。 mtext虽然起名是 text ，但是消息类型并不局限于文本。任何形式的数据都是允许的。内核根本不解释消 息数据的内容。ptr 所指向的是一个含有消息类型的长整数，消息本身则紧跟着它之后。 msgsnd 的 length 参数以字节为单位指定待发送消息的长度。是用户自定义的，可以是0. flag 参数既可以是0，也可以是IPC_NOWAIT 。IPC_NOWAIT 标志使得 msgsnd 调用非阻塞：如果没有存放新消息的可用空间，该函数马上返回。这个条件可能发生的情况包括： 在指定的队列中已有太多的字节（对应 该队列的msqid_ds 结构中的msg_qbytes 值）； 在系统范围存在太多的消息。 如果两个条件一个存在，而且IPC_NOWAIT标志已指定，msgsnd 就返回一个EAGAIN 错误。如果两个条件一个存在，标志未指定，那么调用线程就被投入睡眠，直到： 具备存放新消息的空间； 由 msqgid 标识的消息队列从系统中删除（这个情况下回返回一个EIDRM 错误）； 调用线程被某个捕获的信息所中断。 ","date":"2017-04-22","objectID":"/posts/unix-network/:4:0","tags":["unix"],"title":"Unix 网络编程","uri":"/posts/unix-network/"},{"categories":["技术"],"content":"跨域资源共享 CORS 详解 CORS是一个W3C标准，全称是\"跨域资源共享\"（Cross-origin resource sharing）。 它允许浏览器向跨源服务器，发出XMLHttpRequest请求，从而克服了AJAX只能同源使用的限制。 本文详细介绍CORS的内部机制。 ","date":"2017-01-30","objectID":"/posts/cors/:0:0","tags":["go","cors"],"title":"跨域资源共享 CORS 详解","uri":"/posts/cors/"},{"categories":["技术"],"content":"一、简介 CORS需要浏览器和服务器同时支持。目前，所有浏览器都支持该功能，IE浏览器不能低于IE10。 整个CORS通信过程，都是浏览器自动完成，不需要用户参与。对于开发者来说，CORS通信与同源的AJAX通信没有差别，代码完全一样。浏览器一旦发现AJAX请求跨源，就会自动添加一些附加的头信息，有时还会多出一次附加的请求，但用户不会有感觉。 因此，实现CORS通信的关键是服务器。只要服务器实现了CORS接口，就可以跨源通信。 ","date":"2017-01-30","objectID":"/posts/cors/:1:0","tags":["go","cors"],"title":"跨域资源共享 CORS 详解","uri":"/posts/cors/"},{"categories":["技术"],"content":"二、两种请求 浏览器将CORS请求分成两类：简单请求（simple request）和非简单请求（not-so-simple request）。 只要同时满足以下两大条件，就属于简单请求。 请求方法是以下三种方法之一： HEAD GET POST （2）HTTP的头信息不超出以下几种字段： Accept Accept-Language Content-Language Last-Event-ID Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain 凡是不同时满足上面两个条件，就属于非简单请求。 浏览器对这两种请求的处理，是不一样的。 ","date":"2017-01-30","objectID":"/posts/cors/:2:0","tags":["go","cors"],"title":"跨域资源共享 CORS 详解","uri":"/posts/cors/"},{"categories":["技术"],"content":"三、简单请求 ","date":"2017-01-30","objectID":"/posts/cors/:3:0","tags":["go","cors"],"title":"跨域资源共享 CORS 详解","uri":"/posts/cors/"},{"categories":["技术"],"content":"3.1 基本流程 对于简单请求，浏览器直接发出CORS请求。具体来说，就是在头信息之中，增加一个Origin字段。 下面是一个例子，浏览器发现这次跨源AJAX请求是简单请求，就自动在头信息之中，添加一个Origin字段。 GET /cors HTTP/1.1 Origin: http://api.bob.com Host: api.alice.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 上面的头信息中，Origin字段用来说明，本次请求来自哪个源（协议 + 域名 + 端口）。服务器根据这个值，决定是否同意这次请求。 如果Origin指定的源，不在许可范围内，服务器会返回一个正常的HTTP回应。浏览器发现，这个回应的头信息没有包含Access-Control-Allow-Origin字段（详见下文），就知道出错了，从而抛出一个错误，被XMLHttpRequest的onerror回调函数捕获。注意，这种错误无法通过状态码识别，因为HTTP回应的状态码有可能是200。 如果Origin指定的域名在许可范围内，服务器返回的响应，会多出几个头信息字段。 Access-Control-Allow-Origin: http://api.bob.com Access-Control-Allow-Credentials: true Access-Control-Expose-Headers: FooBar Content-Type: text/html; charset=utf-8 上面的头信息之中，有三个与CORS请求相关的字段，都以Access-Control-开头。 （1）Access-Control-Allow-Origin 该字段是必须的。它的值要么是请求时Origin字段的值，要么是一个*，表示接受任意域名的请求。 （2）Access-Control-Allow-Credentials 该字段可选。它的值是一个布尔值，表示是否允许发送Cookie。默认情况下，Cookie不包括在CORS请求之中。设为true，即表示服务器明确许可，Cookie可以包含在请求中，一起发给服务器。这个值也只能设为true，如果服务器不要浏览器发送Cookie，删除该字段即可。 （3）Access-Control-Expose-Headers 该字段可选。CORS请求时，XMLHttpRequest对象的getResponseHeader()方法只能拿到6个基本字段：Cache-Control、Content-Language、Content-Type、Expires、Last-Modified、Pragma。如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定。上面的例子指定，getResponseHeader('FooBar')可以返回FooBar字段的值。 ","date":"2017-01-30","objectID":"/posts/cors/:3:1","tags":["go","cors"],"title":"跨域资源共享 CORS 详解","uri":"/posts/cors/"},{"categories":["技术"],"content":"3.2 withCredentials 属性 上面说到，CORS请求默认不发送Cookie和HTTP认证信息。如果要把Cookie发到服务器，一方面要服务器同意，指定Access-Control-Allow-Credentials字段。 Access-Control-Allow-Credentials: true 另一方面，开发者必须在AJAX请求中打开withCredentials属性。 var xhr = new XMLHttpRequest(); xhr.withCredentials = true; 否则，即使服务器同意发送Cookie，浏览器也不会发送。或者，服务器要求设置Cookie，浏览器也不会处理。 但是，如果省略withCredentials设置，有的浏览器还是会一起发送Cookie。这时，可以显式关闭withCredentials。 xhr.withCredentials = false; 需要注意的是，如果要发送Cookie，Access-Control-Allow-Origin就不能设为星号，必须指定明确的、与请求网页一致的域名。同时，Cookie依然遵循同源政策，只有用服务器域名设置的Cookie才会上传，其他域名的Cookie并不会上传，且（跨源）原网页代码中的document.cookie也无法读取服务器域名下的Cookie。 ","date":"2017-01-30","objectID":"/posts/cors/:3:2","tags":["go","cors"],"title":"跨域资源共享 CORS 详解","uri":"/posts/cors/"},{"categories":["技术"],"content":"四、非简单请求 ","date":"2017-01-30","objectID":"/posts/cors/:4:0","tags":["go","cors"],"title":"跨域资源共享 CORS 详解","uri":"/posts/cors/"},{"categories":["技术"],"content":"4.1 预检请求 非简单请求是那种对服务器有特殊要求的请求，比如请求方法是PUT或DELETE，或者Content-Type字段的类型是application/json。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为\"预检\"请求（preflight）。 浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 下面是一段浏览器的JavaScript脚本。 var url = 'http://api.alice.com/cors'; var xhr = new XMLHttpRequest(); xhr.open('PUT', url, true); xhr.setRequestHeader('X-Custom-Header', 'value'); xhr.send(); 上面代码中，HTTP请求的方法是PUT，并且发送一个自定义头信息X-Custom-Header。 浏览器发现，这是一个非简单请求，就自动发出一个\"预检\"请求，要求服务器确认可以这样请求。下面是这个\"预检\"请求的HTTP头信息。 OPTIONS /cors HTTP/1.1 Origin: http://api.bob.com Access-Control-Request-Method: PUT Access-Control-Request-Headers: X-Custom-Header Host: api.alice.com Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... “预检\"请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。 除了Origin字段，“预检\"请求的头信息包括两个特殊字段。 （1）Access-Control-Request-Method 该字段是必须的，用来列出浏览器的CORS请求会用到哪些HTTP方法，上例是PUT。 （2）Access-Control-Request-Headers 该字段是一个逗号分隔的字符串，指定浏览器CORS请求会额外发送的头信息字段，上例是X-Custom-Header。 ","date":"2017-01-30","objectID":"/posts/cors/:4:1","tags":["go","cors"],"title":"跨域资源共享 CORS 详解","uri":"/posts/cors/"},{"categories":["技术"],"content":"4.2 预检请求的回应 服务器收到\"预检\"请求以后，检查了Origin、Access-Control-Request-Method和Access-Control-Request-Headers字段以后，确认允许跨源请求，就可以做出回应。 HTTP/1.1 200 OK Date: Mon, 01 Dec 2008 01:15:39 GMT Server: Apache/2.0.61 (Unix) Access-Control-Allow-Origin: http://api.bob.com Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Content-Type: text/html; charset=utf-8 Content-Encoding: gzip Content-Length: 0 Keep-Alive: timeout=2, max=100 Connection: Keep-Alive Content-Type: text/plain 上面的HTTP回应中，关键的是Access-Control-Allow-Origin字段，表示http://api.bob.com可以请求数据。该字段也可以设为星号，表示同意任意跨源请求。 Access-Control-Allow-Origin: * 如果浏览器否定了\"预检\"请求，会返回一个正常的HTTP回应，但是没有任何CORS相关的头信息字段。这时，浏览器就会认定，服务器不同意预检请求，因此触发一个错误，被XMLHttpRequest对象的onerror回调函数捕获。控制台会打印出如下的报错信息。 XMLHttpRequest cannot load http://api.alice.com. Origin http://api.bob.com is not allowed by Access-Control-Allow-Origin. 服务器回应的其他CORS相关字段如下。 Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Access-Control-Allow-Credentials: true Access-Control-Max-Age: 1728000 （1）Access-Control-Allow-Methods 该字段必需，它的值是逗号分隔的一个字符串，表明服务器支持的所有跨域请求的方法。注意，返回的是所有支持的方法，而不单是浏览器请求的那个方法。这是为了避免多次\"预检\"请求。 （2）Access-Control-Allow-Headers 如果浏览器请求包括Access-Control-Request-Headers字段，则Access-Control-Allow-Headers字段是必需的。它也是一个逗号分隔的字符串，表明服务器支持的所有头信息字段，不限于浏览器在\"预检\"中请求的字段。 （3）Access-Control-Allow-Credentials 该字段与简单请求时的含义相同。 （4）Access-Control-Max-Age 该字段可选，用来指定本次预检请求的有效期，单位为秒。上面结果中，有效期是20天（1728000秒），即允许缓存该条回应1728000秒（即20天），在此期间，不用发出另一条预检请求。 ","date":"2017-01-30","objectID":"/posts/cors/:4:2","tags":["go","cors"],"title":"跨域资源共享 CORS 详解","uri":"/posts/cors/"},{"categories":["技术"],"content":"4.3 浏览器的正常请求和回应 一旦服务器通过了\"预检\"请求，以后每次浏览器正常的CORS请求，就都跟简单请求一样，会有一个Origin头信息字段。服务器的回应，也都会有一个Access-Control-Allow-Origin头信息字段。 下面是\"预检\"请求之后，浏览器的正常CORS请求。 PUT /cors HTTP/1.1 Origin: http://api.bob.com Host: api.alice.com X-Custom-Header: value Accept-Language: en-US Connection: keep-alive User-Agent: Mozilla/5.0... 上面头信息的Origin字段是浏览器自动添加的。 下面是服务器正常的回应。 Access-Control-Allow-Origin: http://api.bob.com Content-Type: text/html; charset=utf-8 上面头信息中，Access-Control-Allow-Origin字段是每次回应都必定包含的。 ","date":"2017-01-30","objectID":"/posts/cors/:4:3","tags":["go","cors"],"title":"跨域资源共享 CORS 详解","uri":"/posts/cors/"},{"categories":["技术"],"content":"五、与JSONP的比较 CORS与JSONP的使用目的相同，但是比JSONP更强大。 JSONP只支持GET请求，CORS支持所有类型的HTTP请求。JSONP的优势在于支持老式浏览器，以及可以向不支持CORS的网站请求数据。 （完） ","date":"2017-01-30","objectID":"/posts/cors/:5:0","tags":["go","cors"],"title":"跨域资源共享 CORS 详解","uri":"/posts/cors/"},{"categories":["基础知识"],"content":"welcome to learn terminal command!!! linux命令 ","date":"2016-12-28","objectID":"/posts/linux-cmd/:0:0","tags":["linux"],"title":"linux命令","uri":"/posts/linux-cmd/"},{"categories":["基础知识"],"content":"永！远！不！要！执！行！你！不！清！楚！在！干！啥！的！命！令！ ","date":"2016-12-28","objectID":"/posts/linux-cmd/:0:1","tags":["linux"],"title":"linux命令","uri":"/posts/linux-cmd/"},{"categories":["基础知识"],"content":"实用性 $ ls -l | sed '1d' | sort -n -k5 | awk '{printf \"%15s %10s\\n\", $9,$5}' 按文件大小增序打印出当前目录下的文件名及其文件大小(单位字节） $ history | awk '{print $2}' | sort | uniq -c | sort -rn | head -10 输出你最常用的十条命令 $ http POST http://localhost:4000/ \u003c /\u003cjson文件路径\u003e 做测试的时候很有用的一个命令，需要下载http $ brew install http $ lsof -n -P -i TCP -s TCP:LISTEN COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME QQ 290 smartestee 33u IPv4 0x2f3beaa58a62d73b 0t0 TCP 127.0.0.1:4300 (LISTEN) QQ 290 smartestee 34u IPv4 0x2f3beaa58c69673b 0t0 TCP 127.0.0.1:4301 (LISTEN) idea 3257 smartestee 164u IPv4 0x2f3beaa588d11e43 0t0 TCP 127.0.0.1:6942 (LISTEN) idea 3257 smartestee 385u IPv4 0x2f3beaa58c69316b 0t0 TCP 127.0.0.1:63342 (LISTEN) 查看端口的使用情况 $ ps -ef 查看进程 $ kill xxxx 端口冲突时，用此命令，关闭某个端口。用PID替换xxxx $ history 查看历史命令记录 $ pwd 当前位置 $ which xx path位置，搭建环境的时候肯定会用得到 ","date":"2016-12-28","objectID":"/posts/linux-cmd/:1:0","tags":["linux"],"title":"linux命令","uri":"/posts/linux-cmd/"},{"categories":["基础知识"],"content":"Linux 文件系统命令 修改问价拥有者 $ chgrp -R 组名 文件 / 目录 $ chown -R 账户名 文件 / 目录 修改文件权限 $ chmod 使用数字 r：4, w：2, x：1 每种身份的权限的累加的。 $ chmod 777 test 使用符号修改 u: user, g: group, o: others, a: all 添加权限用+， 除去用-， 设置用= $ chmod u=rwx, g=rw, o=r test $ chmod a-x test $ chmod go+r test ​ $ sudo !! 以root权限执行上一条命令（注意上一条命令的内容，以免发生意外） 例如：在Ubuntu 安装软件或插件的时候需要用到这个命令 $ sudo apt-get install nginx 查看和修改： $ cat $ more $ less $ head $ tail $ vi $ vim $ mkdir $ touch ","date":"2016-12-28","objectID":"/posts/linux-cmd/:1:1","tags":["linux"],"title":"linux命令","uri":"/posts/linux-cmd/"},{"categories":["基础知识"],"content":"git $ git 先给出比较常用的 $ git add \u003c一个或多个文件名(文件名之间是用空格，也可以是一个点，表示添加全部)\u003e $ git commit -m \"注释\" 本地提交 $ git checkout \u003c分支名或master\u003e 切换分支与master $ git branch \u003c分支名\u003e 新开一个分支 $ git merge \u003c分支名\u003e 主分支与分支的合并 $ git push origin master 提交到github上 $ fuck 纠正命令行输入的错误，比手动改快，实用。 安装： $ brew install thefuck ","date":"2016-12-28","objectID":"/posts/linux-cmd/:1:2","tags":["linux"],"title":"linux命令","uri":"/posts/linux-cmd/"},{"categories":["基础知识"],"content":"娱乐 $ cmatrix $ telnet towel.blinkenlights.nl telnet是基于Telnet协议的远程登录客户端程序,经常用来远程登录服务器.除此还可以用它来观看星球大战 $ fortune 随机输出名言或者笑话， 还有很多，有兴趣的可以通过这个链接去看：知乎 个人博客 yusank 比较牛逼的一个查找命令的网站：http://www.commandlinefu.com/commands/browse/sort-by-votes 每天都有更新各种命令组合 ","date":"2016-12-28","objectID":"/posts/linux-cmd/:2:0","tags":["linux"],"title":"linux命令","uri":"/posts/linux-cmd/"}]