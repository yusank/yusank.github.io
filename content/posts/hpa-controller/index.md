---
title: "HPA controller æºç è§£è¯»"
date: 2022-01-20T10:50:00+08:00
lastmod: 2022-01-20T11:50:00+08:00
categories: ["kubernetes"]
tags: ["go", "k8s", "æºç è§£è¯»"]
draft: false
---

> æœ¬ç¯‡è®²è¿° `kubernetes` çš„æ¨ªå‘ pod ä¼¸ç¼©(HorizontalPodAutoscaler) æ§åˆ¶å™¨çš„æ•°æ®ç»“æ„ï¼Œé€»è¾‘å¤„ç†ï¼Œmetrics è®¡ç®—ä»¥åŠç›¸å…³ç»†èŠ‚çš„æºç è§£è¯»ã€‚

<!--more-->

## 1. å‰è¨€

åœ¨ `k8s` ç¯å¢ƒå†…å¼¹æ€§ä¼¸ç¼©å¹¶ä¸æ˜¯ä¸€ä¸ªé™Œç”Ÿçš„æ¦‚å¿µï¼Œæ˜¯ä¸€ä¸ªå¸¸è§ä¸”ä¸éš¾ç†è§£çš„äº‹ä»¶ã€‚å°±æ˜¯æ ¹æ®ç‰¹å®šçš„äº‹ä»¶æˆ–æ•°æ®æ¥è§¦å‘å¯ä¼¸ç¼©èµ„æºçš„ä¼¸ç¼©èƒ½åŠ›ã€‚ä¸€èˆ¬æœ‰ HPA å’Œ VPA ä¸¤ä¸ªæ¦‚å¿µï¼ŒHPA å…¨ç§° [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) å³æ¨ªå‘ pod çš„è‡ªåŠ¨ä¼¸ç¼©ï¼ŒVPA å…¨ç§° [Vertical Pod Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler) å³çºµå‘ pod çš„è‡ªåŠ¨ä¼¸ç¼©ã€‚

### 1.1. HPA

`HPA` å…³æ³¨ pod çš„æ•°é‡ï¼Œæ ¹æ®ç°æœ‰ pod çš„æ•°æ®(cpu, memory)æˆ–å¤–éƒ¨æ•°æ®(metrics)æ¥è®¡ç®—å®é™…éœ€è¦çš„ pod æ•°é‡ï¼Œä»è€Œè°ƒæ•´ pod çš„æ€»æ•°ã€‚`HPA` æ“ä½œçš„å¯¹è±¡éœ€è¦å®ç° `ScaleInterface` ã€‚

{{< admonition type=quote title="ScaleInterface" open=true >}}

```go
// ScaleInterface can fetch and update scales for
// resources in a particular namespace which implement
// the scale subresource.
type ScaleInterface interface {
    // Get fetches the scale of the given scalable resource.
    Get(ctx context.Context, resource schema.GroupResource, name string, opts metav1.GetOptions) (*autoscalingapi.Scale, error)

    // Update updates the scale of the given scalable resource.
    Update(ctx context.Context, resource schema.GroupResource, scale *autoscalingapi.Scale, opts metav1.UpdateOptions) (*autoscalingapi.Scale, error)

    // Patch patches the scale of the given scalable resource.
    Patch(ctx context.Context, gvr schema.GroupVersionResource, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions) (*autoscalingapi.Scale, error)
}
```

{{< /admonition >}}

k8s åŸç”Ÿèµ„æºä¸­ HPA å¯æ“ä½œæ€§çš„å¯¹è±¡æ˜¯ä»¥ä¸‹ä¸‰ä¸ª:

- `Deployment`
- `ReplicaSet`
- `StatefulSet`

ä¸€èˆ¬ä¸šåŠ¡åœºæ™¯ç”¨ HPA èƒ½æ»¡è¶³éœ€æ±‚ï¼Œå³ä¸šåŠ¡é«˜å³°å¢åŠ  pod æ•°æ›´å¥½å¤„ç†ä¸šåŠ¡ï¼Œä¸šåŠ¡ä½å³°é™ä½pod æ•°èŠ‚çœèµ„æºã€‚

### 1.2. VPA

`VPA` å…³æ³¨çš„æ˜¯ pod çš„èµ„æºï¼Œæ ¹æ®å½“å‰èµ„æºåˆ©ç”¨ç‡ç­‰æ•°æ®ä¸º pod æä¾›æ›´å¤šçš„èµ„æºï¼ˆcpuï¼Œmemory ç­‰ï¼‰ã€‚æœ¬äººå¯¹ VPA ä¹Ÿæ˜¯è¡¨é¢ç†è§£ï¼Œæ‰€ä»¥è¿™é‡Œä¸åšè¯¦ç»†çš„è§£è¯»ã€‚

`VPA` æ›´é€‚åˆå¤§è®¡ç®—ã€ç¦»çº¿è®¡ç®—ã€æœºå™¨å­¦ä¹ ç­‰åœºæ™¯ï¼Œéœ€è¦å¤§é‡çš„ CPUï¼ŒGPUï¼Œå†…å­˜æ¥è¿›è¡Œè®¡ç®—ã€‚

## 2. åŸºç¡€ç”¨æ³•

é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¼€å¯å¯¹æŸä¸ªèµ„æºçš„ HPA èƒ½åŠ›ï¼š

```shell
âœ kubectl autoscale (-f FILENAME | TYPE NAME | TYPE/NAME) [--min=MINPODS] --max=MAXPODS [--cpu-percent=CPU] [options]
```

{{< admonition type=warning title="Warning" open=true >}}
éœ€è¦å¼€å¯ `metric server` æ‰èƒ½è¯»å–åˆ°cpu åˆ©ç”¨ç‡ï¼Œé»˜è®¤æ˜¯ä¸å¼€å¯çš„ã€‚è¯¦æƒ…è¯·çœ‹å®˜æ–¹æ–‡æ¡£: https://github.com/kubernetes-sigs/metrics-server
{{< /admonition >}}

å®é™…ä½¿ç”¨å¦‚ä¸‹ï¼š

```shell
# å¯¹ deployment/klyn-deploy è¿›è¡Œ CPU ç›‘æ§ï¼Œè¶…è¿‡ 10%çš„å¹³å‡åˆ©ç”¨ç‡å³è¿›è¡Œscale upï¼Œæœ€å¤§ 3 ä¸ª pod æœ€å° 1 ä¸ª
âœ kubectl autoscale deployment klyn-deploy --cpu-percent=10 --min=1 --max=3
```

ç„¶åå¯ä»¥é€šè¿‡ `describe` å‘½ä»¤æŸ¥çœ‹åˆ›å»ºå HPA çš„è¯¦æƒ…ï¼š

```shell
âœ kubectl describe hpa klyn-deploy
Warning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+
Name:                                                  klyn-deploy
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Thu, 20 Jan 2022 11:43:43 +0800
Reference:                                             Deployment/klyn-deploy
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  4% (10m) / 10% # å¯ä»¥çœ‹åˆ°å·²ç»è¯»åˆ° cpu æ•°æ®
Min replicas:                                          1
Max replicas:                                          3
Deployment pods:                                       1 current / 1 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:           <none>
```

å¯ä»¥ä»ä¸Šè¿°è¯¦æƒ…çœ‹åˆ° HPA èµ„æºçš„è¯¦ç»†ä¿¡æ¯ä»¥åŠæœ€ä¸‹é¢çš„æ­¥éª¤ä¿¡æ¯ï¼Œå½“è¿›è¡Œä¼¸ç¼©æ—¶ä¼šåŒæ­¥ä¼¸ç¼©è¿‡ç¨‹å’ŒåŸå› åˆ° `Events` å­—æ®µä¸Šã€‚

ä¹‹åå¯ä»¥é€šè¿‡å‹æµ‹çš„æ–¹å¼å°† cpu çš„åˆ©ç”¨ç‡æå‡ç„¶åå¯ä»¥åˆ° `Deployment` çš„ replicas æ•°é‡çš„æå‡ï¼Œå¹¶å‹æµ‹ç»“æŸä¸€æ®µæ—¶é—´(ä¼šæœ‰å†·å´æ—¶é—´)ååˆé™åˆ° 1 ä¸ª replicas.

## 3. æ•°æ®ç»“æ„

HPA èµ„æº YAML ç»“æ„ï¼š

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: "2022-01-20T03:43:43Z"
  name: klyn-deploy
  namespace: default
  resourceVersion: "6602029"
  uid: 385f4234-025b-453d-8270-788f7ce3ced6
spec:
  maxReplicas: 3
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 10
        type: Utilization
    type: Resource
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: klyn-deploy
status:
  conditions:
  - lastTransitionTime: "2022-01-20T03:43:58Z"
    message: recommended size matches current size
    reason: ReadyForNewScale
    status: "True"
    type: AbleToScale
  - lastTransitionTime: "2022-01-20T03:43:58Z"
    message: the HPA was able to successfully calculate a replica count from cpu resource
      utilization (percentage of request)
    reason: ValidMetricFound
    status: "True"
    type: ScalingActive
  - lastTransitionTime: "2022-01-20T03:43:58Z"
    message: the desired count is within the acceptable range
    reason: DesiredWithinRange
    status: "False"
    type: ScalingLimited
  currentMetrics:
  - resource:
      current:
        averageUtilization: 3
        averageValue: 9m
      name: cpu
    type: Resource
  currentReplicas: 1
  desiredReplicas: 1
  lastScaleTime: "2022-01-20T03:51:44Z"
```

ä¸Šé¢å¯¹ `HPA` çš„æ¦‚å¿µå’Œå¦‚ä½•ä½¿ç”¨æœ‰äº†ä¸€å®šçš„è®¤çŸ¥ï¼Œä»è¿™é‡Œå¼€å§‹å¯¹ HPA Controller çš„æºç è¿›è¡Œè§£è¯»ã€‚

æ•°æ®ç»“æ„ï¼š

```go
// HorizontalController is responsible for the synchronizing HPA objects stored
// in the system with the actual deployments/replication controllers they
// control.
type HorizontalController struct {
    // Scale èµ„æºçš„è¯»å†™ï¼ˆå¦‚ Deployment çš„ Scaleï¼‰
    scaleNamespacer scaleclient.ScalesGetter
    // HorizontalPodAutoscaler èµ„æºçš„è¯»å†™
    hpaNamespacer   autoscalingclient.HorizontalPodAutoscalersGetter
    // ç”¨äºæ ¹æ®ç±»å‹å’Œç‰ˆæœ¬è·å–èµ„æºä¿¡æ¯
    mapper          apimeta.RESTMapper
     
    // è®¡ç®— replica æ•°é‡
    replicaCalc   *ReplicaCalculator
    // è®¢é˜… HPA èµ„æºï¼Œå¤„ç†èµ„æºå˜åŒ–
    eventRecorder record.EventRecorder
 
    // æ¯æ¬¡ç¼©å®¹ä¹‹é—´ç­‰å¾…æ—¶é—´
    downscaleStabilisationWindow time.Duration
 
    // hpaLister is able to list/get HPAs from the shared cache from the informer passed in to
    // NewHorizontalController.
    // ç”¨äºè¯»å– hpa å¯¹è±¡
    hpaLister       autoscalinglisters.HorizontalPodAutoscalerLister
    hpaListerSynced cache.InformerSynced
 
    // podLister is able to list/get Pods from the shared cache from the informer passed in to
    // NewHorizontalController.
    // ç”¨äºè¯»å– pod èµ„æº
    podLister       corelisters.PodLister
    podListerSynced cache.InformerSynced
 
    // Controllers that need to be synced
    // åªä¼šå¯åŠ¨ä¸€ä¸ª workerï¼Œå³å¦‚æœæœ‰å¤§é‡çš„ HPA èµ„æºæ—¶ï¼Œä¸€éƒ¨åˆ†èµ„æºçš„æ‰©ç¼©å®¹å¯èƒ½ä¸é‚£ä¹ˆåŠæ—¶
    queue workqueue.RateLimitingInterface
 
    // Latest unstabilized recommendations for each autoscaler.
    // è®°å½•æ¨èä¼¸ç¼©æ•°é‡
    recommendations map[string][]timestampedRecommendation
 
    // Latest autoscaler events
    // è®°å½•ä¼¸ç¼©äº‹ä»¶
    scaleUpEvents   map[string][]timestampedScaleEvent
    scaleDownEvents map[string][]timestampedScaleEvent
}
```

## 4. æ§åˆ¶å™¨é€»è¾‘

### 4.1. ä¼¸ç¼©è¿‡ç¨‹

ä¼¸ç¼©è¿‡ç¨‹çš„è§¦å‘ä¸æ˜¯å®æ—¶çš„ï¼Œè€Œæ˜¯ä» `queue` é‡Œæ¶ˆè´¹æ•°æ®ï¼Œè¿›è¡Œä¸€æ¬¡ä¼¸ç¼©æµç¨‹ï¼Œå†æ¬¡å°†èµ„æºåæ”¾å…¥ `queue`, å®Œæˆä¸€æ¬¡å¾ªç¯ã€‚

è¯¦ç»†æµç¨‹å¦‚ä¸‹ï¼š

#### 4.1.1. ä» `queue` è¯»å–ä¸€æ¡æ¶ˆæ¯ï¼Œæ ¹æ®æ¶ˆæ¯ä¸­çš„ä¿¡æ¯ï¼Œè·å– `hpa` å¯¹è±¡

```go
func (a *HorizontalController) processNextWorkItem() bool {
    key, quit := a.queue.Get()
    if quit {
        return false
    }
    // å¤„ç†å®Œæ ‡è®°å®Œæˆ
    defer a.queue.Done(key)

    // å¤„ç†å‡½æ•°å…¥å£
    deleted, err := a.reconcileKey(key.(string))
    if err != nil {
        utilruntime.HandleError(err)
    }
    // Add request processing HPA to queue with resyncPeriod delay.
    // Requests are always added to queue with resyncPeriod delay. If there's already request
    // for the HPA in the queue then a new request is always dropped. Requests spend resyncPeriod
    // in queue so HPAs are processed every resyncPeriod.
    // Request is added here just in case last resync didn't insert request into the queue. This
    // happens quite often because there is race condition between adding request after resyncPeriod
    // and removing them from queue. Request can be added by resync before previous request is
    // removed from queue. If we didn't add request here then in this case one request would be dropped
    // and HPA would processed after 2 x resyncPeriod.
    if !deleted {
        // å¦‚æœ hpa æ²¡æœ‰è¢«åˆ é™¤ï¼Œå†æ¬¡æ”¾å›é˜Ÿåˆ—é‡Œï¼Œç»è¿‡é»˜è®¤æ—¶é—´é—´éš”ï¼ˆ15sï¼Œk8s å¯åŠ¨å‚æ•°å¯é…ç½®ï¼‰åå†è¯»å–å¤„ç†ä¸€æ¬¡
        a.queue.AddRateLimited(key)
    }

    return true
}

func (a *HorizontalController) reconcileKey(key string) (deleted bool, err error) {
    namespace, name, err := cache.SplitMetaNamespaceKey(key)
    if err != nil {
        return true, err
    }

    // è¯»å– hpa å¯¹è±¡ `*autoscalingv1.HorizontalPodAutoscaler`
    hpa, err := a.hpaLister.HorizontalPodAutoscalers(namespace).Get(name)
    if errors.IsNotFound(err) {
        klog.Infof("Horizontal Pod Autoscaler %s has been deleted in %s", name, namespace)
        delete(a.recommendations, key)
        delete(a.scaleUpEvents, key)
        delete(a.scaleDownEvents, key)
        return true, nil
    }
    if err != nil {
        return false, err
    }

    // å¤„ç†æœ¬æ¬¡ä¼¸ç¼©é€»è¾‘
    return false, a.reconcileAutoscaler(hpa, key)
}
```

#### 4.1.2. ä¸ºäº†å…¼å®¹è€ç‰ˆæœ¬ï¼Œè¯»å–æ—¶ hpa å¯¹è±¡ä¸º v1 ç‰ˆæœ¬ï¼Œè¯»å–ååœ¨ä»£ç é‡Œä¼šå…ˆç»Ÿä¸€è½¬æ¢ä¸º v2 ç‰ˆæœ¬,ä»è€Œä¹‹åçš„é€»è¾‘ç»Ÿä¸€

```go
func (a *HorizontalController) reconcileAutoscaler(hpav1Shared *autoscalingv1.HorizontalPodAutoscaler, key string) error {
    // make a copy so that we never mutate the shared informer cache (conversion can mutate the object)
    hpav1 := hpav1Shared.DeepCopy()
    // then, convert to autoscaling/v2, which makes our lives easier when calculating metrics
    hpaRaw, err := unsafeConvertToVersionVia(hpav1, autoscalingv2.SchemeGroupVersion)
    if err != nil {
        a.eventRecorder.Event(hpav1, v1.EventTypeWarning, "FailedConvertHPA", err.Error())
        return fmt.Errorf("failed to convert the given HPA to %s: %v", autoscalingv2.SchemeGroupVersion.String(), err)
    }
    hpa := hpaRaw.(*autoscalingv2.HorizontalPodAutoscaler)
    ...
}
```

#### 4.1.3. æ ¹æ® `hpa.Spec.ScaleTargetRef` ä¿¡æ¯è¯»åˆ°éœ€è¦ä¼¸ç¼©çš„èµ„æº

```go
func (a *HorizontalController) reconcileAutoscaler(hpav1Shared *autoscalingv1.HorizontalPodAutoscaler, key string) error {
    ...
    // çœç•¥éƒ¨åˆ†ä»£ç 
    hpa := hpaRaw.(*autoscalingv2.HorizontalPodAutoscaler)

    reference := fmt.Sprintf("%s/%s/%s", hpa.Spec.ScaleTargetRef.Kind, hpa.Namespace, hpa.Spec.ScaleTargetRef.Name)

    // è§£æèµ„æº api version
    targetGV, err := schema.ParseGroupVersion(hpa.Spec.ScaleTargetRef.APIVersion)
    if err != nil {
        a.eventRecorder.Event(hpa, v1.EventTypeWarning, "FailedGetScale", err.Error())
        setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionFalse, "FailedGetScale", "the HPA controller was unable to get the target's current scale: %v", err)
        a.updateStatusIfNeeded(hpaStatusOriginal, hpa)
        return fmt.Errorf("invalid API version in scale target reference: %v", err)
    }

    // èµ„æºç±»å‹
    targetGK := schema.GroupKind{
        Group: targetGV.Group,
        Kind:  hpa.Spec.ScaleTargetRef.Kind,
    }

    // æŸ¥è¯¢èµ„æºå¯¹è±¡
    mappings, err := a.mapper.RESTMappings(targetGK)
    if err != nil {
        a.eventRecorder.Event(hpa, v1.EventTypeWarning, "FailedGetScale", err.Error())
        setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionFalse, "FailedGetScale", "the HPA controller was unable to get the target's current scale: %v", err)
        a.updateStatusIfNeeded(hpaStatusOriginal, hpa)
        return fmt.Errorf("unable to determine resource for scale target reference: %v", err)
    }

    // æ ¹æ® ns èµ„æºç±»å‹ èµ„æºç‰ˆæœ¬å’Œèµ„æºåç§° ç¡®å®šæœ€ç»ˆå”¯ä¸€çš„æ“ä½œçš„å¯¹è±¡ scale
    // scale æ˜¯ä¸€ä¸ªæŠ½è±¡çš„æ¦‚å¿µï¼Œè¡¨ç¤ºå¯ä¼¸ç¼©çš„èµ„æº(Deployment, replicaSet StatefulSet ç­‰ç­‰)
    scale, targetGR, err := a.scaleForResourceMappings(hpa.Namespace, hpa.Spec.ScaleTargetRef.Name, mappings)
    if err != nil {
        a.eventRecorder.Event(hpa, v1.EventTypeWarning, "FailedGetScale", err.Error())
        setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionFalse, "FailedGetScale", "the HPA controller was unable to get the target's current scale: %v", err)
        a.updateStatusIfNeeded(hpaStatusOriginal, hpa)
        return fmt.Errorf("failed to query scale subresource for %s: %v", reference, err)
    }
    //  è®°å½•ä¿¡æ¯
    setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionTrue, "SucceededGetScale", "the HPA controller was able to get the target's current scale")
    // è·å–å½“å‰å‰¯æœ¬æ•°
    currentReplicas := scale.Spec.Replicas
    a.recordInitialRecommendation(currentReplicas, key)
    // çœç•¥éƒ¨åˆ†ä»£ç 
    ...
}
```

#### 4.1.4. ç¡®å®šå½“å‰å‰¯æœ¬æ•°ï¼Œæœ€å¤§æœ€å°å¯ä¼¸ç¼©çš„å‰¯æœ¬æ•°

```go

func (a *HorizontalController) reconcileAutoscaler(hpav1Shared *autoscalingv1.HorizontalPodAutoscaler, key string) error {
    ...
    // çœç•¥éƒ¨åˆ†ä»£ç 
    currentReplicas := scale.Spec.Replicas

    var (
        metricStatuses        []autoscalingv2.MetricStatus
        metricDesiredReplicas int32
        metricName            string
    )

    desiredReplicas := int32(0)
    rescaleReason := ""

    var minReplicas int32

    if hpa.Spec.MinReplicas != nil {
        minReplicas = *hpa.Spec.MinReplicas
    } else {
        // Default value
        minReplicas = 1
    }

    rescale := true

    // å¦‚æœå½“å‰å‰¯æœ¬æ•°ä¸º 0 ä½†æ˜¯ hpa å®šä¹‰çš„æœ€å° replicas ä¸ç­‰0 æ—¶ï¼Œè¢«è®¤è¯†æ˜¯å…³é—­å¼¹æ€§ä¼¸ç¼©çš„èƒ½åŠ›ï¼Œä¸ä¼šè¿›è¡Œä¼¸ç¼©æ“ä½œ
    if scale.Spec.Replicas == 0 && minReplicas != 0 {
        // Autoscaling is disabled for this resource
        desiredReplicas = 0
        rescale = false
        setCondition(hpa, autoscalingv2.ScalingActive, v1.ConditionFalse, "ScalingDisabled", "scaling is disabled since the replica count of the target is zero")
    } else if currentReplicas > hpa.Spec.MaxReplicas {
        // å¦‚æœå½“å‰å‰¯æœ¬æ•°è¶…è¿‡é¢„è®¾æœ€å¤§æ•°ï¼Œç›®æ ‡å‰¯æœ¬æ•°è®¾å®šä¸ºé¢„è®¾æœ€å¤§å€¼ï¼Œä¸ä¼šå»è¿›è¡Œè®¡ç®—æ“ä½œ
        rescaleReason = "Current number of replicas above Spec.MaxReplicas"
        desiredReplicas = hpa.Spec.MaxReplicas
    } else if currentReplicas < minReplicas {
        // å¦‚æœå½“å‰å‰¯æœ¬æ•°è¶…ä½äºé¢„è®¾æœ€å°å€¼ï¼Œç›®æ ‡å‰¯æœ¬æ•°è®¾å®šä¸ºé¢„è®¾æœ€å°å€¼ï¼Œä¸ä¼šå»è¿›è¡Œè®¡ç®—æ“ä½œ
        rescaleReason = "Current number of replicas below Spec.MinReplicas"
        desiredReplicas = minReplicas
    } else {
        // éœ€è¦é€šè¿‡è®¡ç®—è·å¾—ç›®æ ‡å‰¯æœ¬æ•°
        ...
    }
    ...
}
```

#### 4.1.5. è®¡ç®—å¹¶å¤„ç†å‰¯æœ¬æ•°

```go
...
} else {
    var metricTimestamp time.Time
    // è·å– metric æ•°æ®å¹¶æ ¹æ® metrics è®¡ç®—ç›®æ ‡å‰¯æœ¬æ•°
    metricDesiredReplicas, metricName, metricStatuses, metricTimestamp, err = a.computeReplicasForMetrics(hpa, scale, hpa.Spec.Metrics)
    if err != nil {
        a.setCurrentReplicasInStatus(hpa, currentReplicas)
        if err := a.updateStatusIfNeeded(hpaStatusOriginal, hpa); err != nil {
            utilruntime.HandleError(err)
        }
        a.eventRecorder.Event(hpa, v1.EventTypeWarning, "FailedComputeMetricsReplicas", err.Error())
        return fmt.Errorf("failed to compute desired number of replicas based on listed metrics for %s: %v", reference, err)
    }

    klog.V(4).Infof("proposing %v desired replicas (based on %s from %s) for %s", metricDesiredReplicas, metricName, metricTimestamp, reference)

    rescaleMetric := ""
    // æ­¤æ—¶ desiredReplicas == 0 
    // å¦‚æœè®¡ç®—å‡ºç»“æœå¤§äº 0ï¼Œåˆ™èµ‹å€¼ç»™ desiredReplicas
    if metricDesiredReplicas > desiredReplicas {
        desiredReplicas = metricDesiredReplicas
        rescaleMetric = metricName
    }
    // å‡†å¤‡ rescaleReason ç”¨äºè®°å½•åˆ° hpa çš„ Conditions å­—æ®µï¼Œå¯ä»¥æŸ¥çœ‹
    if desiredReplicas > currentReplicas {
        rescaleReason = fmt.Sprintf("%s above target", rescaleMetric)
    }
    if desiredReplicas < currentReplicas {
        rescaleReason = "All metrics below target"
    }
    if hpa.Spec.Behavior == nil {
        // é»˜è®¤è§„åˆ™é™åˆ¶ä¸€æ¬¡ä¼¸ç¼©çš„æ•°é‡ å•æ¬¡ä¼¸ç¼©æ¯”ä¾‹ä¸ä¼šè¶…è¿‡ Max(currentReplicas * 2, 4)
        desiredReplicas = a.normalizeDesiredReplicas(hpa, key, currentReplicas, desiredReplicas, minReplicas)
    } else {
        // å¦‚æœ hpa.Spec.Behavior != nil åˆ™å¤„ç†æ˜¯å¦éœ€è¦é…ç½®çš„è§„åˆ™
        // å¦‚æœé…ç½®è§„åˆ™ä¸ºç¦ç”¨(å¯ä»¥å•ç‹¬ç¦ç”¨ scale up æˆ– scale down)ï¼Œåˆ™è¿”å› currentReplicas
        // å¦‚æœé…åˆäº† hpa.Spec.Behavior.Policies, åˆ™æŒ‰ç­–ç•¥è¿›è¡Œä¼¸ç¼©(å¯ä»¥é…ç½®ä¸€æ¬¡ä¼¸ç¼©çš„ pod æ•°é‡æˆ–è€…ç™¾åˆ†æ¯”ï¼Œä»è€Œå¯ä»¥ç¡®ä¿ä¼¸ç¼©è·¨åº¦ä¸ä¼šä¸€æ¬¡æ€§å¾ˆå¤§)
        desiredReplicas = a.normalizeDesiredReplicasWithBehaviors(hpa, key, currentReplicas, desiredReplicas, minReplicas)
    }
    // åˆ¤æ–­æ˜¯å¦éœ€è¦ä¼¸ç¼©ï¼Œæœ€ç»ˆè®¡ç®—ç»“æœå¯èƒ½ä¼šå½“å‰å‰¯æœ¬æ•°ä¸€è‡´
    rescale = desiredReplicas != currentReplicas
}
...
```

#### 4.1.6. è°ƒæ•´å‰¯æœ¬æ•°

```go
func (a *HorizontalController) reconcileAutoscaler(hpav1Shared *autoscalingv1.HorizontalPodAutoscaler, key string) error {
    ...

    // éœ€è¦ä¼¸ç¼©
    if rescale {
        scale.Spec.Replicas = desiredReplicas
        // æ›´æ–° scale å¯¹è±¡ï¼Œæœ€ç»ˆä¼¸ç¼©æ“ä½œä¼šç”± scale å¯¹åº”çš„ controller å»å®Œæˆ
        _, err = a.scaleNamespacer.Scales(hpa.Namespace).Update(context.TODO(), targetGR, scale, metav1.UpdateOptions{})
        if err != nil {
            // è®°å½•æ—¥å¿—å’Œ event
            a.eventRecorder.Eventf(hpa, v1.EventTypeWarning, "FailedRescale", "New size: %d; reason: %s; error: %v", desiredReplicas, rescaleReason, err.Error())
            setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionFalse, "FailedUpdateScale", "the HPA controller was unable to update the target scale: %v", err)
            a.setCurrentReplicasInStatus(hpa, currentReplicas)
            if err := a.updateStatusIfNeeded(hpaStatusOriginal, hpa); err != nil {
                utilruntime.HandleError(err)
            }
            return fmt.Errorf("failed to rescale %s: %v", reference, err)
        }
        setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionTrue, "SucceededRescale", "the HPA controller was able to update the target scale to %d", desiredReplicas)
        a.eventRecorder.Eventf(hpa, v1.EventTypeNormal, "SuccessfulRescale", "New size: %d; reason: %s", desiredReplicas, rescaleReason)
        a.storeScaleEvent(hpa.Spec.Behavior, key, currentReplicas, desiredReplicas)
        klog.Infof("Successful rescale of %s, old size: %d, new size: %d, reason: %s",
            hpa.Name, currentReplicas, desiredReplicas, rescaleReason)
    } else {
        klog.V(4).Infof("decided not to scale %s to %v (last scale time was %s)", reference, desiredReplicas, hpa.Status.LastScaleTime)
        desiredReplicas = currentReplicas
    }

    // æ›´æ–°çŠ¶æ€
    a.setStatus(hpa, currentReplicas, desiredReplicas, metricStatuses, rescale)
    return a.updateStatusIfNeeded(hpaStatusOriginal, hpa)
}
```

è‡³æ­¤ï¼Œä¼¸ç¼©çš„å¤§æµç¨‹æ˜¯å®Œæˆäº†ï¼Œå‰©ä¸‹çš„å°±æ˜¯å¯¹ç»†èŠ‚çš„äº†è§£äº†ã€‚æ¯”å¦‚å¦‚ä½•è®¡ç®—ç›®æ ‡å‰¯æœ¬æ•°çš„ï¼Œå¦‚ä½•å¤„ç†ä¼¸ç¼©çš„ç­–ç•¥ç­‰ç­‰ã€‚

{{< image src="hpa_controller.png" caption="ä¼¸ç¼©æµç¨‹" width="800" >}}

### 4.2. è®¡ç®—å‰¯æœ¬æ•°è¿‡ç¨‹

å¯é€šè¿‡ä¸Šé¢æµç¨‹çœ‹åˆ°ä¼šæœ‰ä¸€ä¸ªè®¡ç®—ç›®æ ‡å‰¯æœ¬æ•°çš„è¿‡ç¨‹ `a.computeReplicasForMetrics`ï¼Œçœ‹ä¼¼ç®€å•å…¶å®å†…éƒ¨ç›¸å½“ä¸°å¯Œçš„ä¸€ä¸ªè¿‡ç¨‹ï¼Œä¸‹é¢ä¸€èµ·çœ‹ä¸€ä¸‹å¦‚ä½•å»è®¡ç®—çš„ã€‚

#### 4.2.1. éå† `spec.Metrics` è®¡ç®—å¾—å‡ºæœ€å¤§å€¼

```go
// computeReplicasForMetrics computes the desired number of replicas for the metric specifications listed in the HPA,
// returning the maximum  of the computed replica counts, a description of the associated metric, and the statuses of
// all metrics computed.
func (a *HorizontalController) computeReplicasForMetrics(hpa *autoscalingv2.HorizontalPodAutoscaler, scale *autoscalingv1.Scale,
    metricSpecs []autoscalingv2.MetricSpec) (replicas int32, metric string, statuses []autoscalingv2.MetricStatus, timestamp time.Time, err error) {

    /*
    * çœç•¥äº†éƒ¨åˆ†æ— å…³ä»£ç 
    */
    if scale.Status.Selector == "" {
        errMsg := "selector is required"
        return 0, "", nil, time.Time{}, fmt.Errorf(errMsg)
    }

    selector, err := labels.Parse(scale.Status.Selector)
    if err != nil {
        return 0, "", nil, time.Time{}, fmt.Errorf(errMsg)
    }

    specReplicas := scale.Spec.Replicas
    statusReplicas := scale.Status.Replicas
    // è®°å½•æ¯ä¸ª metric çš„ç»“æœ
    statuses = make([]autoscalingv2.MetricStatus, len(metricSpecs))

    invalidMetricsCount := 0
    var invalidMetricError error
    var invalidMetricCondition autoscalingv2.HorizontalPodAutoscalerCondition

    // éå†è®¡ç®—
    for i, metricSpec := range metricSpecs {
        // è®¡ç®—å•ä¸ª metric çš„æ–¹æ³•
        replicaCountProposal, metricNameProposal, timestampProposal, condition, err := a.computeReplicasForMetric(hpa, metricSpec, specReplicas, statusReplicas, selector, &statuses[i])

        if err != nil {
            if invalidMetricsCount <= 0 {
                invalidMetricCondition = condition
                invalidMetricError = err
            }
            invalidMetricsCount++
        }
        // å–æœ€å¤§
        if err == nil && (replicas == 0 || replicaCountProposal > replicas) {
            timestamp = timestampProposal
            replicas = replicaCountProposal
            metric = metricNameProposal
        }
    }

    // If all metrics are invalid or some are invalid and we would scale down,
    // return an error and set the condition of the hpa based on the first invalid metric.
    // Otherwise set the condition as scaling active as we're going to scale
    if invalidMetricsCount >= len(metricSpecs) || (invalidMetricsCount > 0 && replicas < specReplicas) {
        return 0, "", statuses, time.Time{}, fmt.Errorf("invalid metrics (%v invalid out of %v), first error is: %v", invalidMetricsCount, len(metricSpecs), invalidMetricError)
    }

    return replicas, metric, statuses, timestamp, nil
}
```

#### 4.2.2. è®¡ç®—å•ä¸ª metricï¼šæ ¹æ®ç±»å‹è®¡ç®— metric

{{< admonition type=inf title="åè¯è§£æ" open=true >}}
hpa å¯¹è±¡çš„ `spac.metrics` ä¸­çš„å…ƒç´ æœ‰ä¸ª `Type` å­—æ®µ,è®°å½• metric æ•°æ®æºçš„ç±»å‹ï¼Œç›®å‰æ”¯æŒçš„ä»¥ä¸‹å‡ ç§ï¼š

| ç±»å‹ | è¯´æ˜ | æ”¯æŒçš„è®¡ç®—ç±»å‹ | ä¸æ”¯æŒçš„è®¡ç®—ç±»å‹ |
| :-------: | :----- | :----- |:----- |
| Object | ç”± k8s æœ¬èº«èµ„æºæä¾›çš„æ•°æ®ï¼Œå¦‚ ingress æä¾›å‘½ä¸­è§„åˆ™æ•°é‡ |AverageValue <br> Value| AverageUtilization|
| Pods | ç”± pod æä¾›çš„é™¤ CPU å†…å­˜ä¹‹å¤–çš„æ•°æ® |AverageValue| AverageUtilization <br> Value|
| Resource | pod æä¾›çš„ç³»ç»Ÿèµ„æºï¼ˆCPU å†…å­˜ï¼‰ |AverageUtilization <br> AverageValue | Value |
| External | å¤–éƒ¨æä¾›çš„ç›‘æ§æŒ‡æ ‡ |AverageValue <br> Value |AverageUtilization |

- AverageValueï¼šè®¾å®šå¹³å‡å€¼ï¼Œå¦‚qpsï¼Œtpsã€‚è®¡ç®—æ—¶è¯»å– metric æ€»å’Œï¼Œä¸é¢„è®¾å¹³å‡å€¼âœ–ï¸å‰¯æœ¬æ•°è¿›è¡Œå¯¹æ¯”ï¼Œä»è€Œåˆ¤æ–­æ˜¯å¦éœ€è¦ä¼¸ç¼©ã€‚

- Valueï¼šè®¾å®šå›ºå®šå€¼ï¼Œå¦‚é˜Ÿåˆ—ä¸­æ¶ˆæ¯æ•°é‡ï¼ŒRedis ä¸­ list é•¿åº¦ç­‰ã€‚è®¡ç®—æ—¶ç›´æ¥æ‹¿ metric å’Œé¢„è®¾å€¼è¿›è¡Œå¯¹æ¯”ã€‚

- AverageUtilizationï¼š å¹³å‡åˆ©ç”¨ç‡ï¼Œå¦‚ CPU å’Œå†…å­˜ã€‚å…ˆè®¡ç®— podçš„åŸºæ•°çš„æ€»å’Œï¼ˆtotalMetric å’Œ totalRequestï¼‰ï¼Œæœ€ç»ˆè®¡ç®—åˆ©ç”¨ç‡ â†’ totalMetric/totalRequest
{{< /admonition >}}

æºç ï¼š

```go
// Computes the desired number of replicas for a specific hpa and metric specification,
// returning the metric status and a proposed condition to be set on the HPA object.
func (a *HorizontalController) computeReplicasForMetric(hpa *autoscalingv2.HorizontalPodAutoscaler, spec autoscalingv2.MetricSpec,
    specReplicas, statusReplicas int32, selector labels.Selector, status *autoscalingv2.MetricStatus) (replicaCountProposal int32, metricNameProposal string,
    timestampProposal time.Time, condition autoscalingv2.HorizontalPodAutoscalerCondition, err error) {

    switch spec.Type {
    case autoscalingv2.ObjectMetricSourceType:
        metricSelector, err := metav1.LabelSelectorAsSelector(spec.Object.Metric.Selector)
        if err != nil {
            condition := a.getUnableComputeReplicaCountCondition(hpa, "FailedGetObjectMetric", err)
            return 0, "", time.Time{}, condition, fmt.Errorf("failed to get object metric value: %v", err)
        }
        replicaCountProposal, timestampProposal, metricNameProposal, condition, err = a.computeStatusForObjectMetric(specReplicas, statusReplicas, spec, hpa, selector, status, metricSelector)
        if err != nil {
            return 0, "", time.Time{}, condition, fmt.Errorf("failed to get object metric value: %v", err)
        }
    case autoscalingv2.PodsMetricSourceType:
        metricSelector, err := metav1.LabelSelectorAsSelector(spec.Pods.Metric.Selector)
        if err != nil {
            condition := a.getUnableComputeReplicaCountCondition(hpa, "FailedGetPodsMetric", err)
            return 0, "", time.Time{}, condition, fmt.Errorf("failed to get pods metric value: %v", err)
        }
        replicaCountProposal, timestampProposal, metricNameProposal, condition, err = a.computeStatusForPodsMetric(specReplicas, spec, hpa, selector, status, metricSelector)
        if err != nil {
            return 0, "", time.Time{}, condition, fmt.Errorf("failed to get pods metric value: %v", err)
        }
    case autoscalingv2.ResourceMetricSourceType:
        replicaCountProposal, timestampProposal, metricNameProposal, condition, err = a.computeStatusForResourceMetric(specReplicas, spec, hpa, selector, status)
        if err != nil {
            return 0, "", time.Time{}, condition, err
        }
    case autoscalingv2.ExternalMetricSourceType:
        replicaCountProposal, timestampProposal, metricNameProposal, condition, err = a.computeStatusForExternalMetric(specReplicas, statusReplicas, spec, hpa, selector, status)
        if err != nil {
            return 0, "", time.Time{}, condition, err
        }
    default:
        errMsg := fmt.Sprintf("unknown metric source type %q", string(spec.Type))
        err = fmt.Errorf(errMsg)
        condition := a.getUnableComputeReplicaCountCondition(hpa, "InvalidMetricSourceType", err)
        return 0, "", time.Time{}, condition, err
    }
    return replicaCountProposal, metricNameProposal, timestampProposal, autoscalingv2.HorizontalPodAutoscalerCondition{}, nil
}
```

ä¸‹é¢ä¼šä»¥ `ExternalMetricSourceType` ä¸ºä¾‹ã€‚

#### 4.2.3. æ ¹æ®è®¡ç®—ç±»å‹æ‰§è¡Œå¯¹åº”çš„è®¡ç®—é€»è¾‘

>ä¸‹é¢ä»¥ External çš„ AverageValue ä¸ºä¾‹

```go
// GetExternalPerPodMetricReplicas calculates the desired replica count based on a
// target metric value per pod (as a milli-value) for the external metric in the
// given namespace, and the current replica count.
func (c *ReplicaCalculator) GetExternalPerPodMetricReplicas(statusReplicas int32, targetUtilizationPerPod int64, metricName, namespace string, metricSelector *metav1.LabelSelector) (replicaCount int32, utilization int64, timestamp time.Time, err error) {
    // æ„é€  metric æŸ¥è¯¢ label
    metricLabelSelector, err := metav1.LabelSelectorAsSelector(metricSelector)
    if err != nil {
        return 0, 0, time.Time{}, err
    }
    // æ‹‰å– metrics
    // https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis
    metrics, timestamp, err := c.metricsClient.GetExternalMetric(metricName, namespace, metricLabelSelector)
    if err != nil {
        return 0, 0, time.Time{}, fmt.Errorf("unable to get external metric %s/%s/%+v: %s", namespace, metricName, metricSelector, err)
    }
    // è®¡ç®—æ€»å’Œ
    utilization = 0
    for _, val := range metrics {
        utilization = utilization + val
    }

    replicaCount = statusReplicas
    // è®¡ç®— ä¼¸ç¼©æ¯”ä¾‹
    // targetUtilizationPerPod ä¸ºé…ç½®çš„å•ä¸ª pod çš„é¢„è®¾å€¼ï¼Œæ‰€ä»¥éœ€è¦ä¹˜ä»¥å‰¯æœ¬æ•°
    // å¦‚æœå½“å‰ metric æŸ¥è¯¢æ•°æ®ä¸º 100ï¼Œ é¢„è®¾çš„å•ä¸ª pod çš„å€¼ä¸º 20, å½“å‰æœ‰3ä¸ªå‰¯æœ¬æ•°
    // æ­¤æ—¶ä¼¸ç¼©æ¯”ä¾‹ä¸º 1.6667
    usageRatio := float64(utilization) / (float64(targetUtilizationPerPod) * float64(replicaCount))
    // c.tolerance å®¹å¿åº¦ï¼Œé»˜è®¤å€¼ä¸º 0.1
    // å¦‚æœ 1.0 - ä¼¸ç¼©æ¯”ä¾‹ è¶…è¿‡å®¹å¿åº¦åˆ™è®¤ä¸ºéœ€è¦ä¼¸ç¼©ã€‚è¿™é‡Œå¯ä»¥ç†è§£ä¸ºæŸ¥è¯¢åˆ°çš„ metric æ•°æ®ä¸é¢„è®¾çš„å€¼æµ®åŠ¨å°äº 10% æ—¶ ä¸éœ€è¦ä¼¸ç¼©
    if math.Abs(1.0-usageRatio) > c.tolerance {
        // update number of replicas if the change is large enough
        // ç»§ç»­ä¸Šé¢çš„æ•°æ®è®¡ç®— æ–°çš„ replica æ•°é‡ä¸º 100/20 -> 5
        replicaCount = int32(math.Ceil(float64(utilization) / float64(targetUtilizationPerPod)))
    }
    // è®¡ç®—å½“å‰çš„å¹³å‡æ•°æ® 100/3 -> 33.33 è¿™ä¸ªå€¼ä¼šåœ¨ HPA å¯¹è±¡çš„ æ—¥å¿—é‡Œæ‰“å°å‡ºæ¥å¯ä»¥çœ‹åˆ°
    utilization = int64(math.Ceil(float64(utilization) / float64(statusReplicas)))
    return replicaCount, utilization, timestamp, nil
}
```

> è®¡ç®—ç»“æœä¸€è·¯è¿”å›åˆ° `reconcileAutoscaler` æ–¹æ³•è¿›è¡Œæœ€ç»ˆä¼¸ç¼©ã€‚

## 5. æ‰©å±•ç”¨æ³•

åœ¨å®é™…å¼€å‘ç¯å¢ƒåªç”¨ `cpu`, `memory` æ¥ä½œä¸ºå¼¹æ€§ä¼¸ç¼©ä¾æ®æ˜¯è¿œè¿œä¸å¤Ÿçš„ã€‚å¤§å¤šæ•°æƒ…å†µå¯èƒ½ä¼šæ ¹æ® `qps`, `tps`, `å¹³å‡å»¶è¿Ÿæ—¶é—´`, `MQ ä¸­çš„æ¶ˆæ¯æ•°é‡`, `Redis ä¸­çš„æ•°æ®é‡` ç­‰ä¸ä¸šåŠ¡æ¯æ¯ç›¸å…³çš„æ•°æ®é‡åˆ¤æ–­ä¼¸ç¼©çš„ä¾æ®ï¼Œè¿™äº›éƒ½å±äº HPA çš„ External ç±»å‹çš„èŒƒç•´ã€‚ä½†æ˜¯ External ç±»å‹çš„æ•°æ®æºéœ€è¦å¯¹æ¥ `Adapter` çš„æ–¹å¼æ‰èƒ½ç”¨åˆ° HPA å¯¹è±¡å†…å®¹(å…·ä½“è¯·æŸ¥çœ‹: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis) ï¼Œä¹Ÿå°±æ˜¯è¯´éœ€è¦é¢å¤–å¼€å‘é‡çš„ã€‚

è¿™é‡Œæˆ‘æ¨èä¸€ä¸ªçŸ¥ååº¦æ¯”è¾ƒé«˜ä¸”æ”¯æŒçš„æ•°æ®é‡æ¯”è¾ƒå¹¿æ³›çš„ Adapter çš„å®ç° -- [KEDA](https://keda.sh/)ã€‚åŸºäºäº‹ä»¶é©±åŠ¨çš„autoscaler,ä¸”æ”¯æŒä» 0 åˆ° 1  1 åˆ° 0 çš„ä¼¸ç¼©ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸šåŠ¡ä½å³°æˆ–è€…æ— æµé‡æ—¶å¯ä»¥é™åˆ° 0 ä¸ªå‰¯æœ¬æ•°(è¿™ä¸ªåœ¨ HPA è¢«è®¤ä¸ºæ˜¯ç¦ç”¨è¯¥åŠŸèƒ½ æ‰€ä»¥ KEDA è‡ªå·±å®ç°çš„ 0 åˆ° 1  1 åˆ° 0 çš„ä¼¸ç¼©çš„èƒ½åŠ›ï¼Œå‰©ä½™çš„æƒ…å†µå®ƒå«ä¸ª HPA å¤„ç†)ã€‚

ç›®å‰ KEDA æ”¯æŒäº‹ä»¶ç±»å‹å¦‚ä¸‹ï¼š

{{< image src="keda.png" caption="KEDA æ”¯æŒçš„äº‹ä»¶" width="800" >}}

åªéœ€è¦åˆ›å»º KEDA çš„ CRD èµ„æºï¼Œå°±èƒ½å®ç°åŸºäºäº‹ä»¶çš„å¼¹æ€§ä¼¸ç¼©ï¼ŒKEDA çš„ controller ä¼šåˆ›å»ºå¯¹åº”çš„ HPA å¯¹è±¡ã€‚

ä¸‹é¢ä»¥ `Prometheus` ä¸ºä¾‹ï¼š

KEDA.ScaledObject:

```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: prom-scaledobject
  namespace: default
spec:
  scaleTargetRef:
    name: klyn-deploy
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://10.103.255.235:9090 # Prometheus æŸ¥è¯¢æ¥å£
      metricName: http_request_count # è‡ªå®šä¹‰åå­—
      query: sum(rate(http_request_count{job="klyn-service"}[2m])) # æŸ¥è¯¢è¯­å¥ 2m å†…çš„å¹³å‡ qps
      threshold: '50'
  maxReplicaCount: 5
  minReplicaCount: 1
  pollingInterval: 5
```

apply è¯¥ yaml åï¼Œä¼šåˆ›å»ºä¸€ä¸ª `ScaledObject` å’Œ `HPA` å¯¹è±¡ã€‚

```shell
âœ kubectl get ScaledObject      
NAME                SCALETARGETKIND      SCALETARGETNAME   MIN   MAX   TRIGGERS     AUTHENTICATION   READY   ACTIVE   FALLBACK   AGE
prom-scaledobject   apps/v1.Deployment   klyn-deploy       1     5     prometheus                    True    False    False      9d

## è¯¥ HPA å¯¹è±¡æœ‰ KEDA çš„ controller åˆ›å»ºçš„
âœ kubectl get hpa               
NAME                         REFERENCE                TARGETS      MINPODS   MAXPODS   REPLICAS   AGE
keda-hpa-prom-scaledobject   Deployment/klyn-deploy   0/50 (avg)   1         5         1          9d
```

è¿™æ ·ä¸€æ¥ï¼Œå¯ä»¥æ ¹æ®å¾ˆå¤šäº‹ä»¶æ¥æ§åˆ¶ä¼¸ç¼©çš„èƒ½åŠ›ï¼Œè¿™æ¯”å•ä¸€æ¥ cpu,å†…å­˜åˆ©ç”¨ç‡æ¥çœ‹æ›´çµæ´»ä¸”åŠæ—¶ã€‚å®Œå…¨å¯ä»¥æ ¹æ®äº‹ä»¶åœ¨æœåŠ¡çš„å‰¯æœ¬æ•°ä¸å¤Ÿç”¨æˆ–è€…æœ‰ä¸€å †äº‹ä»¶å‡†å¤‡å¤„ç†æ—¶ï¼Œå°½å¯èƒ½å¿«é€Ÿæ‰©å®¹ï¼Œç¡®ä¿å¤„ç†èƒ½åŠ›ä¸ä¼šå—æŸã€‚

## 6. æ€»ç»“

è¿™ç¯‡æ–‡ç« ä¸»è¦è®² HPA controller çš„æºç å’Œå¦‚ä½•ä½¿ç”¨ HPA ç›¸å…³å†…å®¹

- `HPA/VPA` çš„è§£é‡Š
- å¦‚ä½•ä½¿ç”¨ `HPA`
- `HPA Controller` å¦‚ä½•å¤„ç†ä¸€æ¬¡ä¼¸ç¼©äº‹ä»¶çš„
- `HPA Controller` å¦‚ä½•è®¡ç®—ç›®æ ‡å®ä¾‹æ•°çš„
- è®¤è¯†å’Œä½¿ç”¨ `KEDA` -- ä¸€ä¸ªåŸºäºäº‹ä»¶é©±åŠ¨çš„ autoscaler

## 7. é“¾æ¥ğŸ”—

- [horizontal-pod-autoscale](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [pvertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)
- [metrics server](https://github.com/kubernetes-sigs/metrics-server)
- [KEDA](https://keda.sh)
